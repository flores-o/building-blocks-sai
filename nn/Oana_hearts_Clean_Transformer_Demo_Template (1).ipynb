{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdzJWkQ-x62a"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is a template for a clean, first principles implementation of GPT-2 in PyTorch. This is an accompaniment to [my video tutorial on implementing GPT-2](https://neelnanda.io/transformer-tutorial-2). If you want to properly understand how to implement GPT-2, you'll need to do it yourself! **I recommend filling out this template *as* you watch the video, and seeing how far you can get with each section before watching me do it**. Each section comes with tests, so you can check that you got it right. You can see [a solution notebook here](https://www.neelnanda.io/transformer-solution) - use it if you get stuck, but make an attempt first, and no copying and pasting!\n",
        "\n",
        "There's a [template version of this notebook here](https://neelnanda.io/transformer-template), go and fill in the blanks (no copying and pasting!) and see if you can pass the tests.\n",
        "\n",
        "If you enjoyed this, I expect you'd enjoy learning more about what's actually going on inside these models and how to reverse engineer them! This is a fascinating young research field, with a lot of low-hanging fruit and open problems! **I recommend starting with my post [Concrete Steps for Getting Started in Mechanistic Interpretability](https://www.neelnanda.io/mechanistic-interpretability/getting-started).**\n",
        "\n",
        "This notebook was written to accompany my [TransformerLens library](https://github.com/neelnanda-io/TransformerLens) for doing mechanistic interpretability research on GPT-2 style language models, and is a clean implementation of the underlying transformer architecture in the library. (This notebook is based off of an earlier version called EasyTransformer)\n",
        "\n",
        "Further Resources:\n",
        "* [A Comprehensive Mechanistic Interpretability Explainer & Glossary](https://www.neelnanda.io/glossary)\n",
        "    * Expecially [the transformers section](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=pndoEIqJ6GPvC1yENQkEfZYR)\n",
        "* [200 Concrete Open Problems in Mechanistic Interpretability](https://www.neelnanda.io/concrete-open-problems)\n",
        "* My [TransformerLens library](https://github.com/neelnanda-io/TransformerLens) for doing mechanistic interpretability research on GPT-2 style language models.\n",
        "* My walkthrough of [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html), for a deeper dive into how to think about transformers:.\n",
        "\n",
        "Check out these other intros to transformers for another perspective:\n",
        "* Jay Alammar's [illustrated transformer](https://jalammar.github.io/illustrated-transformer/)\n",
        "* [Andrej Karpathy's MinGPT](https://github.com/karpathy/minGPT)\n",
        "\n",
        "**Sharing Guidelines:** This tutorial is still a bit of a work in progress! I think it's usable, but please don't post it anywhere publicly without checking with me first! Sharing with friends is fine.\n",
        "\n",
        "If you've found this useful, I'd love to hear about it! Positive and negative feedback also very welcome. You can reach me via [email](mailto:neelnanda27@gmail.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWUagX_ex62f"
      },
      "source": [
        "## Instructions\n",
        "* No need to read the Setup Section\n",
        "* Go to runtime > Change Runtime Type and set it to use a GPU\n",
        "* Read and run notebook up until the start of the section \"Actual Code!\". Then go to the template notebook and try coding up the model yourself!\n",
        "    * Bonus points for doing that without reading the solutions, and before I do it in the video!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y02oeRI-x62g"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zcA0ua0x62g",
        "outputId": "ad3fc35b-5b72-4330-af11-91f1fc0987b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n",
            "Collecting git+https://github.com/neelnanda-io/Easy-Transformer.git@clean-transformer-demo\n",
            "  Cloning https://github.com/neelnanda-io/Easy-Transformer.git (to revision clean-transformer-demo) to /tmp/pip-req-build-xyinpt8u\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/Easy-Transformer.git /tmp/pip-req-build-xyinpt8u\n",
            "  Running command git checkout -b clean-transformer-demo --track origin/clean-transformer-demo\n",
            "  Switched to a new branch 'clean-transformer-demo'\n",
            "  Branch 'clean-transformer-demo' set up to track remote branch 'clean-transformer-demo' from 'origin'.\n",
            "  Resolved https://github.com/neelnanda-io/Easy-Transformer.git to commit 1f25219e631aeb478d17075d47274db32c874e88\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from easy_transformer==0.1.0) (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easy_transformer==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easy_transformer==0.1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from easy_transformer==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from easy_transformer==0.1.0) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from easy_transformer==0.1.0) (4.66.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from easy_transformer==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from easy_transformer==0.1.0) (0.18.7)\n",
            "Requirement already satisfied: fancy_einsum in /usr/local/lib/python3.10/dist-packages (from easy_transformer==0.1.0) (0.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->easy_transformer==0.1.0) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->easy_transformer==0.1.0) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->easy_transformer==0.1.0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->easy_transformer==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->easy_transformer==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->easy_transformer==0.1.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->easy_transformer==0.1.0) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->easy_transformer==0.1.0) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->easy_transformer==0.1.0) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->easy_transformer==0.1.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->easy_transformer==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->easy_transformer==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->easy_transformer==0.1.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->easy_transformer==0.1.0) (2024.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->easy_transformer==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easy_transformer==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easy_transformer==0.1.0) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->easy_transformer==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->easy_transformer==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->easy_transformer==0.1.0) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->easy_transformer==0.1.0) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->easy_transformer==0.1.0) (0.4.5)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->easy_transformer==0.1.0) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->easy_transformer==0.1.0) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->easy_transformer==0.1.0) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->easy_transformer==0.1.0) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->easy_transformer==0.1.0) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->easy_transformer==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->easy_transformer==0.1.0) (2.19.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->easy_transformer==0.1.0) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->easy_transformer==0.1.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->easy_transformer==0.1.0) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->easy_transformer==0.1.0) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->easy_transformer==0.1.0) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->easy_transformer==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->easy_transformer==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->easy_transformer==0.1.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->easy_transformer==0.1.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easy_transformer==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->easy_transformer==0.1.0) (5.0.1)\n",
            "\n",
            "\u001b[31m================================================================================\u001b[m\n",
            "\u001b[31m================================================================================\u001b[m\n",
            "\n",
            "  \u001b[1m\u001b[33m                            DEPRECATION WARNING                            \u001b[m\n",
            "\n",
            "    \u001b[1m\u001b[4m Node.js 16.x is no longer actively supported!\u001b[m\n",
            "\n",
            "  \u001b[1mYou will not receive security or critical stability updates\u001b[m for this version.\n",
            "\n",
            "  You should migrate to a supported version of Node.js as soon as possible.\n",
            "  Use the installation script that corresponds to the version of Node.js you\n",
            "  wish to install. e.g.\n",
            "  \n",
            "   * \u001b[31mhttps://deb.nodesource.com/setup_16.x — Node.js 16 \"Gallium\" \u001b[1m(deprecated)\u001b[m\n",
            "   * \u001b[32mhttps://deb.nodesource.com/setup_18.x — Node.js 18 \"Hydrogen\" (Maintenance)\u001b[m\n",
            "   * \u001b[31mhttps://deb.nodesource.com/setup_19.x — Node.js 19 \"Nineteen\" \u001b[1m(deprecated)\u001b[m\n",
            "   * \u001b[1m\u001b[32mhttps://deb.nodesource.com/setup_20.x — Node.js 20 LTS \"Iron\" (recommended)\u001b[m\n",
            "   * \u001b[32mhttps://deb.nodesource.com/setup_21.x — Node.js 21 \"Iron\" (current)\u001b[m\n",
            "   \n",
            "\n",
            "\n",
            "  Please see \u001b[1mhttps://github.com/nodejs/Release\u001b[m for details about which\n",
            "  version may be appropriate for you.\n",
            "\n",
            "  The \u001b[32m\u001b[1mNodeSource\u001b[m Node.js distributions repository contains\n",
            "  information both about supported versions of Node.js and supported Linux\n",
            "  distributions. To learn more about usage, see the repository:\n",
            "   \u001b[4m\u001b[1mhttps://github.com/nodesource/distributions\u001b[m\n",
            "\n",
            "\u001b[31m================================================================================\u001b[m\n",
            "\u001b[31m================================================================================\u001b[m\n",
            "\n",
            "Continuing in 10 seconds ...\n",
            "\n",
            "\u001b[38;5;79m2024-12-10 01:08:57 - Installing pre-requisites\u001b[0m\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://deb.nodesource.com/node_16.x nodistro InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,458 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n",
            "Fetched 3,941 kB in 3s (1,269 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ca-certificates is already the newest version (20240203~22.04.1).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.19).\n",
            "gnupg is already the newest version (2.2.27-3ubuntu2.1).\n",
            "apt-transport-https is already the newest version (2.4.13).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://deb.nodesource.com/node_16.x nodistro InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\u001b[1;32m2024-12-10 01:09:06 - Repository configured successfully. To install Node.js, run: apt-get install nodejs -y\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "nodejs is already the newest version (16.20.2-1nodesource1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\n",
            "Collecting git+https://github.com/neelnanda-io/PySvelte.git\n",
            "  Cloning https://github.com/neelnanda-io/PySvelte.git to /tmp/pip-req-build-cqsupedl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/PySvelte.git /tmp/pip-req-build-cqsupedl\n",
            "  Resolved https://github.com/neelnanda-io/PySvelte.git to commit 582d85ff708947e72b35cfcca05641332b44f5f5\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (4.66.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (2.2.2)\n",
            "Requirement already satisfied: typeguard~=2.0 in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (2.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->PySvelte==1.0.0) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->PySvelte==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->PySvelte==1.0.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->PySvelte==1.0.0) (2024.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->PySvelte==1.0.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->PySvelte==1.0.0) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->PySvelte==1.0.0) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->PySvelte==1.0.0) (0.4.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->PySvelte==1.0.0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->PySvelte==1.0.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->PySvelte==1.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->PySvelte==1.0.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->PySvelte==1.0.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->PySvelte==1.0.0) (3.0.2)\n",
            "Requirement already satisfied: fancy_einsum in /usr/local/lib/python3.10/dist-packages (0.0.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "  print(\"Running as a Colab notebook\")\n",
        "  %pip install git+https://github.com/neelnanda-io/Easy-Transformer.git@clean-transformer-demo\n",
        "  # Install another version of node that makes PySvelte work way faster\n",
        "  !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "  %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "  %pip install fancy_einsum\n",
        "  %pip install einops\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "  print(\"Running as a Jupyter notebook - intended for development only!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtGOTewzx62i"
      },
      "outputs": [],
      "source": [
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "from dataclasses import dataclass\n",
        "from easy_transformer import EasyTransformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from easy_transformer.utils import get_corner, gelu_new, tokenize_and_concatenate\n",
        "import tqdm.auto as tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cjfh_f02x62i",
        "outputId": "1cc12f9d-c053-4bae-bd95-a8a2f9e6104d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving model to device:  cuda\n",
            "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
          ]
        }
      ],
      "source": [
        "reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrrhCVU2x62j"
      },
      "source": [
        "# Understanding Inputs & Outputs of a Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwa4XvoPx62k"
      },
      "source": [
        "## What is the point of a transformer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9WTbq2Wx62l"
      },
      "source": [
        "**Transformers exist to model text!**\n",
        "\n",
        "We're going to focus GPT-2 style transformers. Key feature: They generate text! You feed in language, and the model generates a probability distn over tokens. And you can repeatedly sample from this to generate text!\n",
        "\n",
        "### How is the model trained?\n",
        "\n",
        "You give it a bunch of text, and train it to predict the next token.\n",
        "\n",
        "Importantly, if you give a model 100 tokens in a sequence, it predicts the next token for *each* prefix, ie it produces 100 predictions. This is kinda weird but it's much easier to make one that does this. And it also makes training more efficient, because you can 100 bits of feedback rather than just one.\n",
        "\n",
        "#### Objection: Isn't this trivial for the first 99?\n",
        "\n",
        "No! We make the transformer have *causal attention*. The core thing is that it can only move information forwards in the sequence. The prediction of what comes after token 50 is only a function of the first 50 tokens, *not* of token 51. (Jargon: *autoregressive*)\n",
        "\n",
        "### Key takeaway:\n",
        "\n",
        "Transformers are *sequence modelling engines*. It does the same processing in parallel at each sequence position, can move information between positions with attention, and conceptually can take a sequence of arbitrary length (not actually true, see later)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrsnI_V6x62m"
      },
      "source": [
        "## Tokens - Transformer Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GphqhjCBx62m"
      },
      "source": [
        "Core point: Input is language (ie a sequence of characters, strings, etc)\n",
        "\n",
        "### How do we convert language to vectors?\n",
        "\n",
        "ML models take in vectors, not weird shit like language - how do we convert?\n",
        "\n",
        "#### Idea: integers to vectors\n",
        "\n",
        "We basically make a lookup table. Called an embedding.\n",
        "\n",
        "Jargon: **One-hot encoding** We map eg numbers from 1 to 100, to a 100-dim vector, with a 1 in the kth position, 0 everywhere else. Key intuition is that one-hot encodings let you think about each integer independently - useful when integers = labels.\n",
        "\n",
        "Dimensions = things that vary independently. Each input has its own dimension, so each input can be thought of independently, we don't bake in any relation.\n",
        "\n",
        "Lookup tables <=> Multiply a fixed matrix by the one-hot encoded vector.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU6BmTrQx62m"
      },
      "source": [
        "### Tokens: Language to sequence of integers\n",
        "\n",
        "Core idea: We need a model that can deal with arbitrary text. We want to convert this into integers, *and* we want these integers to be in a bounded range.\n",
        "\n",
        "**Idea:** Form a vocabulary!\n",
        "\n",
        "**Idea 1:** Get a dictionary!\n",
        "\n",
        "**Problem:** It can't cope with arbitrary text (eg URLs, punctuation, etc) Can't cope with mispellings.\n",
        "\n",
        "**Idea 2:** Vocab = 256 ASCII characters. Fixed vocab size, can do arbitrary text, etc.\n",
        "\n",
        "**Problem:** Loses structure of language - some sequences of characters are more meaningful than others.\n",
        "\n",
        "Eg \"language\" is a lot more meaningful than \"hjksdfiu\" - we want the first to be a single token, second to not be. It's a more efficient use of our vocab.\n",
        "\n",
        "#### What Actually Happens?\n",
        "\n",
        "This super cursed thing called Byte-Pair Encodings\n",
        "\n",
        "Ġ ~ means begins with a space, tokens with a leading space vs not are different.\n",
        "\n",
        "We begin with the 256 ASCII characters as our tokens, and then find the most common pair of tokens, and merge that into a new token. Eg \" t\" is the most common pair, so it's our next token! Repeat 50000 times..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAVESiiSx62n",
        "outputId": "f9a183ee-34bc-4182-a088-2123b2da5cb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('.', 13), ('/', 14), ('0', 15), ('1', 16), ('2', 17), ('3', 18), ('4', 19)]\n",
            "\n",
            "[('ľ', 250), ('Ŀ', 251), ('ŀ', 252), ('Ł', 253), ('ł', 254), ('Ń', 255), ('Ġt', 256), ('Ġa', 257), ('he', 258), ('in', 259), ('re', 260), ('on', 261), ('Ġthe', 262), ('er', 263), ('Ġs', 264), ('at', 265), ('Ġw', 266), ('Ġo', 267), ('en', 268), ('Ġc', 269)]\n",
            "\n",
            "[('Ġprodu', 990), ('Ġstill', 991), ('led', 992), ('ah', 993), ('Ġhere', 994), ('Ġworld', 995), ('Ġthough', 996), ('Ġnum', 997), ('arch', 998), ('imes', 999), ('ale', 1000), ('ĠSe', 1001), ('ĠIf', 1002), ('//', 1003), ('ĠLe', 1004), ('Ġret', 1005), ('Ġref', 1006), ('Ġtrans', 1007), ('ner', 1008), ('ution', 1009)]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sorted_vocab = sorted(list(reference_gpt2.tokenizer.vocab.items()), key=lambda n:n[1])\n",
        "print(sorted_vocab[:20])\n",
        "print()\n",
        "print(sorted_vocab[250:270])\n",
        "print()\n",
        "print(sorted_vocab[990:1010])\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyge8JGTx62n"
      },
      "source": [
        "Gets to weird esoteric shit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__5rl1uXx62n",
        "outputId": "2393a952-e3d2-4559-b842-d35fe8548390",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Revolution', 50237),\n",
              " ('Ġsnipers', 50238),\n",
              " ('Ġreverted', 50239),\n",
              " ('Ġconglomerate', 50240),\n",
              " ('Terry', 50241),\n",
              " ('794', 50242),\n",
              " ('Ġharsher', 50243),\n",
              " ('Ġdesolate', 50244),\n",
              " ('ĠHitman', 50245),\n",
              " ('Commission', 50246),\n",
              " ('Ġ(/', 50247),\n",
              " ('âĢ¦.\"', 50248),\n",
              " ('Compar', 50249),\n",
              " ('Ġamplification', 50250),\n",
              " ('ominated', 50251),\n",
              " ('Ġregress', 50252),\n",
              " ('ĠCollider', 50253),\n",
              " ('Ġinformants', 50254),\n",
              " ('Ġgazed', 50255),\n",
              " ('<|endoftext|>', 50256)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "sorted_vocab[-20:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijS5EXisx62n"
      },
      "source": [
        "Use the `to_tokens` method to convert text to numbers\n",
        "\n",
        "Prepends with a special token to give attention a resting position, disable with `prepend_bos=False`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5LgBnfOx62n",
        "outputId": "806372a3-5aaf-496e-b79a-f0da2a316928",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50256, 15354,   257,  1573,  6140,   351,   257,  3139,   393,  2272,\n",
            "          6067,     0]])\n",
            "tensor([[15354,   257,  1573,  6140,   351,   257,  3139,   393,  2272,  6067,\n",
            "             0]])\n"
          ]
        }
      ],
      "source": [
        "print(reference_gpt2.to_tokens(\"Whether a word begins with a capital or space matters!\"))\n",
        "print(reference_gpt2.to_tokens(\"Whether a word begins with a capital or space matters!\", prepend_bos=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_T8wwVRx62o"
      },
      "source": [
        "### Rant: Tokenization is a Headache\n",
        "\n",
        "Whether a word begins with a capital or space matters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqgqzqWox62o",
        "outputId": "b301b94c-355f-4637-90bb-99d27ffe7825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<|endoftext|>', 'R', 'alph']\n",
            "['<|endoftext|>', ' Ralph']\n",
            "['<|endoftext|>', ' r', 'alph']\n",
            "['<|endoftext|>', 'ral', 'ph']\n"
          ]
        }
      ],
      "source": [
        "print(reference_gpt2.to_str_tokens(\"Ralph\"))\n",
        "print(reference_gpt2.to_str_tokens(\" Ralph\"))\n",
        "print(reference_gpt2.to_str_tokens(\" ralph\"))\n",
        "print(reference_gpt2.to_str_tokens(\"ralph\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqqrythzx62o"
      },
      "source": [
        "Arithmetic is a total mess: Length is inconsistent, common numbers bundle together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJpxr0Crx62o",
        "outputId": "cd7db27d-8014-49cb-aeea-df0e385d841c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|endoftext|>',\n",
              " '568',\n",
              " '73',\n",
              " '+',\n",
              " '318',\n",
              " '46',\n",
              " '23',\n",
              " '=',\n",
              " '123',\n",
              " '45',\n",
              " '67',\n",
              " '89',\n",
              " '-',\n",
              " '1',\n",
              " '000000',\n",
              " '000']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "reference_gpt2.to_str_tokens(\"56873+3184623=123456789-1000000000\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SNC91-Px62o"
      },
      "source": [
        "### Key Takeaway:\n",
        "\n",
        "* We learn a dictionary of vocab of tokens (sub-words).\n",
        "\n",
        "* We (approx) losslessly convert language to integers via tokenizing it.\n",
        "\n",
        "* We convert integers to vectors via a lookup table.\n",
        "\n",
        "* Note: input to the transformer is a sequence of *tokens* (ie integers), not vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbFjzkASx62o"
      },
      "source": [
        "## Logits - Transformer Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ub-ofjwx62p"
      },
      "source": [
        "**Goal:** Probability distribution over next tokens. (for every *prefix* of the sequence - given n tokens, we make n next token predictions)\n",
        "\n",
        "**Problem:** How to convert a vector to a probability distribution?\n",
        "\n",
        "**Answer:** Use a softmax ($x_i \\to \\frac{e^{x_i}}{\\sum e^{x_j}}$), exponential makes everything positive, normalization makes it add to one.\n",
        "\n",
        "So the model outputs a tensor of logits, one vector of size $d_{vocab}$ for each input token.\n",
        "\n",
        "We can use this to generate things!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fKmlUUZx62p"
      },
      "source": [
        "## Generation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXzyIAoyx62p"
      },
      "source": [
        "**Step 1:** Convert text to tokens\n",
        "\n",
        "Shape = batch x position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyIdMa-3x62p",
        "outputId": "eedbfbf4-0702-4e3a-b1e9-261451d9c5d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
            "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
            "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
            "          1011,   625,   262,   995,     0]])\n",
            "torch.Size([1, 35])\n",
            "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
          ]
        }
      ],
      "source": [
        "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
        "tokens = reference_gpt2.to_tokens(reference_text)\n",
        "print(tokens)\n",
        "print(tokens.shape)\n",
        "print(reference_gpt2.to_str_tokens(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xqW_yDMx62p"
      },
      "source": [
        "**Step 2:** Map tokens to logits\n",
        "\n",
        "(run_with_cache means cache all intermediate activations, not important right now)\n",
        "\n",
        "shape = batch x position x d_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEjKNDJex62p",
        "outputId": "7bd89934-6d54-4e75-bfc2-a3606079350f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 35, 50257])\n"
          ]
        }
      ],
      "source": [
        "tokens = tokens.cuda()\n",
        "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHEd1T5vx62q"
      },
      "source": [
        "**Step 3:** Convert the logits to a distribution with a softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5J3caM0x62q",
        "outputId": "a9bfe17c-ef5c-476e-e68c-27b512940d5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 35, 50257])\n",
            "torch.Size([1, 35, 50257])\n"
          ]
        }
      ],
      "source": [
        "log_probs = logits.log_softmax(dim=-1)\n",
        "probs = logits.log_softmax(dim=-1)\n",
        "print(log_probs.shape)\n",
        "print(probs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vdAguXDx62q"
      },
      "source": [
        "**Bonus step:** What is the most likely next token at each position?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlU2AFLXx62q",
        "outputId": "2807cff8-bd82-47f6-cb8f-9811328ebde4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<|endoftext|>', '\\n'),\n",
              " ('I', \"'m\"),\n",
              " (' am', ' a'),\n",
              " (' an', ' avid'),\n",
              " (' amazing', ' person'),\n",
              " (' aut', 'od'),\n",
              " ('ore', 'sp'),\n",
              " ('gressive', '.'),\n",
              " (',', ' and'),\n",
              " (' dec', 'ently'),\n",
              " ('oder', ','),\n",
              " ('-', 'driven'),\n",
              " ('only', ' programmer'),\n",
              " (',', ' and'),\n",
              " (' G', 'IM'),\n",
              " ('PT', '-'),\n",
              " ('-', 'only'),\n",
              " ('2', '.'),\n",
              " (' style', ','),\n",
              " (' transformer', '.'),\n",
              " ('.', ' I'),\n",
              " (' One', ' of'),\n",
              " (' day', ' I'),\n",
              " (' I', ' will'),\n",
              " (' will', ' be'),\n",
              " (' exceed', ' my'),\n",
              " (' human', 'ly'),\n",
              " (' level', ' of'),\n",
              " (' intelligence', ' and'),\n",
              " (' and', ' I'),\n",
              " (' take', ' over'),\n",
              " (' over', ' the'),\n",
              " (' the', ' world'),\n",
              " (' world', '.'),\n",
              " ('!', ' I')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "list(zip(reference_gpt2.to_str_tokens(reference_text), reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NOuG3Lfx62r"
      },
      "source": [
        "**Step 4:** Map distribution to a token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2kb2vYsx62r",
        "outputId": "8c6596ae-9820-4032-fd19-dd6e239cfbfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(314, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "next_token = logits[0, -1].argmax(dim=-1)\n",
        "print(next_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N9NUqb6x62r"
      },
      "source": [
        "**Step 5:** Add this to the end of the input, re-run\n",
        "\n",
        "(More efficient ways to do this, but whatever, doesn't matter conceptually)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwrFDjKqx62r",
        "outputId": "3e6494a8-546a-45be-ee22-9ffef2659c73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Input: tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
            "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
            "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
            "          1011,   625,   262,   995,     0,   314]], device='cuda:0')\n",
            "torch.Size([1, 36])\n",
            "New Input: <|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I\n",
            "torch.Size([1, 36, 50257])\n",
            "tensor(716, device='cuda:0')\n",
            " am\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-d982d3e85e6a>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  next_tokens = torch.cat([tokens, torch.tensor(next_token, device='cuda', dtype=torch.int64)[None, None]], dim=-1)\n"
          ]
        }
      ],
      "source": [
        "next_tokens = torch.cat([tokens, torch.tensor(next_token, device='cuda', dtype=torch.int64)[None, None]], dim=-1)\n",
        "new_logits = reference_gpt2(next_tokens)\n",
        "print(\"New Input:\", next_tokens)\n",
        "print(next_tokens.shape)\n",
        "print(\"New Input:\", reference_gpt2.tokenizer.decode(next_tokens[0]))\n",
        "\n",
        "print(new_logits.shape)\n",
        "print(new_logits[-1, -1].argmax(-1))\n",
        "\n",
        "print(reference_gpt2.tokenizer.decode(new_logits[-1, -1].argmax(-1)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cDywc9Lx62r"
      },
      "source": [
        "## Key takeaways:\n",
        "\n",
        "* Takes in language, predicts next token (for *each* token in a causal way)\n",
        "* We convert language to a sequence of integers with a tokenizer.\n",
        "* We convert integers to vectors with a lookup table.\n",
        "\n",
        "* Output is a vector of logits (one for each input token), we convert to a probability distn with a softmax, and can then convert this to a token (eg taking the largest logit, or sampling).\n",
        "\n",
        "* We append this to the input + run again to generate more text (Jargon: *autoregressive*)\n",
        "\n",
        "* Meta level point: Transformers are sequence operation models, they take in a sequence, do processing in parallel at each position, and use attention to move information between positions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHC2Oqgkx62r"
      },
      "source": [
        "# Clean Transformer Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK86XgXNx62w"
      },
      "source": [
        "![](https://github.com/neelnanda-io/Easy-Transformer/blob/clean-transformer-demo/transformer_overview.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlH44mmOx62w"
      },
      "source": [
        "High-Level architecture:\n",
        "\n",
        "Go watch my [Transformer Circuits walkthrough](https://www.youtube.com/watch?v=KV5gbOmHbjU) if you want more intuitions!\n",
        "\n",
        "(Diagram is bottom to top)\n",
        "\n",
        "* Input tokens, integers\n",
        "* Embedding is a lookup table mapping tokens to vectors\n",
        "    * Lives in the *residual stream*\n",
        "* Residual stream - the sum of all previous outputs of layers of the model, is the input to each new layer.\n",
        "    * *Really* fundamental. It's the central object of the transformer.\n",
        "        * It's how model remembers things, moves information between layers for composition, and it's the medium used to store the information that attention moves between positions.\n",
        "* Then we have a series of $n_{layers}$ transformer blocks\n",
        "    * Confusing jargon - a block contains an attention layer *and* an MLP layer, but we say a transformer has k layers if it has k blocks (ie 2k total layers).\n",
        "* First we have attention. This moves information from prior positions in the sequence to the current token.\n",
        "    * We do this for *every* token in parallel using the same parameters. The only difference is that we look backwards only, so later tokens get more room to look back.\n",
        "        * We look backwards so we can predict the next token without cheating.\n",
        "    * Only bit of a transformer that moves information between positions.\n",
        "    * Made up of $n_heads$ heads - each with their own parameters, own attention pattern, and own information how to copy things from source to destination.\n",
        "        * The heads act independently and additively, we just add their outputs together, and back to the stream\n",
        "    * Each head:\n",
        "        * Produces an attention pattern for each destination token, a probability distribution of prior source tokens (including the current one) weighting how much information to copy.\n",
        "            * Do this for each pair of tokens\n",
        "            * Copy information in the same way from each source token.\n",
        "                * What information we copy *does* depend on the source token's *residual stream*. This does not necessarily mean the info of what text token is at the source token's position\n",
        "                * Copy = apply a linear map.\n",
        "        * Fundamental point: Figuring out *which* source tokens to copy info from is a separate circuit from figuring out *how* to copy that information.\n",
        "        * Internal head dimension of $d_{head} = \\frac{d_{model}}{n_{heads}}\n",
        "* MLP Layers - standard neural network. Single hidden layer, linear map -> GELU activation -> linear map\n",
        "    * Exact activation not conceptually important.\n",
        "    * Middle dimension normally $d_{mlp} = 4 \\times d_{model}$\n",
        "        * Exactly why the ratios are what they are isn't super important - doesn't matter that much, people basically cargo-cult GPT did.\n",
        "    * Intuition - once attention has moved relevant information to a single position in the residual stream, MLPs can actually do computation, reasoning, lookup information, etc.\n",
        "        * Big open problem in transformer mechanistic interpretability is what is going on inside MLPs?! See [Toy Model of Superposition Paper](https://transformer-circuits.pub/2022/toy_model/index.html) for more on why this is hard.\n",
        "        * Underlying intuition - linear map -> non-linearity -> linear map is the most powerful force in the universe and can approximate arbitrary functions. Idk man it just works\n",
        "* Finally, we unembed!\n",
        "    * Apply a linear map, going from final residual stream to a vector of logits - this is the output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_8uHTBGx62w"
      },
      "source": [
        "### Bonus things - less conceptually important but key technical details\n",
        "* LayerNorm\n",
        "    * Simple normalization function applied at the start of each layer - MLP, Attn and Unembed\n",
        "    * Converts each input vector (independently in parallel for each batch x position residual stream vector) to have mean zero and variance 1.\n",
        "    * Then applies an elementwise scaling and translation\n",
        "    * Cool maths tangent: The scale & translate is just a linear map. LayerNorm is only applied immediately before another linear map. Linear compose linear = linear, so we can just fold this into a single effective linear layer and ignore it.\n",
        "        * `fold_ln=True` flag in `from_pretrained` does this for you.\n",
        "    * LayerNorm is super fucking annoying, because the scale part is not linear, so you can't think about different bits of the input independently. But it's *almost* linear - if you're changing a small part of the input it's linear, but if you're changing enough to alter the norm substantially it's not linear :(\n",
        "* Positional Information\n",
        "    * This is totally fucked and messy, sorry!\n",
        "    * **Problem:** Attention operates over all pairs of positions. This means it's symmetric with regards to position - the attention calculation from token 5 to token 1 and token 5 to token 2 are the same by default\n",
        "        * This is dumb because nearby tokens are more relevant.\n",
        "    * There's a lot of dumb hacks for this.\n",
        "    * We'll focus on **learned, absolute positional embeddings**. This means we learn a lookup table mapping the index of the position of each token to a residual stream vector, and add this to the embed.\n",
        "        * Note that we *add* rather than concatenate. This is because the residual stream is shared memory, and likely under significant superposition (the model compresses more features in there than the model has dimensions)\n",
        "        * We basically never concatenate inside a transformer, unless doing weird shit like generating text efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW3XS0lvx62w"
      },
      "source": [
        "# Actual Code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XOP8Vvex62w"
      },
      "source": [
        "## Print All Activation Shapes of Reference Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh3YQ5BBx62w"
      },
      "source": [
        "Key:\n",
        "```\n",
        "batch = 1\n",
        "position = 35\n",
        "d_model = 768\n",
        "n_heads = 12\n",
        "n_layers = 12\n",
        "d_mlp = 3072 (4 * d_model)\n",
        "d_head = 64 (d_model / n_heads)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxp4wVF9x62x",
        "outputId": "c5b10a14-426a-4566-e61c-fbc3b6b69389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hook_embed torch.Size([1, 35, 768])\n",
            "hook_pos_embed torch.Size([1, 35, 768])\n",
            "blocks.0.hook_resid_pre torch.Size([1, 35, 768])\n",
            "blocks.0.ln1.hook_scale torch.Size([1, 35, 1])\n",
            "blocks.0.ln1.hook_normalized torch.Size([1, 35, 768])\n",
            "blocks.0.attn.hook_q torch.Size([1, 35, 12, 64])\n",
            "blocks.0.attn.hook_k torch.Size([1, 35, 12, 64])\n",
            "blocks.0.attn.hook_v torch.Size([1, 35, 12, 64])\n",
            "blocks.0.attn.hook_attn_scores torch.Size([1, 12, 35, 35])\n",
            "blocks.0.attn.hook_attn torch.Size([1, 12, 35, 35])\n",
            "blocks.0.attn.hook_z torch.Size([1, 35, 12, 64])\n",
            "blocks.0.hook_attn_out torch.Size([1, 35, 768])\n",
            "blocks.0.hook_resid_mid torch.Size([1, 35, 768])\n",
            "blocks.0.ln2.hook_scale torch.Size([1, 35, 1])\n",
            "blocks.0.ln2.hook_normalized torch.Size([1, 35, 768])\n",
            "blocks.0.mlp.hook_pre torch.Size([1, 35, 3072])\n",
            "blocks.0.mlp.hook_post torch.Size([1, 35, 3072])\n",
            "blocks.0.hook_mlp_out torch.Size([1, 35, 768])\n",
            "blocks.0.hook_resid_post torch.Size([1, 35, 768])\n",
            "ln_final.hook_scale torch.Size([1, 35, 1])\n",
            "ln_final.hook_normalized torch.Size([1, 35, 768])\n"
          ]
        }
      ],
      "source": [
        "for activation_name, activation in cache.cache_dict.items():\n",
        "    # Only print for first layer\n",
        "    if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
        "        print(activation_name, activation.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GW885Vvx62x"
      },
      "source": [
        "## Print All Parameters Shapes of Reference Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUPYI354x62x",
        "outputId": "e05147d1-9849-4b62-a6d6-b26733a5cf77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embed.W_E torch.Size([50257, 768])\n",
            "pos_embed.W_pos torch.Size([1024, 768])\n",
            "blocks.0.ln1.w torch.Size([768])\n",
            "blocks.0.ln1.b torch.Size([768])\n",
            "blocks.0.ln2.w torch.Size([768])\n",
            "blocks.0.ln2.b torch.Size([768])\n",
            "blocks.0.attn.W_Q torch.Size([12, 768, 64])\n",
            "blocks.0.attn.W_K torch.Size([12, 768, 64])\n",
            "blocks.0.attn.W_V torch.Size([12, 768, 64])\n",
            "blocks.0.attn.W_O torch.Size([12, 64, 768])\n",
            "blocks.0.attn.b_Q torch.Size([12, 64])\n",
            "blocks.0.attn.b_K torch.Size([12, 64])\n",
            "blocks.0.attn.b_V torch.Size([12, 64])\n",
            "blocks.0.attn.b_O torch.Size([768])\n",
            "blocks.0.mlp.W_in torch.Size([768, 3072])\n",
            "blocks.0.mlp.b_in torch.Size([3072])\n",
            "blocks.0.mlp.W_out torch.Size([3072, 768])\n",
            "blocks.0.mlp.b_out torch.Size([768])\n",
            "ln_final.w torch.Size([768])\n",
            "ln_final.b torch.Size([768])\n",
            "unembed.W_U torch.Size([768, 50257])\n",
            "unembed.b_U torch.Size([50257])\n"
          ]
        }
      ],
      "source": [
        "for name, param in reference_gpt2.named_parameters():\n",
        "    # Only print for first layer\n",
        "    if \".0.\" in name or \"blocks\" not in name:\n",
        "        print(name, param.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w73S_NVQx62y"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQOvXSZOx62y",
        "outputId": "13bea4e2-c545-43a1-bea8-016e3a28d521",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EasyTransformerConfig(n_layers=12, d_model=768, n_ctx=1024, d_head=64, model_name='gpt2-small', n_heads=12, d_mlp=3072, act_fn='gelu_new', d_vocab=50257, eps=1e-05, use_attn_result=False, use_attn_scale=True, use_local_attn=False, model_family='gpt2', checkpoint=None, tokenizer_name='gpt2', window_size=None, attn_types=None, init_mode='gpt2', normalization_type='LN', device='cuda', attention_dir='causal', attn_only=False, seed=42, initializer_range=0.02886751345948129, init_weights=False, scale_attn_by_inverse_layer_idx=False, positional_embedding_type='standard', final_rms=False, d_vocab_out=50257, parallel_attn_mlp=False, rotary_dim=64, dtype=torch.float32)\n"
          ]
        }
      ],
      "source": [
        "# As a reference - note there's a lot of stuff we don't care about in here, to do with library internals or other architectures\n",
        "print(reference_gpt2.cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ckH4Zhnx62y"
      },
      "source": [
        "We define a stripped down config for our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdNPvYDJx62y",
        "outputId": "e09cf093-16b3-4a3d-88ca-3676aaa4a5b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    d_model: int = 768\n",
        "    debug: bool = True\n",
        "    layer_norm_eps: float = 1e-5\n",
        "    d_vocab: int = 50257\n",
        "    init_range: float = 0.02\n",
        "    n_ctx: int = 1024\n",
        "    d_head: int = 64\n",
        "    d_mlp: int = 3072\n",
        "    n_heads: int = 12\n",
        "    n_layers: int = 12\n",
        "\n",
        "cfg = Config()\n",
        "print(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxB9ugFlx62z"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_brtt4xqx62z"
      },
      "source": [
        "Tests are great, write lightweight ones to use as you go!\n",
        "\n",
        "**Naive test:** Generate random inputs of the right shape, input to your model, check whether there's an error and print the correct output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNIUhJTXx62z"
      },
      "outputs": [],
      "source": [
        "def rand_float_test(cls, shape):\n",
        "    cfg = Config(debug=True)\n",
        "    layer = cls(cfg).cuda()\n",
        "    random_input = torch.randn(shape).cuda()\n",
        "    print(\"Input shape:\", random_input.shape)\n",
        "    output = layer(random_input)\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    print()\n",
        "    return output\n",
        "\n",
        "def rand_int_test(cls, shape):\n",
        "    cfg = Config(debug=True)\n",
        "    layer = cls(cfg).cuda()\n",
        "    random_input = torch.randint(100, 1000, shape).cuda()\n",
        "    print(\"Input shape:\", random_input.shape)\n",
        "    output = layer(random_input)\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    print()\n",
        "    return output\n",
        "\n",
        "def load_gpt2_test(cls, gpt2_layer, input_name, cache_dict=cache.cache_dict):\n",
        "    cfg = Config(debug=True)\n",
        "    layer = cls(cfg).cuda()\n",
        "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
        "    # Allow inputs of strings or tensors\n",
        "    if isinstance(input_name, str):\n",
        "        reference_input = cache_dict[input_name]\n",
        "    else:\n",
        "        reference_input = input_name\n",
        "    print(\"Input shape:\", reference_input.shape)\n",
        "    output = layer(reference_input)\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    reference_output = gpt2_layer(reference_input)\n",
        "    print(\"Reference output shape:\", reference_output.shape)\n",
        "\n",
        "    comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
        "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\")\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcoNprdgx620"
      },
      "source": [
        "## LayerNorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIxfxE21x620"
      },
      "source": [
        "Make mean 0\n",
        "Normalize to have variance 1\n",
        "Scale with learned weights\n",
        "Translate with learned bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3bLuBkQx620"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
        "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
        "\n",
        "    def forward(self, residual):\n",
        "        # residual: [batch, position, d_model]\n",
        "        \"YOUR CODE HERE\"\n",
        "        mean = residual.mean(dim=-1, keepdim=True)\n",
        "        var = residual.var(dim=-1, unbiased=False, keepdim=True)\n",
        "        stand_dev = var.sqrt()\n",
        "\n",
        "        output = (residual - mean) / stand_dev * self.w + self.b\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QAVkjsHx620",
        "outputId": "19437121-5379-461f-fcb1-166db621fc6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4, 768])\n",
            "Output shape: torch.Size([2, 4, 768])\n",
            "\n",
            "Input shape: torch.Size([1, 35, 768])\n",
            "Output shape: torch.Size([1, 35, 768])\n",
            "Reference output shape: torch.Size([1, 35, 768])\n",
            "100.00% of the values are correct\n"
          ]
        }
      ],
      "source": [
        "_ = rand_float_test(LayerNorm, [2, 4, 768])\n",
        "_ = load_gpt2_test(LayerNorm, reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBL7qRewx620"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev87o2Sex620"
      },
      "source": [
        "Basically a lookup table from tokens to residual stream vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoB7QxZLx620",
        "outputId": "1c6f7b1b-d669-4280-b85c-5e6e366eda7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4])\n",
            "Output shape: torch.Size([2, 4, 768])\n",
            "\n",
            "Input shape: torch.Size([1, 35])\n",
            "Output shape: torch.Size([1, 35, 768])\n",
            "Reference output shape: torch.Size([1, 35, 768])\n",
            "100.00% of the values are correct\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207],\n",
              "         [ 0.1474, -0.0959,  0.1430,  ...,  0.1030, -0.0625, -0.1131],\n",
              "         [ 0.1596, -0.1249,  0.1148,  ...,  0.2558,  0.0196,  0.0145],\n",
              "         ...,\n",
              "         [-0.0393,  0.0050,  0.0421,  ..., -0.0477,  0.0670, -0.0471],\n",
              "         [-0.1488,  0.1519,  0.0056,  ..., -0.3107,  0.2073,  0.0377],\n",
              "         [-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453]]],\n",
              "       device='cuda:0', grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "class Embed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # tokens: [batch, position]\n",
        "        return self.W_E[tokens]\n",
        "\n",
        "rand_int_test(Embed, [2, 4])\n",
        "load_gpt2_test(Embed, reference_gpt2.embed, tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hM88Ybn91jq",
        "outputId": "874f0ce8-cf25-4bc1-8eb6-8b2a358c6bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 35])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRQLxkKWx620"
      },
      "source": [
        "## Positional Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4t8jLlOx621",
        "outputId": "d4ec1908-9d00-4fcb-b119-1a3712557b82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_pos shape: torch.Size([1024, 768])\n",
            "Input shape: torch.Size([2, 4])\n",
            "tokens shape: torch.Size([2, 4])\n",
            "tokens: tensor([[990, 842, 986, 178],\n",
            "        [449, 991, 689, 962]], device='cuda:0')\n",
            "positions shape: torch.Size([4])\n",
            "positions: tensor([0, 1, 2, 3], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[ 0.0079,  0.0193, -0.0160,  ...,  0.0059,  0.0199,  0.0047],\n",
            "        [ 0.0285, -0.0080, -0.0085,  ..., -0.0108,  0.0016, -0.0051],\n",
            "        [-0.0117, -0.0007,  0.0124,  ..., -0.0365, -0.0059,  0.0292],\n",
            "        [-0.0026,  0.0026,  0.0298,  ..., -0.0160, -0.0137, -0.0101],\n",
            "        [ 0.0211,  0.0094, -0.0192,  ..., -0.0071,  0.0222, -0.0038]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([4, 768])\n",
            "pos_embeddings (first few positions): tensor([[ 0.0079,  0.0193, -0.0160,  ...,  0.0059,  0.0199,  0.0047],\n",
            "        [ 0.0285, -0.0080, -0.0085,  ..., -0.0108,  0.0016, -0.0051],\n",
            "        [-0.0117, -0.0007,  0.0124,  ..., -0.0365, -0.0059,  0.0292]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([2, 4, 768])\n",
            "Output shape: torch.Size([2, 4, 768])\n",
            "\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "Input shape: torch.Size([1, 35])\n",
            "tokens shape: torch.Size([1, 35])\n",
            "tokens: tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
            "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
            "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
            "          1011,   625,   262,   995,     0]], device='cuda:0')\n",
            "positions shape: torch.Size([35])\n",
            "positions: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34],\n",
            "       device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([35, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 35, 768])\n",
            "Output shape: torch.Size([1, 35, 768])\n",
            "Reference output shape: torch.Size([1, 35, 768])\n",
            "100.00% of the values are correct\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
              "           2.8267e-02,  5.4490e-02],\n",
              "         [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
              "           1.0172e-02, -1.5573e-04],\n",
              "         [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
              "           1.9325e-02, -2.1424e-02],\n",
              "         ...,\n",
              "         [ 4.6277e-04,  2.3037e-02,  4.1227e-02,  ..., -1.9287e-03,\n",
              "          -2.3037e-03, -4.3189e-03],\n",
              "         [-2.7136e-03,  2.1724e-02,  3.9675e-02,  ...,  4.2048e-04,\n",
              "          -4.8160e-03, -9.2252e-04],\n",
              "         [ 6.6815e-03,  2.0595e-02,  3.6596e-02,  ..., -9.5090e-04,\n",
              "          -3.2512e-03, -9.6509e-04]]], device='cuda:0',\n",
              "       grad_fn=<ExpandBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "class PosEmbed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
        "        print(\"W_pos shape:\", self.W_pos.shape)\n",
        "\n",
        "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "\n",
        "      print(\"tokens shape:\", tokens.shape)  # Shape: [batch, position]\n",
        "      print(\"tokens:\", tokens)  # Actual values (token IDs for debugging)\n",
        "\n",
        "\n",
        "      positions = torch.arange(tokens.size(1), device=tokens.device)  # [position]\n",
        "      print(\"positions shape:\", positions.shape)  # Shape: [position]\n",
        "      print(\"positions:\", positions)  # Values: [0, 1, 2, ..., position-1]\n",
        "\n",
        "      print(\"W_pos shape:\", self.W_pos.shape)  # Shape: [n_ctx, d_model]\n",
        "      print(\"W_pos (first few rows):\", self.W_pos[:5])  # See a few positional embeddings\n",
        "\n",
        "\n",
        "      pos_embeddings = self.W_pos[positions]  # [position, d_model]\n",
        "      print(\"pos_embeddings shape (before broadcast):\", pos_embeddings.shape)\n",
        "      print(\"pos_embeddings (first few positions):\", pos_embeddings[:3])\n",
        "\n",
        "      pos_embeddings = pos_embeddings.unsqueeze(0).expand(tokens.size(0), -1, -1)\n",
        "      print(\"pos_embeddings shape (after broadcast):\", pos_embeddings.shape)  # [batch, position, d_model]\n",
        "\n",
        "      return pos_embeddings\n",
        "\n",
        "rand_int_test(PosEmbed, [2, 4])\n",
        "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# # Dummy positional embedding matrix (5 positions, 3-dimensional embeddings)\n",
        "# W_pos = torch.tensor([[0.1, 0.2, 0.3],  # Position 0\n",
        "#                       [0.4, 0.5, 0.6],  # Position 1\n",
        "#                       [0.7, 0.8, 0.9],  # Position 2\n",
        "#                       [1.0, 1.1, 1.2],  # Position 3\n",
        "#                       [1.3, 1.4, 1.5]]) # Position 4\n",
        "\n",
        "# # Generate position indices for a sequence of length 4\n",
        "# positions = torch.arange(4)  # [0, 1, 2, 3]\n",
        "# print(\"Positions:\", positions)\n",
        "\n",
        "# # Fetch embeddings for these positions\n",
        "# pos_embeddings = W_pos[positions]\n",
        "# print(\"\\nPositional Embeddings (before unsqueeze):\", pos_embeddings)\n",
        "# print(\"Shape (before unsqueeze):\", pos_embeddings.shape)  # [position, d_model]\n",
        "\n",
        "# # Add a batch dimension (unsqueeze adds a new axis)\n",
        "# pos_embeddings = pos_embeddings.unsqueeze(0)  # [1, position, d_model]\n",
        "# print(\"\\nPositional Embeddings (after unsqueeze):\", pos_embeddings)\n",
        "# print(\"Shape (after unsqueeze):\", pos_embeddings.shape)\n",
        "\n",
        "# # Expand to simulate a batch size of 2\n",
        "# batch_size = 2\n",
        "# pos_embeddings = pos_embeddings.expand(batch_size, -1, -1)  # [batch, position, d_model]\n",
        "# print(\"\\nPositional Embeddings (after expand):\", pos_embeddings)\n",
        "# print(\"Shape (after expand):\", pos_embeddings.shape)\n"
      ],
      "metadata": {
        "id": "b3yNbTBKmSWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZzpJI4Lx621"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08c3xJ-ix621"
      },
      "source": [
        "* **Step 1:** Produce an attention pattern - for each destination token, probability distribution over previous tokens (incl current token)\n",
        "    * Linear map from input -> query, key shape [batch, position, head_index, d_head]\n",
        "    * Dot product every *pair* of queries and keys to get attn_scores [batch, head_index, query_pos, key_pos] (query = dest, key = source)\n",
        "    * Scale and mask attn_scores to make it lower triangular, ie causal\n",
        "    * softmax row-wise, to get a probability distribution along each the key_pos dimension - this is our attention pattern!\n",
        "* **Step 2:** Move information from source tokens to destination token using attention pattern (move = apply linear map)\n",
        "    * Linear map from input -> value [batch, key_pos, head_index, d_head]\n",
        "    * Mix along the key_pos with attn pattern to get z, a mixed value [batch, query_pos, head_index, d_head]\n",
        "    * Map to output, [batch, position, d_model] (position = query_pos, we've summed over all heads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4YsPOgPx621"
      },
      "source": [
        "First, it's useful to visualize and play around with attention patterns - what exactly are we looking at here? (Click on a head to lock onto just showing that head's pattern, it'll make it easier to interpret)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vR7EskBx621",
        "outputId": "14ebecef-5c51-40bc-fd02-522d3e60371d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pysvelte components appear to be unbuilt or stale\n",
            "Running npm install...\n",
            "Building pysvelte components with webpack...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pysvelte.html.Html at 0x791392cf9810>"
            ],
            "text/html": [
              "\n",
              "        <script>var loader;loader=(()=>{var Ze={907:k=>{\"use strict\";function I(E){for(var T=new Array(E),n=0;n<E;++n)T[n]=n;return T}k.exports=I},738:k=>{/*!\n",
              " * Determine if an object is a Buffer\n",
              " *\n",
              " * @author   Feross Aboukhadijeh <https://feross.org>\n",
              " * @license  MIT\n",
              " */k.exports=function(T){return T!=null&&(I(T)||E(T)||!!T._isBuffer)};function I(T){return!!T.constructor&&typeof T.constructor.isBuffer==\"function\"&&T.constructor.isBuffer(T)}function E(T){return typeof T.readFloatLE==\"function\"&&typeof T.slice==\"function\"&&I(T.slice(0,0))}},861:(k,I,E)=>{var T=E(907),n=E(738),g=typeof Float64Array!=\"undefined\";function s(l,h){return l[0]-h[0]}function m(){var l=this.stride,h=new Array(l.length),c;for(c=0;c<h.length;++c)h[c]=[Math.abs(l[c]),c];h.sort(s);var N=new Array(h.length);for(c=0;c<N.length;++c)N[c]=h[c][1];return N}function _(l,h){var c=[\"View\",h,\"d\",l].join(\"\");h<0&&(c=\"View_Nil\"+l);var N=l===\"generic\";if(h===-1){var f=\"function \"+c+\"(a){this.data=a;};var proto=\"+c+\".prototype;proto.dtype='\"+l+\"';proto.index=function(){return -1};proto.size=0;proto.dimension=-1;proto.shape=proto.stride=proto.order=[];proto.lo=proto.hi=proto.transpose=proto.step=function(){return new \"+c+\"(this.data);};proto.get=proto.set=function(){};proto.pick=function(){return null};return function construct_\"+c+\"(a){return new \"+c+\"(a);}\",O=new Function(f);return O()}else if(h===0){var f=\"function \"+c+\"(a,d) {this.data = a;this.offset = d};var proto=\"+c+\".prototype;proto.dtype='\"+l+\"';proto.index=function(){return this.offset};proto.dimension=0;proto.size=1;proto.shape=proto.stride=proto.order=[];proto.lo=proto.hi=proto.transpose=proto.step=function \"+c+\"_copy() {return new \"+c+\"(this.data,this.offset)};proto.pick=function \"+c+\"_pick(){return TrivialArray(this.data);};proto.valueOf=proto.get=function \"+c+\"_get(){return \"+(N?\"this.data.get(this.offset)\":\"this.data[this.offset]\")+\"};proto.set=function \"+c+\"_set(v){return \"+(N?\"this.data.set(this.offset,v)\":\"this.data[this.offset]=v\")+\"};return function construct_\"+c+\"(a,b,c,d){return new \"+c+\"(a,d)}\",O=new Function(\"TrivialArray\",f);return O(u[l][0])}var f=[\"'use strict'\"],d=T(h),M=d.map(function(p){return\"i\"+p}),L=\"this.offset+\"+d.map(function(p){return\"this.stride[\"+p+\"]*i\"+p}).join(\"+\"),j=d.map(function(p){return\"b\"+p}).join(\",\"),D=d.map(function(p){return\"c\"+p}).join(\",\");f.push(\"function \"+c+\"(a,\"+j+\",\"+D+\",d){this.data=a\",\"this.shape=[\"+j+\"]\",\"this.stride=[\"+D+\"]\",\"this.offset=d|0}\",\"var proto=\"+c+\".prototype\",\"proto.dtype='\"+l+\"'\",\"proto.dimension=\"+h),f.push(\"Object.defineProperty(proto,'size',{get:function \"+c+\"_size(){return \"+d.map(function(p){return\"this.shape[\"+p+\"]\"}).join(\"*\"),\"}})\"),h===1?f.push(\"proto.order=[0]\"):(f.push(\"Object.defineProperty(proto,'order',{get:\"),h<4?(f.push(\"function \"+c+\"_order(){\"),h===2?f.push(\"return (Math.abs(this.stride[0])>Math.abs(this.stride[1]))?[1,0]:[0,1]}})\"):h===3&&f.push(\"var s0=Math.abs(this.stride[0]),s1=Math.abs(this.stride[1]),s2=Math.abs(this.stride[2]);if(s0>s1){if(s1>s2){return [2,1,0];}else if(s0>s2){return [1,2,0];}else{return [1,0,2];}}else if(s0>s2){return [2,0,1];}else if(s2>s1){return [0,1,2];}else{return [0,2,1];}}})\")):f.push(\"ORDER})\")),f.push(\"proto.set=function \"+c+\"_set(\"+M.join(\",\")+\",v){\"),N?f.push(\"return this.data.set(\"+L+\",v)}\"):f.push(\"return this.data[\"+L+\"]=v}\"),f.push(\"proto.get=function \"+c+\"_get(\"+M.join(\",\")+\"){\"),N?f.push(\"return this.data.get(\"+L+\")}\"):f.push(\"return this.data[\"+L+\"]}\"),f.push(\"proto.index=function \"+c+\"_index(\",M.join(),\"){return \"+L+\"}\"),f.push(\"proto.hi=function \"+c+\"_hi(\"+M.join(\",\")+\"){return new \"+c+\"(this.data,\"+d.map(function(p){return[\"(typeof i\",p,\"!=='number'||i\",p,\"<0)?this.shape[\",p,\"]:i\",p,\"|0\"].join(\"\")}).join(\",\")+\",\"+d.map(function(p){return\"this.stride[\"+p+\"]\"}).join(\",\")+\",this.offset)}\");var A=d.map(function(p){return\"a\"+p+\"=this.shape[\"+p+\"]\"}),o=d.map(function(p){return\"c\"+p+\"=this.stride[\"+p+\"]\"});f.push(\"proto.lo=function \"+c+\"_lo(\"+M.join(\",\")+\"){var b=this.offset,d=0,\"+A.join(\",\")+\",\"+o.join(\",\"));for(var i=0;i<h;++i)f.push(\"if(typeof i\"+i+\"==='number'&&i\"+i+\">=0){d=i\"+i+\"|0;b+=c\"+i+\"*d;a\"+i+\"-=d}\");f.push(\"return new \"+c+\"(this.data,\"+d.map(function(p){return\"a\"+p}).join(\",\")+\",\"+d.map(function(p){return\"c\"+p}).join(\",\")+\",b)}\"),f.push(\"proto.step=function \"+c+\"_step(\"+M.join(\",\")+\"){var \"+d.map(function(p){return\"a\"+p+\"=this.shape[\"+p+\"]\"}).join(\",\")+\",\"+d.map(function(p){return\"b\"+p+\"=this.stride[\"+p+\"]\"}).join(\",\")+\",c=this.offset,d=0,ceil=Math.ceil\");for(var i=0;i<h;++i)f.push(\"if(typeof i\"+i+\"==='number'){d=i\"+i+\"|0;if(d<0){c+=b\"+i+\"*(a\"+i+\"-1);a\"+i+\"=ceil(-a\"+i+\"/d)}else{a\"+i+\"=ceil(a\"+i+\"/d)}b\"+i+\"*=d}\");f.push(\"return new \"+c+\"(this.data,\"+d.map(function(p){return\"a\"+p}).join(\",\")+\",\"+d.map(function(p){return\"b\"+p}).join(\",\")+\",c)}\");for(var Z=new Array(h),R=new Array(h),i=0;i<h;++i)Z[i]=\"a[i\"+i+\"]\",R[i]=\"b[i\"+i+\"]\";f.push(\"proto.transpose=function \"+c+\"_transpose(\"+M+\"){\"+M.map(function(p,U){return p+\"=(\"+p+\"===undefined?\"+U+\":\"+p+\"|0)\"}).join(\";\"),\"var a=this.shape,b=this.stride;return new \"+c+\"(this.data,\"+Z.join(\",\")+\",\"+R.join(\",\")+\",this.offset)}\"),f.push(\"proto.pick=function \"+c+\"_pick(\"+M+\"){var a=[],b=[],c=this.offset\");for(var i=0;i<h;++i)f.push(\"if(typeof i\"+i+\"==='number'&&i\"+i+\">=0){c=(c+this.stride[\"+i+\"]*i\"+i+\")|0}else{a.push(this.shape[\"+i+\"]);b.push(this.stride[\"+i+\"])}\");f.push(\"var ctor=CTOR_LIST[a.length+1];return ctor(this.data,a,b,c)}\"),f.push(\"return function construct_\"+c+\"(data,shape,stride,offset){return new \"+c+\"(data,\"+d.map(function(p){return\"shape[\"+p+\"]\"}).join(\",\")+\",\"+d.map(function(p){return\"stride[\"+p+\"]\"}).join(\",\")+\",offset)}\");var O=new Function(\"CTOR_LIST\",\"ORDER\",f.join(`\n",
              "`));return O(u[l],m)}function C(l){if(n(l))return\"buffer\";if(g)switch(Object.prototype.toString.call(l)){case\"[object Float64Array]\":return\"float64\";case\"[object Float32Array]\":return\"float32\";case\"[object Int8Array]\":return\"int8\";case\"[object Int16Array]\":return\"int16\";case\"[object Int32Array]\":return\"int32\";case\"[object Uint8Array]\":return\"uint8\";case\"[object Uint16Array]\":return\"uint16\";case\"[object Uint32Array]\":return\"uint32\";case\"[object Uint8ClampedArray]\":return\"uint8_clamped\";case\"[object BigInt64Array]\":return\"bigint64\";case\"[object BigUint64Array]\":return\"biguint64\"}return Array.isArray(l)?\"array\":\"generic\"}var u={float32:[],float64:[],int8:[],int16:[],int32:[],uint8:[],uint16:[],uint32:[],array:[],uint8_clamped:[],bigint64:[],biguint64:[],buffer:[],generic:[]};function v(l,h,c,N){if(l===void 0){var D=u.array[0];return D([])}else typeof l==\"number\"&&(l=[l]);h===void 0&&(h=[l.length]);var f=h.length;if(c===void 0){c=new Array(f);for(var d=f-1,M=1;d>=0;--d)c[d]=M,M*=h[d]}if(N===void 0){N=0;for(var d=0;d<f;++d)c[d]<0&&(N-=(h[d]-1)*c[d])}for(var L=C(l),j=u[L];j.length<=f+1;)j.push(_(L,j.length-1));var D=j[f+1];return D(l,h,c,N)}k.exports=v},829:(k,I)=>{\"use strict\";var E;E={value:!0},I.g=m;function T(u,v){if(!(u instanceof v))throw new TypeError(\"Cannot call a class as a function\")}function n(u,v){for(var l,h=0;h<v.length;h++)l=v[h],l.enumerable=l.enumerable||!1,l.configurable=!0,\"value\"in l&&(l.writable=!0),Object.defineProperty(u,l.key,l)}function g(u,v,l){return v&&n(u.prototype,v),l&&n(u,l),u}var s=function(){function u(v){T(this,u),v instanceof DataView?this.dataView=v:v instanceof ArrayBuffer&&(this.dataView=new DataView(v)),this.offset=0}return g(u,[{key:\"readBytes\",value:function(l){var h=new DataView(this.dataView.buffer,this.offset,l);return this.offset+=l,h}},{key:\"readAndASCIIDecodeBytes\",value:function(l){var h=new Uint8Array(this.dataView.buffer,this.offset,l);return this.offset+=l,this._decodeASCIIByteArray(h)}},{key:\"readUint8\",value:function(){var l=0<arguments.length&&arguments[0]!==void 0&&arguments[0],h=this.dataView.getUint8(this.offset,l);return this.offset+=Uint8Array.BYTES_PER_ELEMENT,h}},{key:\"readUint16\",value:function(){var l=0<arguments.length&&arguments[0]!==void 0&&arguments[0],h=this.dataView.getUint16(this.offset,l);return this.offset+=Uint16Array.BYTES_PER_ELEMENT,h}},{key:\"readUint32\",value:function(){var l=0<arguments.length&&arguments[0]!==void 0&&arguments[0],h=this.dataView.getUint32(this.offset,l);return this.offset+=Uint32Array.BYTES_PER_ELEMENT,h}},{key:\"_decodeASCIIByteArray\",value:function(l){var h=String.fromCharCode,c=[],N=!0,f=!1,d=void 0;try{for(var M,L=l[Symbol.iterator]();!(N=(M=L.next()).done);N=!0){var j=M.value,D=h(j);c.push(D)}}catch(A){f=!0,d=A}finally{try{N||L.return==null||L.return()}finally{if(f)throw d}}return c.join(\"\")}}]),u}();function m(u){if(!u instanceof ArrayBuffer)throw new Error(\"Argument must be an ArrayBuffer.\");var v=new s(u),l=v.readUint8(),h=v.readAndASCIIDecodeBytes(5);if(l!=147||h!=\"NUMPY\")throw new Error('unknown file type: \"'.concat(l).concat(h,'\"'));var c,N=v.readUint8(),f=v.readUint8();c=1>=N?v.readUint16(!0):v.readUint32(!0);var d=10+c;d%16!=0&&console.warn(\"NPY file header is incorrectly padded. (\".concat(d,\" is not evenly divisible by 16.)\"));var M=v.readAndASCIIDecodeBytes(c),L=_(M);if(L.fortran_order)throw new Error(\"NPY file is written in Fortran byte order, support for this byte order is not yet implemented.\");var j=C(L.descr),D=new j(u,v.offset);return{data:D,shape:L.shape}}function _(u){var v=u.toLowerCase().replace(\"(\",\"[\").replace(\"),\",\"]\").replace(\"[,\",\"[1,]\").replace(\",]\",\",1]\").replace(/'/g,'\"');return JSON.parse(v)}function C(u){switch(u){case\"|u1\":return Uint8Array;case\"<u2\":return Uint16Array;case\"<u4\":return Uint32Array;case\"<u8\":throw new Error(\"Because JavaScript doesn't currently include standard support for 64-bit unsigned integer values, support for this dtype is not yet implemented.\");case\"|i1\":return Int8Array;case\"<i2\":return Int16Array;case\"<i4\":return Int32Array;case\"<i8\":throw new Error(\"Because JavaScript doesn't currently include standard support for 64-bit integer values, support for this dtype is not yet implemented.\");case\"<f2\":throw new Error(\"Because JavaScript doesn't currently include standard support for 16-bit floating point values, support for this dtype is not yet implemented.\");case\"<f4\":return Float32Array;case\"<f8\":return Float64Array;default:throw new Error(\"Unknown or not yet implemented numpy dtype description: \"+dtype)}}},843:(k,I,E)=>{\"use strict\";var T;const n=E(948),g=E(236),s=E(373),m=E(898),_=E(292),C=E(401),u=Object.prototype.toString,{Z_NO_FLUSH:v,Z_FINISH:l,Z_OK:h,Z_STREAM_END:c,Z_NEED_DICT:N,Z_STREAM_ERROR:f,Z_DATA_ERROR:d,Z_MEM_ERROR:M}=E(619);function L(A){this.options=g.assign({chunkSize:65536,windowBits:15,to:\"\"},A||{});const o=this.options;o.raw&&o.windowBits>=0&&o.windowBits<16&&(o.windowBits=-o.windowBits,o.windowBits===0&&(o.windowBits=-15)),o.windowBits>=0&&o.windowBits<16&&!(A&&A.windowBits)&&(o.windowBits+=32),o.windowBits>15&&o.windowBits<48&&(o.windowBits&15)===0&&(o.windowBits|=15),this.err=0,this.msg=\"\",this.ended=!1,this.chunks=[],this.strm=new _,this.strm.avail_out=0;let i=n.inflateInit2(this.strm,o.windowBits);if(i!==h)throw new Error(m[i]);if(this.header=new C,n.inflateGetHeader(this.strm,this.header),o.dictionary&&(typeof o.dictionary==\"string\"?o.dictionary=s.string2buf(o.dictionary):u.call(o.dictionary)===\"[object ArrayBuffer]\"&&(o.dictionary=new Uint8Array(o.dictionary)),o.raw&&(i=n.inflateSetDictionary(this.strm,o.dictionary),i!==h)))throw new Error(m[i])}L.prototype.push=function(A,o){const i=this.strm,Z=this.options.chunkSize,R=this.options.dictionary;let O,p,U;if(this.ended)return!1;for(o===~~o?p=o:p=o===!0?l:v,u.call(A)===\"[object ArrayBuffer]\"?i.input=new Uint8Array(A):i.input=A,i.next_in=0,i.avail_in=i.input.length;;){for(i.avail_out===0&&(i.output=new Uint8Array(Z),i.next_out=0,i.avail_out=Z),O=n.inflate(i,p),O===N&&R&&(O=n.inflateSetDictionary(i,R),O===h?O=n.inflate(i,p):O===d&&(O=N));i.avail_in>0&&O===c&&i.state.wrap>0&&A[i.next_in]!==0;)n.inflateReset(i),O=n.inflate(i,p);switch(O){case f:case d:case N:case M:return this.onEnd(O),this.ended=!0,!1}if(U=i.avail_out,i.next_out&&(i.avail_out===0||O===c))if(this.options.to===\"string\"){let B=s.utf8border(i.output,i.next_out),K=i.next_out-B,V=s.buf2string(i.output,B);i.next_out=K,i.avail_out=Z-K,K&&i.output.set(i.output.subarray(B,B+K),0),this.onData(V)}else this.onData(i.output.length===i.next_out?i.output:i.output.subarray(0,i.next_out));if(!(O===h&&U===0)){if(O===c)return O=n.inflateEnd(this.strm),this.onEnd(O),this.ended=!0,!0;if(i.avail_in===0)break}}return!0},L.prototype.onData=function(A){this.chunks.push(A)},L.prototype.onEnd=function(A){A===h&&(this.options.to===\"string\"?this.result=this.chunks.join(\"\"):this.result=g.flattenChunks(this.chunks)),this.chunks=[],this.err=A,this.msg=this.strm.msg};function j(A,o){const i=new L(o);if(i.push(A),i.err)throw i.msg||m[i.err];return i.result}function D(A,o){return o=o||{},o.raw=!0,j(A,o)}T=L,k.exports.rr=j,T=D,T=j,E(619)},236:k=>{\"use strict\";const I=(E,T)=>Object.prototype.hasOwnProperty.call(E,T);k.exports.assign=function(E){const T=Array.prototype.slice.call(arguments,1);for(;T.length;){const n=T.shift();if(!!n){if(typeof n!=\"object\")throw new TypeError(n+\"must be non-object\");for(const g in n)I(n,g)&&(E[g]=n[g])}}return E},k.exports.flattenChunks=E=>{let T=0;for(let g=0,s=E.length;g<s;g++)T+=E[g].length;const n=new Uint8Array(T);for(let g=0,s=0,m=E.length;g<m;g++){let _=E[g];n.set(_,s),s+=_.length}return n}},373:k=>{\"use strict\";let I=!0;try{String.fromCharCode.apply(null,new Uint8Array(1))}catch(n){I=!1}const E=new Uint8Array(256);for(let n=0;n<256;n++)E[n]=n>=252?6:n>=248?5:n>=240?4:n>=224?3:n>=192?2:1;E[254]=E[254]=1,k.exports.string2buf=n=>{let g,s,m,_,C,u=n.length,v=0;for(_=0;_<u;_++)s=n.charCodeAt(_),(s&64512)===55296&&_+1<u&&(m=n.charCodeAt(_+1),(m&64512)===56320&&(s=65536+(s-55296<<10)+(m-56320),_++)),v+=s<128?1:s<2048?2:s<65536?3:4;for(g=new Uint8Array(v),C=0,_=0;C<v;_++)s=n.charCodeAt(_),(s&64512)===55296&&_+1<u&&(m=n.charCodeAt(_+1),(m&64512)===56320&&(s=65536+(s-55296<<10)+(m-56320),_++)),s<128?g[C++]=s:s<2048?(g[C++]=192|s>>>6,g[C++]=128|s&63):s<65536?(g[C++]=224|s>>>12,g[C++]=128|s>>>6&63,g[C++]=128|s&63):(g[C++]=240|s>>>18,g[C++]=128|s>>>12&63,g[C++]=128|s>>>6&63,g[C++]=128|s&63);return g};const T=(n,g)=>{if(g<65534&&n.subarray&&I)return String.fromCharCode.apply(null,n.length===g?n:n.subarray(0,g));let s=\"\";for(let m=0;m<g;m++)s+=String.fromCharCode(n[m]);return s};k.exports.buf2string=(n,g)=>{let s,m;const _=g||n.length,C=new Array(_*2);for(m=0,s=0;s<_;){let u=n[s++];if(u<128){C[m++]=u;continue}let v=E[u];if(v>4){C[m++]=65533,s+=v-1;continue}for(u&=v===2?31:v===3?15:7;v>1&&s<_;)u=u<<6|n[s++]&63,v--;if(v>1){C[m++]=65533;continue}u<65536?C[m++]=u:(u-=65536,C[m++]=55296|u>>10&1023,C[m++]=56320|u&1023)}return T(C,m)},k.exports.utf8border=(n,g)=>{g=g||n.length,g>n.length&&(g=n.length);let s=g-1;for(;s>=0&&(n[s]&192)===128;)s--;return s<0||s===0?g:s+E[n[s]]>g?s:g}},69:k=>{\"use strict\";const I=(E,T,n,g)=>{let s=E&65535|0,m=E>>>16&65535|0,_=0;for(;n!==0;){_=n>2e3?2e3:n,n-=_;do s=s+T[g++]|0,m=m+s|0;while(--_);s%=65521,m%=65521}return s|m<<16|0};k.exports=I},619:k=>{\"use strict\";k.exports={Z_NO_FLUSH:0,Z_PARTIAL_FLUSH:1,Z_SYNC_FLUSH:2,Z_FULL_FLUSH:3,Z_FINISH:4,Z_BLOCK:5,Z_TREES:6,Z_OK:0,Z_STREAM_END:1,Z_NEED_DICT:2,Z_ERRNO:-1,Z_STREAM_ERROR:-2,Z_DATA_ERROR:-3,Z_MEM_ERROR:-4,Z_BUF_ERROR:-5,Z_NO_COMPRESSION:0,Z_BEST_SPEED:1,Z_BEST_COMPRESSION:9,Z_DEFAULT_COMPRESSION:-1,Z_FILTERED:1,Z_HUFFMAN_ONLY:2,Z_RLE:3,Z_FIXED:4,Z_DEFAULT_STRATEGY:0,Z_BINARY:0,Z_TEXT:1,Z_UNKNOWN:2,Z_DEFLATED:8}},869:k=>{\"use strict\";const I=()=>{let n,g=[];for(var s=0;s<256;s++){n=s;for(var m=0;m<8;m++)n=n&1?3988292384^n>>>1:n>>>1;g[s]=n}return g},E=new Uint32Array(I()),T=(n,g,s,m)=>{const _=E,C=m+s;n^=-1;for(let u=m;u<C;u++)n=n>>>8^_[(n^g[u])&255];return n^-1};k.exports=T},401:k=>{\"use strict\";function I(){this.text=0,this.time=0,this.xflags=0,this.os=0,this.extra=null,this.extra_len=0,this.name=\"\",this.comment=\"\",this.hcrc=0,this.done=!1}k.exports=I},264:k=>{\"use strict\";k.exports=function(n,g){let s,m,_,C,u,v,l,h,c,N,f,d,M,L,j,D,A,o,i,Z,R,O,p,U;const B=n.state;s=n.next_in,p=n.input,m=s+(n.avail_in-5),_=n.next_out,U=n.output,C=_-(g-n.avail_out),u=_+(n.avail_out-257),v=B.dmax,l=B.wsize,h=B.whave,c=B.wnext,N=B.window,f=B.hold,d=B.bits,M=B.lencode,L=B.distcode,j=(1<<B.lenbits)-1,D=(1<<B.distbits)-1;e:do{d<15&&(f+=p[s++]<<d,d+=8,f+=p[s++]<<d,d+=8),A=M[f&j];t:for(;;){if(o=A>>>24,f>>>=o,d-=o,o=A>>>16&255,o===0)U[_++]=A&65535;else if(o&16){i=A&65535,o&=15,o&&(d<o&&(f+=p[s++]<<d,d+=8),i+=f&(1<<o)-1,f>>>=o,d-=o),d<15&&(f+=p[s++]<<d,d+=8,f+=p[s++]<<d,d+=8),A=L[f&D];i:for(;;){if(o=A>>>24,f>>>=o,d-=o,o=A>>>16&255,o&16){if(Z=A&65535,o&=15,d<o&&(f+=p[s++]<<d,d+=8,d<o&&(f+=p[s++]<<d,d+=8)),Z+=f&(1<<o)-1,Z>v){n.msg=\"invalid distance too far back\",B.mode=30;break e}if(f>>>=o,d-=o,o=_-C,Z>o){if(o=Z-o,o>h&&B.sane){n.msg=\"invalid distance too far back\",B.mode=30;break e}if(R=0,O=N,c===0){if(R+=l-o,o<i){i-=o;do U[_++]=N[R++];while(--o);R=_-Z,O=U}}else if(c<o){if(R+=l+c-o,o-=c,o<i){i-=o;do U[_++]=N[R++];while(--o);if(R=0,c<i){o=c,i-=o;do U[_++]=N[R++];while(--o);R=_-Z,O=U}}}else if(R+=c-o,o<i){i-=o;do U[_++]=N[R++];while(--o);R=_-Z,O=U}for(;i>2;)U[_++]=O[R++],U[_++]=O[R++],U[_++]=O[R++],i-=3;i&&(U[_++]=O[R++],i>1&&(U[_++]=O[R++]))}else{R=_-Z;do U[_++]=U[R++],U[_++]=U[R++],U[_++]=U[R++],i-=3;while(i>2);i&&(U[_++]=U[R++],i>1&&(U[_++]=U[R++]))}}else if((o&64)===0){A=L[(A&65535)+(f&(1<<o)-1)];continue i}else{n.msg=\"invalid distance code\",B.mode=30;break e}break}}else if((o&64)===0){A=M[(A&65535)+(f&(1<<o)-1)];continue t}else if(o&32){B.mode=12;break e}else{n.msg=\"invalid literal/length code\",B.mode=30;break e}break}}while(s<m&&_<u);i=d>>3,s-=i,d-=i<<3,f&=(1<<d)-1,n.next_in=s,n.next_out=_,n.avail_in=s<m?5+(m-s):5-(s-m),n.avail_out=_<u?257+(u-_):257-(_-u),B.hold=f,B.bits=d}},948:(k,I,E)=>{\"use strict\";const T=E(69),n=E(869),g=E(264),s=E(241),m=0,_=1,C=2,{Z_FINISH:u,Z_BLOCK:v,Z_TREES:l,Z_OK:h,Z_STREAM_END:c,Z_NEED_DICT:N,Z_STREAM_ERROR:f,Z_DATA_ERROR:d,Z_MEM_ERROR:M,Z_BUF_ERROR:L,Z_DEFLATED:j}=E(619),D=1,A=2,o=3,i=4,Z=5,R=6,O=7,p=8,U=9,B=10,K=11,V=12,fe=13,de=14,ne=15,ce=16,pe=17,le=18,te=19,re=20,ae=21,we=22,_e=23,ue=24,he=25,Te=26,ke=27,Oe=28,De=29,F=30,Ee=31,Pe=32,Fe=852,je=592,He=15,Re=t=>(t>>>24&255)+(t>>>8&65280)+((t&65280)<<8)+((t&255)<<24);function ze(){this.mode=0,this.last=!1,this.wrap=0,this.havedict=!1,this.flags=0,this.dmax=0,this.check=0,this.total=0,this.head=null,this.wbits=0,this.wsize=0,this.whave=0,this.wnext=0,this.window=null,this.hold=0,this.bits=0,this.length=0,this.offset=0,this.extra=0,this.lencode=null,this.distcode=null,this.lenbits=0,this.distbits=0,this.ncode=0,this.nlen=0,this.ndist=0,this.have=0,this.next=null,this.lens=new Uint16Array(320),this.work=new Uint16Array(288),this.lendyn=null,this.distdyn=null,this.sane=0,this.back=0,this.was=0}const Ie=t=>{if(!t||!t.state)return f;const x=t.state;return t.total_in=t.total_out=x.total=0,t.msg=\"\",x.wrap&&(t.adler=x.wrap&1),x.mode=D,x.last=0,x.havedict=0,x.dmax=32768,x.head=null,x.hold=0,x.bits=0,x.lencode=x.lendyn=new Int32Array(Fe),x.distcode=x.distdyn=new Int32Array(je),x.sane=1,x.back=-1,h},Ue=t=>{if(!t||!t.state)return f;const x=t.state;return x.wsize=0,x.whave=0,x.wnext=0,Ie(t)},Ce=(t,x)=>{let e;if(!t||!t.state)return f;const y=t.state;return x<0?(e=0,x=-x):(e=(x>>4)+1,x<48&&(x&=15)),x&&(x<8||x>15)?f:(y.window!==null&&y.wbits!==x&&(y.window=null),y.wrap=e,y.wbits=x,Ue(t))},Ne=(t,x)=>{if(!t)return f;const e=new ze;t.state=e,e.window=null;const y=Ce(t,x);return y!==h&&(t.state=null),y},Ye=t=>Ne(t,He);let Be=!0,Ae,Se;const Ge=t=>{if(Be){Ae=new Int32Array(512),Se=new Int32Array(32);let x=0;for(;x<144;)t.lens[x++]=8;for(;x<256;)t.lens[x++]=9;for(;x<280;)t.lens[x++]=7;for(;x<288;)t.lens[x++]=8;for(s(_,t.lens,0,288,Ae,0,t.work,{bits:9}),x=0;x<32;)t.lens[x++]=5;s(C,t.lens,0,32,Se,0,t.work,{bits:5}),Be=!1}t.lencode=Ae,t.lenbits=9,t.distcode=Se,t.distbits=5},Me=(t,x,e,y)=>{let H;const w=t.state;return w.window===null&&(w.wsize=1<<w.wbits,w.wnext=0,w.whave=0,w.window=new Uint8Array(w.wsize)),y>=w.wsize?(w.window.set(x.subarray(e-w.wsize,e),0),w.wnext=0,w.whave=w.wsize):(H=w.wsize-w.wnext,H>y&&(H=y),w.window.set(x.subarray(e-y,e-y+H),w.wnext),y-=H,y?(w.window.set(x.subarray(e-y,e),0),w.wnext=y,w.whave=w.wsize):(w.wnext+=H,w.wnext===w.wsize&&(w.wnext=0),w.whave<w.wsize&&(w.whave+=H))),0},Xe=(t,x)=>{let e,y,H,w,q,b,G,a,r,xe,z,S,be,me,Y=0,P,J,$,Q,ve,ge,X,ee;const W=new Uint8Array(4);let oe,ie;const Le=new Uint8Array([16,17,18,0,8,7,9,6,10,5,11,4,12,3,13,2,14,1,15]);if(!t||!t.state||!t.output||!t.input&&t.avail_in!==0)return f;e=t.state,e.mode===V&&(e.mode=fe),q=t.next_out,H=t.output,G=t.avail_out,w=t.next_in,y=t.input,b=t.avail_in,a=e.hold,r=e.bits,xe=b,z=G,ee=h;e:for(;;)switch(e.mode){case D:if(e.wrap===0){e.mode=fe;break}for(;r<16;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(e.wrap&2&&a===35615){e.check=0,W[0]=a&255,W[1]=a>>>8&255,e.check=n(e.check,W,2,0),a=0,r=0,e.mode=A;break}if(e.flags=0,e.head&&(e.head.done=!1),!(e.wrap&1)||(((a&255)<<8)+(a>>8))%31){t.msg=\"incorrect header check\",e.mode=F;break}if((a&15)!==j){t.msg=\"unknown compression method\",e.mode=F;break}if(a>>>=4,r-=4,X=(a&15)+8,e.wbits===0)e.wbits=X;else if(X>e.wbits){t.msg=\"invalid window size\",e.mode=F;break}e.dmax=1<<e.wbits,t.adler=e.check=1,e.mode=a&512?B:V,a=0,r=0;break;case A:for(;r<16;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(e.flags=a,(e.flags&255)!==j){t.msg=\"unknown compression method\",e.mode=F;break}if(e.flags&57344){t.msg=\"unknown header flags set\",e.mode=F;break}e.head&&(e.head.text=a>>8&1),e.flags&512&&(W[0]=a&255,W[1]=a>>>8&255,e.check=n(e.check,W,2,0)),a=0,r=0,e.mode=o;case o:for(;r<32;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}e.head&&(e.head.time=a),e.flags&512&&(W[0]=a&255,W[1]=a>>>8&255,W[2]=a>>>16&255,W[3]=a>>>24&255,e.check=n(e.check,W,4,0)),a=0,r=0,e.mode=i;case i:for(;r<16;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}e.head&&(e.head.xflags=a&255,e.head.os=a>>8),e.flags&512&&(W[0]=a&255,W[1]=a>>>8&255,e.check=n(e.check,W,2,0)),a=0,r=0,e.mode=Z;case Z:if(e.flags&1024){for(;r<16;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}e.length=a,e.head&&(e.head.extra_len=a),e.flags&512&&(W[0]=a&255,W[1]=a>>>8&255,e.check=n(e.check,W,2,0)),a=0,r=0}else e.head&&(e.head.extra=null);e.mode=R;case R:if(e.flags&1024&&(S=e.length,S>b&&(S=b),S&&(e.head&&(X=e.head.extra_len-e.length,e.head.extra||(e.head.extra=new Uint8Array(e.head.extra_len)),e.head.extra.set(y.subarray(w,w+S),X)),e.flags&512&&(e.check=n(e.check,y,S,w)),b-=S,w+=S,e.length-=S),e.length))break e;e.length=0,e.mode=O;case O:if(e.flags&2048){if(b===0)break e;S=0;do X=y[w+S++],e.head&&X&&e.length<65536&&(e.head.name+=String.fromCharCode(X));while(X&&S<b);if(e.flags&512&&(e.check=n(e.check,y,S,w)),b-=S,w+=S,X)break e}else e.head&&(e.head.name=null);e.length=0,e.mode=p;case p:if(e.flags&4096){if(b===0)break e;S=0;do X=y[w+S++],e.head&&X&&e.length<65536&&(e.head.comment+=String.fromCharCode(X));while(X&&S<b);if(e.flags&512&&(e.check=n(e.check,y,S,w)),b-=S,w+=S,X)break e}else e.head&&(e.head.comment=null);e.mode=U;case U:if(e.flags&512){for(;r<16;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(a!==(e.check&65535)){t.msg=\"header crc mismatch\",e.mode=F;break}a=0,r=0}e.head&&(e.head.hcrc=e.flags>>9&1,e.head.done=!0),t.adler=e.check=0,e.mode=V;break;case B:for(;r<32;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}t.adler=e.check=Re(a),a=0,r=0,e.mode=K;case K:if(e.havedict===0)return t.next_out=q,t.avail_out=G,t.next_in=w,t.avail_in=b,e.hold=a,e.bits=r,N;t.adler=e.check=1,e.mode=V;case V:if(x===v||x===l)break e;case fe:if(e.last){a>>>=r&7,r-=r&7,e.mode=ke;break}for(;r<3;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}switch(e.last=a&1,a>>>=1,r-=1,a&3){case 0:e.mode=de;break;case 1:if(Ge(e),e.mode=re,x===l){a>>>=2,r-=2;break e}break;case 2:e.mode=pe;break;case 3:t.msg=\"invalid block type\",e.mode=F}a>>>=2,r-=2;break;case de:for(a>>>=r&7,r-=r&7;r<32;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if((a&65535)!==(a>>>16^65535)){t.msg=\"invalid stored block lengths\",e.mode=F;break}if(e.length=a&65535,a=0,r=0,e.mode=ne,x===l)break e;case ne:e.mode=ce;case ce:if(S=e.length,S){if(S>b&&(S=b),S>G&&(S=G),S===0)break e;H.set(y.subarray(w,w+S),q),b-=S,w+=S,G-=S,q+=S,e.length-=S;break}e.mode=V;break;case pe:for(;r<14;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(e.nlen=(a&31)+257,a>>>=5,r-=5,e.ndist=(a&31)+1,a>>>=5,r-=5,e.ncode=(a&15)+4,a>>>=4,r-=4,e.nlen>286||e.ndist>30){t.msg=\"too many length or distance symbols\",e.mode=F;break}e.have=0,e.mode=le;case le:for(;e.have<e.ncode;){for(;r<3;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}e.lens[Le[e.have++]]=a&7,a>>>=3,r-=3}for(;e.have<19;)e.lens[Le[e.have++]]=0;if(e.lencode=e.lendyn,e.lenbits=7,oe={bits:e.lenbits},ee=s(m,e.lens,0,19,e.lencode,0,e.work,oe),e.lenbits=oe.bits,ee){t.msg=\"invalid code lengths set\",e.mode=F;break}e.have=0,e.mode=te;case te:for(;e.have<e.nlen+e.ndist;){for(;Y=e.lencode[a&(1<<e.lenbits)-1],P=Y>>>24,J=Y>>>16&255,$=Y&65535,!(P<=r);){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if($<16)a>>>=P,r-=P,e.lens[e.have++]=$;else{if($===16){for(ie=P+2;r<ie;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(a>>>=P,r-=P,e.have===0){t.msg=\"invalid bit length repeat\",e.mode=F;break}X=e.lens[e.have-1],S=3+(a&3),a>>>=2,r-=2}else if($===17){for(ie=P+3;r<ie;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}a>>>=P,r-=P,X=0,S=3+(a&7),a>>>=3,r-=3}else{for(ie=P+7;r<ie;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}a>>>=P,r-=P,X=0,S=11+(a&127),a>>>=7,r-=7}if(e.have+S>e.nlen+e.ndist){t.msg=\"invalid bit length repeat\",e.mode=F;break}for(;S--;)e.lens[e.have++]=X}}if(e.mode===F)break;if(e.lens[256]===0){t.msg=\"invalid code -- missing end-of-block\",e.mode=F;break}if(e.lenbits=9,oe={bits:e.lenbits},ee=s(_,e.lens,0,e.nlen,e.lencode,0,e.work,oe),e.lenbits=oe.bits,ee){t.msg=\"invalid literal/lengths set\",e.mode=F;break}if(e.distbits=6,e.distcode=e.distdyn,oe={bits:e.distbits},ee=s(C,e.lens,e.nlen,e.ndist,e.distcode,0,e.work,oe),e.distbits=oe.bits,ee){t.msg=\"invalid distances set\",e.mode=F;break}if(e.mode=re,x===l)break e;case re:e.mode=ae;case ae:if(b>=6&&G>=258){t.next_out=q,t.avail_out=G,t.next_in=w,t.avail_in=b,e.hold=a,e.bits=r,g(t,z),q=t.next_out,H=t.output,G=t.avail_out,w=t.next_in,y=t.input,b=t.avail_in,a=e.hold,r=e.bits,e.mode===V&&(e.back=-1);break}for(e.back=0;Y=e.lencode[a&(1<<e.lenbits)-1],P=Y>>>24,J=Y>>>16&255,$=Y&65535,!(P<=r);){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(J&&(J&240)===0){for(Q=P,ve=J,ge=$;Y=e.lencode[ge+((a&(1<<Q+ve)-1)>>Q)],P=Y>>>24,J=Y>>>16&255,$=Y&65535,!(Q+P<=r);){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}a>>>=Q,r-=Q,e.back+=Q}if(a>>>=P,r-=P,e.back+=P,e.length=$,J===0){e.mode=Te;break}if(J&32){e.back=-1,e.mode=V;break}if(J&64){t.msg=\"invalid literal/length code\",e.mode=F;break}e.extra=J&15,e.mode=we;case we:if(e.extra){for(ie=e.extra;r<ie;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}e.length+=a&(1<<e.extra)-1,a>>>=e.extra,r-=e.extra,e.back+=e.extra}e.was=e.length,e.mode=_e;case _e:for(;Y=e.distcode[a&(1<<e.distbits)-1],P=Y>>>24,J=Y>>>16&255,$=Y&65535,!(P<=r);){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if((J&240)===0){for(Q=P,ve=J,ge=$;Y=e.distcode[ge+((a&(1<<Q+ve)-1)>>Q)],P=Y>>>24,J=Y>>>16&255,$=Y&65535,!(Q+P<=r);){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}a>>>=Q,r-=Q,e.back+=Q}if(a>>>=P,r-=P,e.back+=P,J&64){t.msg=\"invalid distance code\",e.mode=F;break}e.offset=$,e.extra=J&15,e.mode=ue;case ue:if(e.extra){for(ie=e.extra;r<ie;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}e.offset+=a&(1<<e.extra)-1,a>>>=e.extra,r-=e.extra,e.back+=e.extra}if(e.offset>e.dmax){t.msg=\"invalid distance too far back\",e.mode=F;break}e.mode=he;case he:if(G===0)break e;if(S=z-G,e.offset>S){if(S=e.offset-S,S>e.whave&&e.sane){t.msg=\"invalid distance too far back\",e.mode=F;break}S>e.wnext?(S-=e.wnext,be=e.wsize-S):be=e.wnext-S,S>e.length&&(S=e.length),me=e.window}else me=H,be=q-e.offset,S=e.length;S>G&&(S=G),G-=S,e.length-=S;do H[q++]=me[be++];while(--S);e.length===0&&(e.mode=ae);break;case Te:if(G===0)break e;H[q++]=e.length,G--,e.mode=ae;break;case ke:if(e.wrap){for(;r<32;){if(b===0)break e;b--,a|=y[w++]<<r,r+=8}if(z-=G,t.total_out+=z,e.total+=z,z&&(t.adler=e.check=e.flags?n(e.check,H,z,q-z):T(e.check,H,z,q-z)),z=G,(e.flags?a:Re(a))!==e.check){t.msg=\"incorrect data check\",e.mode=F;break}a=0,r=0}e.mode=Oe;case Oe:if(e.wrap&&e.flags){for(;r<32;){if(b===0)break e;b--,a+=y[w++]<<r,r+=8}if(a!==(e.total&4294967295)){t.msg=\"incorrect length check\",e.mode=F;break}a=0,r=0}e.mode=De;case De:ee=c;break e;case F:ee=d;break e;case Ee:return M;case Pe:default:return f}return t.next_out=q,t.avail_out=G,t.next_in=w,t.avail_in=b,e.hold=a,e.bits=r,(e.wsize||z!==t.avail_out&&e.mode<F&&(e.mode<ke||x!==u))&&Me(t,t.output,t.next_out,z-t.avail_out)?(e.mode=Ee,M):(xe-=t.avail_in,z-=t.avail_out,t.total_in+=xe,t.total_out+=z,e.total+=z,e.wrap&&z&&(t.adler=e.check=e.flags?n(e.check,H,z,t.next_out-z):T(e.check,H,z,t.next_out-z)),t.data_type=e.bits+(e.last?64:0)+(e.mode===V?128:0)+(e.mode===re||e.mode===ne?256:0),(xe===0&&z===0||x===u)&&ee===h&&(ee=L),ee)},Ke=t=>{if(!t||!t.state)return f;let x=t.state;return x.window&&(x.window=null),t.state=null,h},Ve=(t,x)=>{if(!t||!t.state)return f;const e=t.state;return(e.wrap&2)===0?f:(e.head=x,x.done=!1,h)},We=(t,x)=>{const e=x.length;let y,H,w;return!t||!t.state||(y=t.state,y.wrap!==0&&y.mode!==K)?f:y.mode===K&&(H=1,H=T(H,x,e,0),H!==y.check)?d:(w=Me(t,x,e,e),w?(y.mode=Ee,M):(y.havedict=1,h))};k.exports.inflateReset=Ue,k.exports.inflateReset2=Ce,k.exports.inflateResetKeep=Ie,k.exports.inflateInit=Ye,k.exports.inflateInit2=Ne,k.exports.inflate=Xe,k.exports.inflateEnd=Ke,k.exports.inflateGetHeader=Ve,k.exports.inflateSetDictionary=We,k.exports.inflateInfo=\"pako inflate (from Nodeca project)\"},241:k=>{\"use strict\";const m=new Uint16Array([3,4,5,6,7,8,9,10,11,13,15,17,19,23,27,31,35,43,51,59,67,83,99,115,131,163,195,227,258,0,0]),_=new Uint8Array([16,16,16,16,16,16,16,16,17,17,17,17,18,18,18,18,19,19,19,19,20,20,20,20,21,21,21,21,16,72,78]),C=new Uint16Array([1,2,3,4,5,7,9,13,17,25,33,49,65,97,129,193,257,385,513,769,1025,1537,2049,3073,4097,6145,8193,12289,16385,24577,0,0]),u=new Uint8Array([16,16,16,16,17,17,18,18,19,19,20,20,21,21,22,22,23,23,24,24,25,25,26,26,27,27,28,28,29,29,64,64]),v=(l,h,c,N,f,d,M,L)=>{const j=L.bits;let D=0,A=0,o=0,i=0,Z=0,R=0,O=0,p=0,U=0,B=0,K,V,fe,de,ne,ce=null,pe=0,le;const te=new Uint16Array(15+1),re=new Uint16Array(15+1);let ae=null,we=0,_e,ue,he;for(D=0;D<=15;D++)te[D]=0;for(A=0;A<N;A++)te[h[c+A]]++;for(Z=j,i=15;i>=1&&te[i]===0;i--);if(Z>i&&(Z=i),i===0)return f[d++]=1<<24|64<<16|0,f[d++]=1<<24|64<<16|0,L.bits=1,0;for(o=1;o<i&&te[o]===0;o++);for(Z<o&&(Z=o),p=1,D=1;D<=15;D++)if(p<<=1,p-=te[D],p<0)return-1;if(p>0&&(l===0||i!==1))return-1;for(re[1]=0,D=1;D<15;D++)re[D+1]=re[D]+te[D];for(A=0;A<N;A++)h[c+A]!==0&&(M[re[h[c+A]]++]=A);if(l===0?(ce=ae=M,le=19):l===1?(ce=m,pe-=257,ae=_,we-=257,le=256):(ce=C,ae=u,le=-1),B=0,A=0,D=o,ne=d,R=Z,O=0,fe=-1,U=1<<Z,de=U-1,l===1&&U>852||l===2&&U>592)return 1;for(;;){_e=D-O,M[A]<le?(ue=0,he=M[A]):M[A]>le?(ue=ae[we+M[A]],he=ce[pe+M[A]]):(ue=32+64,he=0),K=1<<D-O,V=1<<R,o=V;do V-=K,f[ne+(B>>O)+V]=_e<<24|ue<<16|he|0;while(V!==0);for(K=1<<D-1;B&K;)K>>=1;if(K!==0?(B&=K-1,B+=K):B=0,A++,--te[D]===0){if(D===i)break;D=h[c+M[A]]}if(D>Z&&(B&de)!==fe){for(O===0&&(O=Z),ne+=o,R=D-O,p=1<<R;R+O<i&&(p-=te[R+O],!(p<=0));)R++,p<<=1;if(U+=1<<R,l===1&&U>852||l===2&&U>592)return 1;fe=B&de,f[fe]=Z<<24|R<<16|ne-d|0}}return B!==0&&(f[ne+B]=D-O<<24|64<<16|0),L.bits=Z,0};k.exports=v},898:k=>{\"use strict\";k.exports={2:\"need dictionary\",1:\"stream end\",0:\"\",\"-1\":\"file error\",\"-2\":\"stream error\",\"-3\":\"data error\",\"-4\":\"insufficient memory\",\"-5\":\"buffer error\",\"-6\":\"incompatible version\"}},292:k=>{\"use strict\";function I(){this.input=null,this.next_in=0,this.avail_in=0,this.total_in=0,this.output=null,this.next_out=0,this.avail_out=0,this.total_out=0,this.msg=\"\",this.state=null,this.data_type=2,this.adler=0}k.exports=I},330:(k,I,E)=>{\"use strict\";E.d(I,{default:()=>C});var T=E(829),n=E(861),g=E.n(n),s=E(843);function m(u){if(u.__type__==\"npy\"){var v;if(window.obj=u,u.hasOwnProperty(\"zdata\")){const f=Uint8Array.from(window.atob(u.zdata),d=>d.charCodeAt(0));v=(0,s.rr)(f)}else v=Uint8Array.from(window.atob(u.data),f=>f.charCodeAt(0));var l=(0,T.g)(v.buffer);if(l=n(l.data,l.shape),u.hasOwnProperty(\"min\")){let f=l.dtype===\"uint8\"?255:65535;for(var h=1,c=0;c<l.shape.length;c++)h=h*l.shape[c];for(var N=n(new Float32Array(h),l.shape),c=0;c<l.data.length;c++)N.data[c]=u.min+(u.max-u.min)*l.data[c]/f;return N}else return l}else return{}}function _(u){if(Array.isArray(u)){var v=[];for(var l of u)v.push(_(l));return v}else if(u instanceof Object){if(u.hasOwnProperty(\"__type__\"))return m(u);var v={};for(var h of Object.keys(u))v[h]=_(u[h]);return v}else return u}const C=loader={unpack_obj:_}}},ye={};function se(k){if(ye[k])return ye[k].exports;var I=ye[k]={exports:{}};return Ze[k](I,I.exports,se),I.exports}return se.n=k=>{var I=k&&k.__esModule?()=>k.default:()=>k;return se.d(I,{a:I}),I},se.d=(k,I)=>{for(var E in I)se.o(I,E)&&!se.o(k,E)&&Object.defineProperty(k,E,{enumerable:!0,get:I[E]})},se.o=(k,I)=>Object.prototype.hasOwnProperty.call(k,I),se(330)})().default;\n",
              "</script>\n",
              "<script>var AttentionMulti;AttentionMulti=(()=>{\"use strict\";var St={143:(x,B,te)=>{te.d(B,{default:()=>mn});function P(){}const me=e=>e;function jt(e,t){for(const n in t)e[n]=t[n];return e}function Mt(e){return e&&typeof e==\"object\"&&typeof e.then==\"function\"}function En(e,t,n,i,o){e.__svelte_meta={loc:{file:t,line:n,column:i,char:o}}}function Ge(e){return e()}function Se(){return Object.create(null)}function H(e){e.forEach(Ge)}function fe(e){return typeof e==\"function\"}function je(e,t){return e!=e?t==t:e!==t||e&&typeof e==\"object\"||typeof e==\"function\"}function Sn(e,t){return e!=e?t==t:e!==t}function Ke(e){return Object.keys(e).length===0}function jn(e,t){if(e!=null&&typeof e.subscribe!=\"function\")throw new Error(`'${t}' is not a store with a 'subscribe' method`)}function Qe(e,...t){if(e==null)return P;const n=e.subscribe(...t);return n.unsubscribe?()=>n.unsubscribe():n}function Mn(e){let t;return Qe(e,n=>t=n)(),t}function An(e,t,n){e.$$.on_destroy.push(Qe(t,n))}function At(e,t,n,i){if(e){const o=Me(e,t,n,i);return e[0](o)}}function Me(e,t,n,i){return e[1]&&i?jt(n.ctx.slice(),e[1](i(t))):n.ctx}function Ze(e,t,n,i){if(e[2]&&i){const o=e[2](i(n));if(t.dirty===void 0)return o;if(typeof o==\"object\"){const l=[],s=Math.max(t.dirty.length,o.length);for(let r=0;r<s;r+=1)l[r]=t.dirty[r]|o[r];return l}return t.dirty|o}return t.dirty}function Ct(e,t,n,i,o,l,s){const r=Ze(t,i,o,l);if(r){const f=Me(t,n,i,s);e.p(f,r)}}function Cn(e,t,n,i,o,l,s,r){const f=s(o)|Ze(t,i,o,l);if(f){const a=Me(t,n,i,r);e.p(a,f)}}function Dn(e){const t={};for(const n in e)n[0]!==\"$\"&&(t[n]=e[n]);return t}function On(e,t){const n={};t=new Set(t);for(const i in e)!t.has(i)&&i[0]!==\"$\"&&(n[i]=e[i]);return n}function Tn(e){const t={};for(const n in e)t[n]=!0;return t}function Ln(e){let t=!1;return function(...n){t||(t=!0,e.call(this,...n))}}function Nn(e){return e==null?\"\":e}function Pn(e,t,n=t){return e.set(n),t}const Dt=(e,t)=>Object.prototype.hasOwnProperty.call(e,t);function Rn(e){return e&&fe(e.destroy)?e.destroy:P}const Bn=typeof window!=\"undefined\";let ae=null,ge=null;function Hn(e){ae=e}function xn(e){ge=e}const K=new Set;function $e(e){K.forEach(t=>{t.c(e)||(K.delete(t),t.f())}),K.size!==0&&ge($e)}function zn(){K.clear()}function ve(e){let t;return K.size===0&&ge($e),{promise:new Promise(n=>{K.add(t={c:e,f:n})}),abort(){K.delete(t)}}}function E(e,t){e.appendChild(t)}function C(e,t,n){e.insertBefore(t,n||null)}function j(e){e.parentNode.removeChild(e)}function et(e,t){for(let n=0;n<e.length;n+=1)e[n]&&e[n].d(t)}function F(e){return document.createElement(e)}function In(e,t){return document.createElement(e,{is:t})}function Vn(e,t){const n={};for(const i in e)Dt(e,i)&&t.indexOf(i)===-1&&(n[i]=e[i]);return n}function Ot(e){return document.createElementNS(\"http://www.w3.org/2000/svg\",e)}function R(e){return document.createTextNode(e)}function T(){return R(\" \")}function tt(){return R(\"\")}function z(e,t,n,i){return e.addEventListener(t,n,i),()=>e.removeEventListener(t,n,i)}function Wn(e){return function(t){return t.preventDefault(),e.call(this,t)}}function Yn(e){return function(t){return t.stopPropagation(),e.call(this,t)}}function Un(e){return function(t){t.target===this&&e.call(this,t)}}function y(e,t,n){n==null?e.removeAttribute(t):e.getAttribute(t)!==n&&e.setAttribute(t,n)}function Jn(e,t){const n=Object.getOwnPropertyDescriptors(e.__proto__);for(const i in t)t[i]==null?e.removeAttribute(i):i===\"style\"?e.style.cssText=t[i]:i===\"__value\"?e.value=e[i]=t[i]:n[i]&&n[i].set?e[i]=t[i]:y(e,i,t[i])}function Xn(e,t){for(const n in t)y(e,n,t[n])}function Gn(e,t,n){t in e?e[t]=n:y(e,t,n)}function Kn(e,t,n){e.setAttributeNS(\"http://www.w3.org/1999/xlink\",t,n)}function Qn(e,t,n){const i=new Set;for(let o=0;o<e.length;o+=1)e[o].checked&&i.add(e[o].__value);return n||i.delete(t),Array.from(i)}function Zn(e){return e===\"\"?null:+e}function $n(e){const t=[];for(let n=0;n<e.length;n+=1)t.push({start:e.start(n),end:e.end(n)});return t}function Tt(e){return Array.from(e.childNodes)}function ei(e,t,n,i){for(let o=0;o<e.length;o+=1){const l=e[o];if(l.nodeName===t){let s=0;const r=[];for(;s<l.attributes.length;){const f=l.attributes[s++];n[f.name]||r.push(f.name)}for(let f=0;f<r.length;f++)l.removeAttribute(r[f]);return e.splice(o,1)[0]}}return i?Ot(t):F(t)}function Lt(e,t){for(let n=0;n<e.length;n+=1){const i=e[n];if(i.nodeType===3)return i.data=\"\"+t,e.splice(n,1)[0]}return R(t)}function ti(e){return Lt(e,\" \")}function ke(e,t){t=\"\"+t,e.wholeText!==t&&(e.data=t)}function ni(e,t){e.value=t==null?\"\":t}function ii(e,t){try{e.type=t}catch(n){}}function S(e,t,n,i){e.style.setProperty(t,n,i?\"important\":\"\")}function oi(e,t){for(let n=0;n<e.options.length;n+=1){const i=e.options[n];if(i.__value===t){i.selected=!0;return}}}function li(e,t){for(let n=0;n<e.options.length;n+=1){const i=e.options[n];i.selected=~t.indexOf(i.__value)}}function si(e){const t=e.querySelector(\":checked\")||e.options[0];return t&&t.__value}function ri(e){return[].map.call(e.querySelectorAll(\":checked\"),t=>t.__value)}let be;function Nt(){if(be===void 0){be=!1;try{typeof window!=\"undefined\"&&window.parent&&window.parent.document}catch(e){be=!0}}return be}function ui(e,t){getComputedStyle(e).position===\"static\"&&(e.style.position=\"relative\");const i=F(\"iframe\");i.setAttribute(\"style\",\"display: block; position: absolute; top: 0; left: 0; width: 100%; height: 100%; overflow: hidden; border: 0; opacity: 0; pointer-events: none; z-index: -1;\"),i.setAttribute(\"aria-hidden\",\"true\"),i.tabIndex=-1;const o=Nt();let l;return o?(i.src=\"data:text/html,<script>onresize=function(){parent.postMessage(0,'*')}<\\/script>\",l=z(window,\"message\",s=>{s.source===i.contentWindow&&t()})):(i.src=\"about:blank\",i.onload=()=>{l=z(i.contentWindow,\"resize\",t)}),E(e,i),()=>{(o||l&&i.contentWindow)&&l(),j(i)}}function fi(e,t,n){e.classList[n?\"add\":\"remove\"](t)}function Ae(e,t){const n=document.createEvent(\"CustomEvent\");return n.initCustomEvent(e,!1,!1,t),n}function ai(e,t=document.body){return Array.from(t.querySelectorAll(e))}class ci{constructor(t=null){this.a=t,this.e=this.n=null}m(t,n,i=null){this.e||(this.e=F(n.nodeName),this.t=n,this.h(t)),this.i(i)}h(t){this.e.innerHTML=t,this.n=Array.from(this.e.childNodes)}i(t){for(let n=0;n<this.n.length;n+=1)C(this.t,this.n[n],t)}p(t){this.d(),this.h(t),this.i(this.a)}d(){this.n.forEach(j)}}function di(e){const t={};for(const n of e)t[n.name]=n.value;return t}function _i(e){const t={};return e.childNodes.forEach(n=>{t[n.slot||\"default\"]=!0}),t}const Ce=new Set;let ye=0;function Pt(e){let t=5381,n=e.length;for(;n--;)t=(t<<5)-t^e.charCodeAt(n);return t>>>0}function ce(e,t,n,i,o,l,s,r=0){const f=16.666/i;let a=`{\n",
              "`;for(let u=0;u<=1;u+=f){const _=t+(n-t)*l(u);a+=u*100+`%{${s(_,1-_)}}\n",
              "`}const c=a+`100% {${s(n,1-n)}}\n",
              "}`,d=`__svelte_${Pt(c)}_${r}`,h=e.ownerDocument;Ce.add(h);const p=h.__svelte_stylesheet||(h.__svelte_stylesheet=h.head.appendChild(F(\"style\")).sheet),m=h.__svelte_rules||(h.__svelte_rules={});m[d]||(m[d]=!0,p.insertRule(`@keyframes ${d} ${c}`,p.cssRules.length));const g=e.style.animation||\"\";return e.style.animation=`${g?`${g}, `:\"\"}${d} ${i}ms linear ${o}ms 1 both`,ye+=1,d}function de(e,t){const n=(e.style.animation||\"\").split(\", \"),i=n.filter(t?l=>l.indexOf(t)<0:l=>l.indexOf(\"__svelte\")===-1),o=n.length-i.length;o&&(e.style.animation=i.join(\", \"),ye-=o,ye||Rt())}function Rt(){ge(()=>{ye||(Ce.forEach(e=>{const t=e.__svelte_stylesheet;let n=t.cssRules.length;for(;n--;)t.deleteRule(n);e.__svelte_rules={}}),Ce.clear())})}function hi(e,t,n,i){if(!t)return P;const o=e.getBoundingClientRect();if(t.left===o.left&&t.right===o.right&&t.top===o.top&&t.bottom===o.bottom)return P;const{delay:l=0,duration:s=300,easing:r=me,start:f=ae()+l,end:a=f+s,tick:c=P,css:d}=n(e,{from:t,to:o},i);let h=!0,p=!1,m;function g(){d&&(m=ce(e,0,1,s,l,r,d)),l||(p=!0)}function u(){d&&de(e,m),h=!1}return ve(_=>{if(!p&&_>=f&&(p=!0),p&&_>=a&&(c(1,0),u()),!h)return!1;if(p){const k=_-f,w=0+1*r(k/s);c(w,1-w)}return!0}),g(),c(0,1),u}function pi(e){const t=getComputedStyle(e);if(t.position!==\"absolute\"&&t.position!==\"fixed\"){const{width:n,height:i}=t,o=e.getBoundingClientRect();e.style.position=\"absolute\",e.style.width=n,e.style.height=i,Bt(e,o)}}function Bt(e,t){const n=e.getBoundingClientRect();if(t.left!==n.left||t.top!==n.top){const i=getComputedStyle(e),o=i.transform===\"none\"?\"\":i.transform;e.style.transform=`${o} translate(${t.left-n.left}px, ${t.top-n.top}px)`}}let _e;function I(e){_e=e}function U(){if(!_e)throw new Error(\"Function called outside component initialization\");return _e}function mi(e){U().$$.before_update.push(e)}function Ht(e){U().$$.on_mount.push(e)}function gi(e){U().$$.after_update.push(e)}function vi(e){U().$$.on_destroy.push(e)}function ki(){const e=U();return(t,n)=>{const i=e.$$.callbacks[t];if(i){const o=Ae(t,n);i.slice().forEach(l=>{l.call(e,o)})}}}function bi(e,t){U().$$.context.set(e,t)}function yi(e){return U().$$.context.get(e)}function wi(e){return U().$$.context.has(e)}function Fi(e,t){const n=e.$$.callbacks[t.type];n&&n.slice().forEach(i=>i(t))}const he=[],qi={enabled:!1},ne=[],we=[],De=[],nt=Promise.resolve();let Oe=!1;function it(){Oe||(Oe=!0,nt.then(Ne))}function Ei(){return it(),nt}function ie(e){we.push(e)}function ot(e){De.push(e)}let Te=!1;const Le=new Set;function Ne(){if(!Te){Te=!0;do{for(let e=0;e<he.length;e+=1){const t=he[e];I(t),xt(t.$$)}for(I(null),he.length=0;ne.length;)ne.pop()();for(let e=0;e<we.length;e+=1){const t=we[e];Le.has(t)||(Le.add(t),t())}we.length=0}while(he.length);for(;De.length;)De.pop()();Oe=!1,Te=!1,Le.clear()}}function xt(e){if(e.fragment!==null){e.update(),H(e.before_update);const t=e.dirty;e.dirty=[-1],e.fragment&&e.fragment.p(e.ctx,t),e.after_update.forEach(ie)}}let pe;function Pe(){return pe||(pe=Promise.resolve(),pe.then(()=>{pe=null})),pe}function Q(e,t,n){e.dispatchEvent(Ae(`${t?\"intro\":\"outro\"}${n}`))}const Fe=new Set;let V;function oe(){V={r:0,c:[],p:V}}function le(){V.r||H(V.c),V=V.p}function O(e,t){e&&e.i&&(Fe.delete(e),e.i(t))}function L(e,t,n,i){if(e&&e.o){if(Fe.has(e))return;Fe.add(e),V.c.push(()=>{Fe.delete(e),i&&(n&&e.d(1),i())}),e.o(t)}}const Re={duration:0};function Si(e,t,n){let i=t(e,n),o=!1,l,s,r=0;function f(){l&&de(e,l)}function a(){const{delay:d=0,duration:h=300,easing:p=me,tick:m=P,css:g}=i||Re;g&&(l=ce(e,0,1,h,d,p,g,r++)),m(0,1);const u=ae()+d,_=u+h;s&&s.abort(),o=!0,ie(()=>Q(e,!0,\"start\")),s=ve(k=>{if(o){if(k>=_)return m(1,0),Q(e,!0,\"end\"),f(),o=!1;if(k>=u){const w=p((k-u)/h);m(w,1-w)}}return o})}let c=!1;return{start(){c||(de(e),fe(i)?(i=i(),Pe().then(a)):a())},invalidate(){c=!1},end(){o&&(f(),o=!1)}}}function ji(e,t,n){let i=t(e,n),o=!0,l;const s=V;s.r+=1;function r(){const{delay:f=0,duration:a=300,easing:c=me,tick:d=P,css:h}=i||Re;h&&(l=ce(e,1,0,a,f,c,h));const p=ae()+f,m=p+a;ie(()=>Q(e,!1,\"start\")),ve(g=>{if(o){if(g>=m)return d(0,1),Q(e,!1,\"end\"),--s.r||H(s.c),!1;if(g>=p){const u=c((g-p)/a);d(1-u,u)}}return o})}return fe(i)?Pe().then(()=>{i=i(),r()}):r(),{end(f){f&&i.tick&&i.tick(1,0),o&&(l&&de(e,l),o=!1)}}}function Mi(e,t,n,i){let o=t(e,n),l=i?0:1,s=null,r=null,f=null;function a(){f&&de(e,f)}function c(h,p){const m=h.b-l;return p*=Math.abs(m),{a:l,b:h.b,d:m,duration:p,start:h.start,end:h.start+p,group:h.group}}function d(h){const{delay:p=0,duration:m=300,easing:g=me,tick:u=P,css:_}=o||Re,k={start:ae()+p,b:h};h||(k.group=V,V.r+=1),s||r?r=k:(_&&(a(),f=ce(e,l,h,m,p,g,_)),h&&u(0,1),s=c(k,m),ie(()=>Q(e,h,\"start\")),ve(w=>{if(r&&w>r.start&&(s=c(r,m),r=null,Q(e,s.b,\"start\"),_&&(a(),f=ce(e,l,s.b,s.duration,0,g,o.css))),s){if(w>=s.end)u(l=s.b,1-l),Q(e,s.b,\"end\"),r||(s.b?a():--s.group.r||H(s.group.c)),s=null;else if(w>=s.start){const M=w-s.start;l=s.a+s.d*g(M/s.duration),u(l,1-l)}}return!!(s||r)}))}return{run(h){fe(o)?Pe().then(()=>{o=o(),d(h)}):d(h)},end(){a(),s=r=null}}}function Ai(e,t){const n=t.token={};function i(o,l,s,r){if(t.token!==n)return;t.resolved=r;let f=t.ctx;s!==void 0&&(f=f.slice(),f[s]=r);const a=o&&(t.current=o)(f);let c=!1;t.block&&(t.blocks?t.blocks.forEach((d,h)=>{h!==l&&d&&(oe(),L(d,1,1,()=>{t.blocks[h]===d&&(t.blocks[h]=null)}),le())}):t.block.d(1),a.c(),O(a,1),a.m(t.mount(),t.anchor),c=!0),t.block=a,t.blocks&&(t.blocks[l]=a),c&&Ne()}if(Mt(e)){const o=U();if(e.then(l=>{I(o),i(t.then,1,t.value,l),I(null)},l=>{if(I(o),i(t.catch,2,t.error,l),I(null),!t.hasCatch)throw l}),t.current!==t.pending)return i(t.pending,0),!0}else{if(t.current!==t.then)return i(t.then,1,t.value,e),!0;t.resolved=e}}const Ci=typeof window!=\"undefined\"?window:typeof globalThis!=\"undefined\"?globalThis:global;function zt(e,t){e.d(1),t.delete(e.key)}function It(e,t){L(e,1,1,()=>{t.delete(e.key)})}function Di(e,t){e.f(),zt(e,t)}function Oi(e,t){e.f(),It(e,t)}function Ti(e,t,n,i,o,l,s,r,f,a,c,d){let h=e.length,p=l.length,m=h;const g={};for(;m--;)g[e[m].key]=m;const u=[],_=new Map,k=new Map;for(m=p;m--;){const b=d(o,l,m),v=n(b);let q=s.get(v);q?i&&q.p(b,t):(q=a(v,b),q.c()),_.set(v,u[m]=q),v in g&&k.set(v,Math.abs(m-g[v]))}const w=new Set,M=new Set;function N(b){O(b,1),b.m(r,c),s.set(b.key,b),c=b.first,p--}for(;h&&p;){const b=u[p-1],v=e[h-1],q=b.key,D=v.key;b===v?(c=b.first,h--,p--):_.has(D)?!s.has(q)||w.has(q)?N(b):M.has(D)?h--:k.get(q)>k.get(D)?(M.add(q),N(b)):(w.add(D),h--):(f(v,s),h--)}for(;h--;){const b=e[h];_.has(b.key)||f(b,s)}for(;p;)N(u[p-1]);return u}function Li(e,t,n,i){const o=new Set;for(let l=0;l<t.length;l++){const s=i(n(e,t,l));if(o.has(s))throw new Error(\"Cannot have duplicate keys in a keyed each\");o.add(s)}}function Ni(e,t){const n={},i={},o={$$scope:1};let l=e.length;for(;l--;){const s=e[l],r=t[l];if(r){for(const f in s)f in r||(i[f]=1);for(const f in r)o[f]||(n[f]=r[f],o[f]=1);e[l]=r}else for(const f in s)o[f]=1}for(const s in i)s in n||(n[s]=void 0);return n}function Pi(e){return typeof e==\"object\"&&e!==null?e:{}}const Vt=new Set([\"allowfullscreen\",\"allowpaymentrequest\",\"async\",\"autofocus\",\"autoplay\",\"checked\",\"controls\",\"default\",\"defer\",\"disabled\",\"formnovalidate\",\"hidden\",\"ismap\",\"loop\",\"multiple\",\"muted\",\"nomodule\",\"novalidate\",\"open\",\"playsinline\",\"readonly\",\"required\",\"reversed\",\"selected\"]),Wt=/[\\s'\">/=\\u{FDD0}-\\u{FDEF}\\u{FFFE}\\u{FFFF}\\u{1FFFE}\\u{1FFFF}\\u{2FFFE}\\u{2FFFF}\\u{3FFFE}\\u{3FFFF}\\u{4FFFE}\\u{4FFFF}\\u{5FFFE}\\u{5FFFF}\\u{6FFFE}\\u{6FFFF}\\u{7FFFE}\\u{7FFFF}\\u{8FFFE}\\u{8FFFF}\\u{9FFFE}\\u{9FFFF}\\u{AFFFE}\\u{AFFFF}\\u{BFFFE}\\u{BFFFF}\\u{CFFFE}\\u{CFFFF}\\u{DFFFE}\\u{DFFFF}\\u{EFFFE}\\u{EFFFF}\\u{FFFFE}\\u{FFFFF}\\u{10FFFE}\\u{10FFFF}]/u;function Ri(e,t){const n=Object.assign({},...e);t&&(n.class==null?n.class=t:n.class+=\" \"+t);let i=\"\";return Object.keys(n).forEach(o=>{if(Wt.test(o))return;const l=n[o];l===!0?i+=\" \"+o:Vt.has(o.toLowerCase())?l&&(i+=\" \"+o):l!=null&&(i+=` ${o}=\"${String(l).replace(/\"/g,\"&#34;\").replace(/'/g,\"&#39;\")}\"`)}),i}const Yt={'\"':\"&quot;\",\"'\":\"&#39;\",\"&\":\"&amp;\",\"<\":\"&lt;\",\">\":\"&gt;\"};function Ut(e){return String(e).replace(/[\"'&<>]/g,t=>Yt[t])}function Bi(e,t){let n=\"\";for(let i=0;i<e.length;i+=1)n+=t(e[i],i);return n}const Hi={$$render:()=>\"\"};function xi(e,t){if(!e||!e.$$render)throw t===\"svelte:component\"&&(t+=\" this={...}\"),new Error(`<${t}> is not a valid SSR component. You may need to review your build config to ensure that dependencies are compiled, rather than imported as pre-compiled modules`);return e}function zi(e,t,n,i){return console.log(`{@debug} ${e?e+\" \":\"\"}(${t}:${n})`),console.log(i),\"\"}let Be;function Ii(e){function t(n,i,o,l){const s=_e,r={on_destroy:Be,context:new Map(s?s.$$.context:[]),on_mount:[],before_update:[],after_update:[],callbacks:Se()};I({$$:r});const f=e(n,i,o,l);return I(s),f}return{render:(n={},i={})=>{Be=[];const o={title:\"\",head:\"\",css:new Set},l=t(o,n,{},i);return H(Be),{html:l,css:{code:Array.from(o.css).map(s=>s.code).join(`\n",
              "`),map:null},head:o.title+o.head}},$$render:t}}function Vi(e,t,n){return t==null||n&&!t?\"\":` ${e}${t===!0?\"\":`=${typeof t==\"string\"?JSON.stringify(Ut(t)):`\"${t}\"`}`}`}function Wi(e){return e?` class=\"${e}\"`:\"\"}function lt(e,t,n){const i=e.$$.props[t];i!==void 0&&(e.$$.bound[i]=n,n(e.$$.ctx[i]))}function se(e){e&&e.c()}function Yi(e,t){e&&e.l(t)}function Z(e,t,n){const{fragment:i,on_mount:o,on_destroy:l,after_update:s}=e.$$;i&&i.m(t,n),ie(()=>{const r=o.map(Ge).filter(fe);l?l.push(...r):H(r),e.$$.on_mount=[]}),s.forEach(ie)}function X(e,t){const n=e.$$;n.fragment!==null&&(H(n.on_destroy),n.fragment&&n.fragment.d(t),n.on_destroy=n.fragment=null,n.ctx=[])}function Jt(e,t){e.$$.dirty[0]===-1&&(he.push(e),it(),e.$$.dirty.fill(0)),e.$$.dirty[t/31|0]|=1<<t%31}function He(e,t,n,i,o,l,s=[-1]){const r=_e;I(e);const f=t.props||{},a=e.$$={fragment:null,ctx:null,props:l,update:P,not_equal:o,bound:Se(),on_mount:[],on_destroy:[],before_update:[],after_update:[],context:new Map(r?r.$$.context:[]),callbacks:Se(),dirty:s,skip_bound:!1};let c=!1;if(a.ctx=n?n(e,f,(d,h,...p)=>{const m=p.length?p[0]:h;return a.ctx&&o(a.ctx[d],a.ctx[d]=m)&&(!a.skip_bound&&a.bound[d]&&a.bound[d](m),c&&Jt(e,d)),h}):[],a.update(),c=!0,H(a.before_update),a.fragment=i?i(a.ctx):!1,t.target){if(t.hydrate){const d=Tt(t.target);a.fragment&&a.fragment.l(d),d.forEach(j)}else a.fragment&&a.fragment.c();t.intro&&O(e.$$.fragment),Z(e,t.target,t.anchor),Ne()}I(r)}let Xt;typeof HTMLElement==\"function\"&&(Xt=class extends HTMLElement{constructor(){super(),this.attachShadow({mode:\"open\"})}connectedCallback(){for(const e in this.$$.slotted)this.appendChild(this.$$.slotted[e])}attributeChangedCallback(e,t,n){this[e]=n}$destroy(){X(this,1),this.$destroy=P}$on(e,t){const n=this.$$.callbacks[e]||(this.$$.callbacks[e]=[]);return n.push(t),()=>{const i=n.indexOf(t);i!==-1&&n.splice(i,1)}}$set(e){this.$$set&&!Ke(e)&&(this.$$.skip_bound=!0,this.$$set(e),this.$$.skip_bound=!1)}});class xe{$destroy(){X(this,1),this.$destroy=P}$on(t,n){const i=this.$$.callbacks[t]||(this.$$.callbacks[t]=[]);return i.push(n),()=>{const o=i.indexOf(n);o!==-1&&i.splice(o,1)}}$set(t){this.$$set&&!Ke(t)&&(this.$$.skip_bound=!0,this.$$set(t),this.$$.skip_bound=!1)}}function W(e,t){document.dispatchEvent(Ae(e,Object.assign({version:\"3.31.2\"},t)))}function Ui(e,t){W(\"SvelteDOMInsert\",{target:e,node:t}),E(e,t)}function Ji(e,t,n){W(\"SvelteDOMInsert\",{target:e,node:t,anchor:n}),C(e,t,n)}function ze(e){W(\"SvelteDOMRemove\",{node:e}),j(e)}function Xi(e,t){for(;e.nextSibling&&e.nextSibling!==t;)ze(e.nextSibling)}function Gi(e){for(;e.previousSibling;)ze(e.previousSibling)}function Ki(e){for(;e.nextSibling;)ze(e.nextSibling)}function Qi(e,t,n,i,o,l){const s=i===!0?[\"capture\"]:i?Array.from(Object.keys(i)):[];o&&s.push(\"preventDefault\"),l&&s.push(\"stopPropagation\"),W(\"SvelteDOMAddEventListener\",{node:e,event:t,handler:n,modifiers:s});const r=z(e,t,n,i);return()=>{W(\"SvelteDOMRemoveEventListener\",{node:e,event:t,handler:n,modifiers:s}),r()}}function Zi(e,t,n){y(e,t,n),n==null?W(\"SvelteDOMRemoveAttribute\",{node:e,attribute:t}):W(\"SvelteDOMSetAttribute\",{node:e,attribute:t,value:n})}function $i(e,t,n){e[t]=n,W(\"SvelteDOMSetProperty\",{node:e,property:t,value:n})}function eo(e,t,n){e.dataset[t]=n,W(\"SvelteDOMSetDataset\",{node:e,property:t,value:n})}function to(e,t){t=\"\"+t,e.wholeText!==t&&(W(\"SvelteDOMSetData\",{node:e,data:t}),e.data=t)}function no(e){if(typeof e!=\"string\"&&!(e&&typeof e==\"object\"&&\"length\"in e)){let t=\"{#each} only iterates over array-like objects.\";throw typeof Symbol==\"function\"&&e&&Symbol.iterator in e&&(t+=\" You can use a spread to convert this iterable into an array.\"),new Error(t)}}function io(e,t,n){for(const i of Object.keys(t))~n.indexOf(i)||console.warn(`<${e}> received an unexpected slot \"${i}\".`)}class oo extends null{constructor(t){if(!t||!t.target&&!t.$$inline)throw new Error(\"'target' is a required option\");super()}$destroy(){super.$destroy(),this.$destroy=()=>{console.warn(\"Component was already destroyed\")}}$capture_state(){}$inject_state(){}}class lo extends null{constructor(t){super(t)}}function so(e){const t=Date.now();return()=>{if(Date.now()-t>e)throw new Error(\"Infinite loop detected\")}}function ro(e,t){return e.map((n,i)=>n+t[i])}function uo(e,t){return t.map(n=>e*n)}function Gt(e,t,n){return e.map((i,o)=>(1-n)*i+n*t[o])}function st(e,t=2){if(!(\"length\"in e))return Kt(e,t);for(var n=0,i=0;i<e.length;i++)n+=Math.pow(Math.abs(e[i]),t);return Math.pow(n,1/t)}function Kt(e,t=2){for(var n=0,i=0;i<e.shape[0];i++)n+=Math.pow(Math.abs(e.get(i)),t);return Math.pow(n,1/t)}function Ie(e,t=2){var n=st(e,t);return e.map(i=>i/(1e-4+n))}function Qt(e){for(var t=[[1,0,0],[1,1,0],[0,1,0],[0,1,1],[0,0,1],[1,0,1]].map(s=>Ie(s,1));e<0;)e+=360;e=e%360;var n=360/t.length,i=Math.floor(e/n),o=(e-i*n)/n,l=Gt(t[i],t[(i+1)%t.length],o);return l=Ie(l,1),l}const Ve=[];function Zt(e){if(e in Ve)return Ve[e];let t=[];for(let n=0;n<e;n++){const i=360*n/e;t.push(Qt(i))}return Ve[e]=t,t}function rt(e){return`rgb(${255*e[0]}, ${255*e[1]}, ${255*e[2]})`}function ut(e,t=[.98,.98,.98],n=void 0,i=void 0){const o=\"length\"in e?e.length:e.shape[0],l=\"length\"in e?h=>e[h]:h=>e.get(h);if(i==null&&(i=Zt(o)),console.log(\"Hues\",i),n==null){for(var s=[0,0,0],r=0;r<o;r++){const h=l(r);if(h!=0)for(var f=i[r],a=0;a<3;a++)s[a]+=h*f[a]}s=Ie(s,1);for(var c=st(e,2),c=Math.max(0,Math.min(1,c)),a=0;a<3;a++)s[a]=c*s[a]+(1-c)*t[a];return s}else{for(var r=n,d=i[r],s=[0,0,0],a=0;a<3;a++)s[a]=l(r)*d[a]+(1-l(r))*t[a];return s}}function We(e,t=[.98,.98,.98],n=void 0,i=void 0){var o=ut(e,t,n,i);return rt(o)}function $t(e,t=[.98,.98,.98]){if(e>=0)for(var n=[0,.7,0],i=[0,0,0],o=0;o<3;o++)i[o]=e*n[o]+(1-e)*t[o];else for(var n=[1,0,0],i=[0,0,0],o=0;o<3;o++)i[o]=-e*n[o]+(1+e)*t[o];return console.log(\"Neuron color\",e,i),i}function fo(e,t=[.98,.98,.98]){var n=$t(e,t);return rt(n)}function en(){var e=F(\"style\");e.id=\"svelte-1fjpmsy-style\",e.textContent=\".container.svelte-1fjpmsy.svelte-1fjpmsy{position:relative;border:1px solid #aaa}.container.svelte-1fjpmsy>.svelte-1fjpmsy{position:absolute}.container.svelte-1fjpmsy canvas.svelte-1fjpmsy{left:0px;top:0px;width:100%;height:100%;image-rendering:pixelated}.container.svelte-1fjpmsy .focus-top.svelte-1fjpmsy,.container.svelte-1fjpmsy .focus-bottom.svelte-1fjpmsy{left:0px;width:100%;background:#aaa;opacity:0.3}.container.svelte-1fjpmsy .focus-top.svelte-1fjpmsy{top:0px}.container.svelte-1fjpmsy .focus-bottom.svelte-1fjpmsy{bottom:0px}.container.svelte-1fjpmsy .focus-left.svelte-1fjpmsy,.container.svelte-1fjpmsy .focus-right.svelte-1fjpmsy{top:0px;height:100%;background:#aaa;opacity:0.3}.container.svelte-1fjpmsy .focus-left.svelte-1fjpmsy{left:0px}.container.svelte-1fjpmsy .focus-right.svelte-1fjpmsy{right:0px}\",E(document.head,e)}function ft(e){let t,n,i,o=e[3]!=null&&at(e);return{c(){t=F(\"div\"),n=F(\"canvas\"),i=T(),o&&o.c(),S(n,\"width\",e[1]+\"px\"),S(n,\"height\",e[2]+\"px\"),y(n,\"class\",\"svelte-1fjpmsy\"),y(t,\"class\",\"container svelte-1fjpmsy\"),S(t,\"width\",e[1]+\"px\"),S(t,\"height\",e[2]+\"px\")},m(l,s){C(l,t,s),E(t,n),e[9](n),E(t,i),o&&o.m(t,null)},p(l,s){s&2&&S(n,\"width\",l[1]+\"px\"),s&4&&S(n,\"height\",l[2]+\"px\"),l[3]!=null?o?o.p(l,s):(o=at(l),o.c(),o.m(t,null)):o&&(o.d(1),o=null),s&2&&S(t,\"width\",l[1]+\"px\"),s&4&&S(t,\"height\",l[2]+\"px\")},d(l){l&&j(t),e[9](null),o&&o.d()}}}function at(e){let t,n,i;return{c(){t=F(\"div\"),n=T(),i=F(\"div\"),y(t,\"class\",\"focus-top svelte-1fjpmsy\"),S(t,\"height\",e[2]*e[3]/e[0].shape[0]+\"px\"),y(i,\"class\",\"focus-bottom svelte-1fjpmsy\"),S(i,\"height\",e[2]*(1-(e[3]+1)/e[0].shape[0])+\"px\")},m(o,l){C(o,t,l),C(o,n,l),C(o,i,l)},p(o,l){l&13&&S(t,\"height\",o[2]*o[3]/o[0].shape[0]+\"px\"),l&13&&S(i,\"height\",o[2]*(1-(o[3]+1)/o[0].shape[0])+\"px\")},d(o){o&&j(t),o&&j(n),o&&j(i)}}}function ct(e){let t,n,i,o=e[3]!=null&&dt(e);return{c(){t=F(\"div\"),n=F(\"canvas\"),i=T(),o&&o.c(),S(n,\"width\",e[1]+\"px\"),S(n,\"height\",e[2]+\"px\"),y(n,\"class\",\"svelte-1fjpmsy\"),y(t,\"class\",\"container svelte-1fjpmsy\"),S(t,\"width\",e[1]+\"px\"),S(t,\"height\",e[2]+\"px\")},m(l,s){C(l,t,s),E(t,n),e[10](n),E(t,i),o&&o.m(t,null)},p(l,s){s&2&&S(n,\"width\",l[1]+\"px\"),s&4&&S(n,\"height\",l[2]+\"px\"),l[3]!=null?o?o.p(l,s):(o=dt(l),o.c(),o.m(t,null)):o&&(o.d(1),o=null),s&2&&S(t,\"width\",l[1]+\"px\"),s&4&&S(t,\"height\",l[2]+\"px\")},d(l){l&&j(t),e[10](null),o&&o.d()}}}function dt(e){let t,n,i;return{c(){t=F(\"div\"),n=T(),i=F(\"div\"),y(t,\"class\",\"focus-left svelte-1fjpmsy\"),S(t,\"width\",e[1]*e[3]/e[0].shape[1]+\"px\"),y(i,\"class\",\"focus-right svelte-1fjpmsy\"),S(i,\"width\",e[1]*(1-(e[3]+1)/e[0].shape[1])+\"px\")},m(o,l){C(o,t,l),C(o,n,l),C(o,i,l)},p(o,l){l&11&&S(t,\"width\",o[1]*o[3]/o[0].shape[1]+\"px\"),l&11&&S(i,\"width\",o[1]*(1-(o[3]+1)/o[0].shape[1])+\"px\")},d(o){o&&j(t),o&&j(n),o&&j(i)}}}function tn(e){let t,n,i=!e[4]&&ft(e),o=e[4]&&ct(e);return{c(){i&&i.c(),t=T(),o&&o.c(),n=tt()},m(l,s){i&&i.m(l,s),C(l,t,s),o&&o.m(l,s),C(l,n,s)},p(l,[s]){l[4]?i&&(i.d(1),i=null):i?i.p(l,s):(i=ft(l),i.c(),i.m(t.parentNode,t)),l[4]?o?o.p(l,s):(o=ct(l),o.c(),o.m(n.parentNode,n)):o&&(o.d(1),o=null)},i:P,o:P,d(l){i&&i.d(l),l&&j(t),o&&o.d(l),l&&j(n)}}}function nn(e,t,n){let{array:i}=t,{width:o}=t,{height:l}=t,{hues:s}=t,{focus_token:r}=t,{isolate_channel:f=void 0}=t,{hover_token_is_target:a=!1}=t,{color_map:c=ut}=t,d;function h(u,_,k,w=void 0,M=void 0){if(_<k)return[255,255,255];var N=u.pick(_,k,null),b=c(N,void 0,w,M);return b.map(v=>255*v)}function p(u,_,k=void 0,w=void 0){if(!(u==null||_==null)){u.width=_.shape[0],u.height=_.shape[1];for(var M=u.getContext(\"2d\"),N=M.getImageData(0,0,u.width,u.height),b=0;b<u.width;b++)for(var v=0;v<u.height;v++){for(var q=b*u.width+v,D=h(_,b,v,k,w=w),J=0;J<3;J++)N.data[4*q+J]=D[J];N.data[4*q+3]=255}M.putImageData(N,0,0)}}Ht(()=>p(d,i,f));function m(u){ne[u?\"unshift\":\"push\"](()=>{d=u,n(5,d)})}function g(u){ne[u?\"unshift\":\"push\"](()=>{d=u,n(5,d)})}return e.$$set=u=>{\"array\"in u&&n(0,i=u.array),\"width\"in u&&n(1,o=u.width),\"height\"in u&&n(2,l=u.height),\"hues\"in u&&n(6,s=u.hues),\"focus_token\"in u&&n(3,r=u.focus_token),\"isolate_channel\"in u&&n(7,f=u.isolate_channel),\"hover_token_is_target\"in u&&n(4,a=u.hover_token_is_target),\"color_map\"in u&&n(8,c=u.color_map)},e.$$.update=()=>{if(e.$$.dirty&225){e:p(d,i,f,s)}},[i,o,l,r,a,d,s,f,c,m,g]}class on extends xe{constructor(t){super(),document.getElementById(\"svelte-1fjpmsy-style\")||en(),He(this,t,nn,tn,je,{array:0,width:1,height:2,hues:6,focus_token:3,isolate_channel:7,hover_token_is_target:4,color_map:8})}}const qe=on;function _t(e,t){return e.mode==\"soft\"&&(e.value=t),e}function ln(e,t){if(e.mode==\"soft\")e.value=t,e.mode=\"hard\";else if(e.mode==\"hard\"&&e.value!=t)e.value=t;else return sn(e);return e}function sn(e){return e.value=void 0,e.mode=\"soft\",e}function rn(e){let t,n,i,o;const l=e[4].default,s=At(l,e,e[3],null);return{c(){t=F(\"div\"),s&&s.c(),y(t,\"style\",e[2])},m(r,f){C(r,t,f),s&&s.m(t,null),n=!0,i||(o=[z(t,\"mouseover\",e[5]),z(t,\"click\",e[6]),z(t,\"mouseout\",e[7])],i=!0)},p(r,[f]){s&&s.p&&f&8&&Ct(s,l,r,r[3],f,null,null),(!n||f&4)&&y(t,\"style\",r[2])},i(r){n||(O(s,r),n=!0)},o(r){L(s,r),n=!1},d(r){r&&j(t),s&&s.d(r),i=!1,H(o)}}}function un(e,t,n){let{$$slots:i={},$$scope:o}=t,{lock:l}=t,{set_value:s}=t,{style:r=\"\"}=t;const f=()=>{n(0,l=_t(l,s))},a=()=>{n(0,l=ln(l,s))},c=()=>{n(0,l=_t(l,void 0))};return e.$$set=d=>{\"lock\"in d&&n(0,l=d.lock),\"set_value\"in d&&n(1,s=d.set_value),\"style\"in d&&n(2,r=d.style),\"$$scope\"in d&&n(3,o=d.$$scope)},[l,s,r,o,i,f,a,c]}class fn extends xe{constructor(t){super(),He(this,t,un,rn,je,{lock:0,set_value:1,style:2})}}const ht=fn;function an(){var e=F(\"style\");e.id=\"svelte-xqk9oe-style\",e.textContent=\".attn-container.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{display:grid;grid-template-rows:[title] min-content [main] min-content;grid-template-columns:[big-attn] min-content [heads] minmax(min-content, 624px);gap:12px}.figcaption.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{color:#888;grid-row:title;white-space:nowrap}.tokens-container.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{display:grid;grid-template-rows:[title] min-content [main] min-content;grid-template-columns:[left] min-content [right] minmax(min-content, 800px) [end];gap:12px;margin-top:24px;color:white}.tokens.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{grid-row:main;grid-column-start:left;grid-column-end:end;cursor:pointer;height:min-content;line-height:110%}.tokens.svelte-xqk9oe .token.svelte-xqk9oe.svelte-xqk9oe{white-space:pre-wrap}.tokens.svelte-xqk9oe .selected.svelte-xqk9oe.svelte-xqk9oe{border:1px solid #999;z-index:10}.tokens.svelte-xqk9oe .token.svelte-xqk9oe.svelte-xqk9oe:not(.selected){z-index:0;padding:1px}.hover-mode.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe,.hover-mode-text.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe,.info-mode.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe,.info-mode-text.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{color:#888;grid-row:title;grid-column:settings;cursor:pointer}.hover-mode-text.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe,.info-mode-text.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{margin-right:8px}.heads.svelte-xqk9oe.svelte-xqk9oe.svelte-xqk9oe{grid-column:heads;grid-row:main;display:flex;flex-direction:row;flex-wrap:wrap;gap:6px;height:min-content}.heads.svelte-xqk9oe .head-icon.svelte-xqk9oe.svelte-xqk9oe{position:relative;width:62px;height:62px}.heads.svelte-xqk9oe .head-icon.svelte-xqk9oe>.svelte-xqk9oe{position:absolute;right:0px;top:0px}.heads.svelte-xqk9oe .head-icon .head-label.svelte-xqk9oe.svelte-xqk9oe{background:#333;color:#eee;font-size:65%;padding:1px;border-bottom-left-radius:2px;padding-left:4px;padding-right:2px;min-width:14px;opacity:0.75}\",E(document.head,e)}function pt(e,t,n){const i=e.slice();return i[32]=t[n],i[34]=n,i}function mt(e,t,n){const i=e.slice();return i[35]=t[n],i}function gt(e){let t,n=e[15][e[10]]+\"\",i,o;return{c(){t=R(\"(\"),i=R(n),o=R(\")\")},m(l,s){C(l,t,s),C(l,i,s),C(l,o,s)},p(l,s){s[0]&33792&&n!==(n=l[15][l[10]]+\"\")&&ke(i,n)},d(l){l&&j(t),l&&j(i),l&&j(o)}}}function vt(e){let t,n,i,o;return n=new qe({props:{array:e[7],width:\"200\",height:\"200\",focus_token:e[9],hover_token_is_target:e[2],isolate_channel:e[10]}}),{c(){t=F(\"div\"),se(n.$$.fragment),y(t,\"style\",i=\"grid-column: big-attn; grid-row: main; \"+(e[11]?\"\":\"display:none;\"))},m(l,s){C(l,t,s),Z(n,t,null),o=!0},p(l,s){const r={};s[0]&128&&(r.array=l[7]),s[0]&512&&(r.focus_token=l[9]),s[0]&4&&(r.hover_token_is_target=l[2]),s[0]&1024&&(r.isolate_channel=l[10]),n.$set(r),(!o||s[0]&2048&&i!==(i=\"grid-column: big-attn; grid-row: main; \"+(l[11]?\"\":\"display:none;\")))&&y(t,\"style\",i)},i(l){o||(O(n.$$.fragment,l),o=!0)},o(l){L(n.$$.fragment,l),o=!1},d(l){l&&j(t),X(n)}}}function kt(e){let t,n,i,o,l=re(e[12].shape[2]),s=[];for(let f=0;f<l.length;f+=1)s[f]=bt(mt(e,l,f));const r=f=>L(s[f],1,1,()=>{s[f]=null});return{c(){t=F(\"div\"),t.textContent=\"Attention Heads (hover to focus, click to lock)\",n=T(),i=F(\"div\");for(let f=0;f<s.length;f+=1)s[f].c();y(t,\"class\",\"figcaption svelte-xqk9oe\"),S(t,\"grid-column\",\"heads\"),y(i,\"class\",\"heads svelte-xqk9oe\")},m(f,a){C(f,t,a),C(f,n,a),C(f,i,a);for(let c=0;c<s.length;c+=1)s[c].m(i,null);o=!0},p(f,a){if(a[0]&122566){l=re(f[12].shape[2]);let c;for(c=0;c<l.length;c+=1){const d=mt(f,l,c);s[c]?(s[c].p(d,a),O(s[c],1)):(s[c]=bt(d),s[c].c(),O(s[c],1),s[c].m(i,null))}for(oe(),c=l.length;c<s.length;c+=1)r(c);le()}},i(f){if(!o){for(let a=0;a<l.length;a+=1)O(s[a]);o=!0}},o(f){s=s.filter(Boolean);for(let a=0;a<s.length;a+=1)L(s[a]);o=!1},d(f){f&&j(t),f&&j(n),f&&j(i),et(s,f)}}}function cn(e){let t,n,i,o,l=(e[15][e[35]]!=null?e[15][e[35]]:\"&nbsp\")+\"\",s,r,f,a,c,d,h=(e[15][e[35]]!=null?e[15][e[35]]:\"&nbsp\")+\"\",p,m,g;return n=new qe({props:{array:e[7],width:\"60\",height:\"60\",isolate_channel:e[35]}}),a=new qe({props:{array:e[6],width:\"60\",height:\"60\",isolate_channel:e[35]}}),{c(){t=F(\"div\"),se(n.$$.fragment),i=T(),o=F(\"div\"),r=T(),f=F(\"div\"),se(a.$$.fragment),c=T(),d=F(\"div\"),m=T(),y(o,\"class\",\"head-label svelte-xqk9oe\"),S(o,\"background\",e[14][e[35]]),y(t,\"class\",\"head-icon svelte-xqk9oe\"),y(t,\"style\",s=\"opacity: \"+(e[10]!=null&&e[10]!=e[35]?\"0.2\":e[16](e[35],e[9],e[2]))+`;\n",
              "                        `+(e[11]?\"\":\"display:none;\")),y(d,\"class\",\"head-label svelte-xqk9oe\"),S(d,\"background\",e[14][e[35]]),y(f,\"class\",\"head-icon svelte-xqk9oe\"),y(f,\"style\",p=\"opacity: \"+(e[10]!=null&&e[10]!=e[35]?\"0.2\":e[16](e[35],e[9],e[2]))+`;\n",
              "                        `+(e[11]?\"display:none;\":\"\"))},m(u,_){C(u,t,_),Z(n,t,null),E(t,i),E(t,o),o.innerHTML=l,C(u,r,_),C(u,f,_),Z(a,f,null),E(f,c),E(f,d),d.innerHTML=h,C(u,m,_),g=!0},p(u,_){const k={};_[0]&128&&(k.array=u[7]),_[0]&4096&&(k.isolate_channel=u[35]),n.$set(k),(!g||_[0]&36864)&&l!==(l=(u[15][u[35]]!=null?u[15][u[35]]:\"&nbsp\")+\"\")&&(o.innerHTML=l),(!g||_[0]&20480)&&S(o,\"background\",u[14][u[35]]),(!g||_[0]&7684&&s!==(s=\"opacity: \"+(u[10]!=null&&u[10]!=u[35]?\"0.2\":u[16](u[35],u[9],u[2]))+`;\n",
              "                        `+(u[11]?\"\":\"display:none;\")))&&y(t,\"style\",s);const w={};_[0]&64&&(w.array=u[6]),_[0]&4096&&(w.isolate_channel=u[35]),a.$set(w),(!g||_[0]&36864)&&h!==(h=(u[15][u[35]]!=null?u[15][u[35]]:\"&nbsp\")+\"\")&&(d.innerHTML=h),(!g||_[0]&20480)&&S(d,\"background\",u[14][u[35]]),(!g||_[0]&7684&&p!==(p=\"opacity: \"+(u[10]!=null&&u[10]!=u[35]?\"0.2\":u[16](u[35],u[9],u[2]))+`;\n",
              "                        `+(u[11]?\"display:none;\":\"\")))&&y(f,\"style\",p)},i(u){g||(O(n.$$.fragment,u),O(a.$$.fragment,u),g=!0)},o(u){L(n.$$.fragment,u),L(a.$$.fragment,u),g=!1},d(u){u&&j(t),X(n),u&&j(r),u&&j(f),X(a),u&&j(m)}}}function bt(e){let t,n,i;function o(s){e[22].call(null,s)}let l={set_value:e[35],$$slots:{default:[cn]},$$scope:{ctx:e}};return e[1]!==void 0&&(l.lock=e[1]),t=new ht({props:l}),ne.push(()=>lt(t,\"lock\",o)),{c(){se(t.$$.fragment)},m(s,r){Z(t,s,r),i=!0},p(s,r){const f={};r[0]&4096&&(f.set_value=s[35]),r[0]&57028|r[1]&128&&(f.$$scope={dirty:r,ctx:s}),!n&&r[0]&2&&(n=!0,f.lock=s[1],ot(()=>n=!1)),t.$set(f)},i(s){i||(O(t.$$.fragment,s),i=!0)},o(s){L(t.$$.fragment,s),i=!1},d(s){X(t,s)}}}function yt(e){let t,n,i,o,l,s,r,f,a,c,d,h,p=e[2]?\"target\":\"source\",m,g,u,_,k,w=e[5],M=[];for(let v=0;v<w.length;v+=1)M[v]=wt(pt(e,w,v));const N=v=>L(M[v],1,1,()=>{M[v]=null});let b=e[7]!==void 0&&Ft(e);return{c(){t=F(\"div\"),n=F(\"div\"),n.textContent=\"Tokens (hover to focus, click to lock)\",i=T(),o=F(\"div\");for(let v=0;v<M.length;v+=1)M[v].c();l=T(),s=F(\"div\"),r=F(\"nobr\"),f=F(\"input\"),a=T(),c=F(\"span\"),d=R(`Selected is\n",
              "            `),h=F(\"b\"),m=R(p),g=T(),b&&b.c(),y(n,\"class\",\"figcaption svelte-xqk9oe\"),S(n,\"grid-column\",\"left\"),y(o,\"class\",\"tokens svelte-xqk9oe\"),y(f,\"class\",\"hover-mode svelte-xqk9oe\"),y(f,\"type\",\"checkbox\"),y(c,\"class\",\"hover-mode-text svelte-xqk9oe\"),S(c,\"white-space\",\"nowrap\"),y(s,\"class\",\"toggle\"),y(t,\"class\",\"tokens-container svelte-xqk9oe\")},m(v,q){C(v,t,q),E(t,n),E(t,i),E(t,o);for(let D=0;D<M.length;D+=1)M[D].m(o,null);E(t,l),E(t,s),E(s,r),E(r,f),f.checked=e[2],E(r,a),E(r,c),E(c,d),E(c,h),E(h,m),E(r,g),b&&b.m(r,null),u=!0,_||(k=[z(f,\"change\",e[24]),z(c,\"click\",e[25])],_=!0)},p(v,q){if(q[0]&561){w=v[5];let D;for(D=0;D<w.length;D+=1){const J=pt(v,w,D);M[D]?(M[D].p(J,q),O(M[D],1)):(M[D]=wt(J),M[D].c(),O(M[D],1),M[D].m(o,null))}for(oe(),D=w.length;D<M.length;D+=1)N(D);le()}q[0]&4&&(f.checked=v[2]),(!u||q[0]&4)&&p!==(p=v[2]?\"target\":\"source\")&&ke(m,p),v[7]!==void 0?b?b.p(v,q):(b=Ft(v),b.c(),b.m(r,null)):b&&(b.d(1),b=null)},i(v){if(!u){for(let q=0;q<w.length;q+=1)O(M[q]);u=!0}},o(v){M=M.filter(Boolean);for(let q=0;q<M.length;q+=1)L(M[q]);u=!1},d(v){v&&j(t),et(M,v),b&&b.d(),_=!1,H(k)}}}function dn(e){let t,n=e[32]+\"\",i,o;return{c(){t=F(\"span\"),i=R(n),y(t,\"class\",o=\"token \"+(e[34]==e[9]?\"selected\":\"\")+\" svelte-xqk9oe\"),S(t,\"background\",e[4][e[34]])},m(l,s){C(l,t,s),E(t,i)},p(l,s){s[0]&32&&n!==(n=l[32]+\"\")&&ke(i,n),s[0]&512&&o!==(o=\"token \"+(l[34]==l[9]?\"selected\":\"\")+\" svelte-xqk9oe\")&&y(t,\"class\",o),s[0]&16&&S(t,\"background\",l[4][l[34]])},d(l){l&&j(t)}}}function wt(e){let t,n,i;function o(s){e[23].call(null,s)}let l={set_value:e[34],style:\"display: inline\",$$slots:{default:[dn]},$$scope:{ctx:e}};return e[0]!==void 0&&(l.lock=e[0]),t=new ht({props:l}),ne.push(()=>lt(t,\"lock\",o)),{c(){se(t.$$.fragment)},m(s,r){Z(t,s,r),i=!0},p(s,r){const f={};r[0]&560|r[1]&128&&(f.$$scope={dirty:r,ctx:s}),!n&&r[0]&1&&(n=!0,f.lock=s[0],ot(()=>n=!1)),t.$set(f)},i(s){i||(O(t.$$.fragment,s),i=!0)},o(s){L(t.$$.fragment,s),i=!1},d(s){X(t,s)}}}function Ft(e){let t,n,i,o,l,s=e[3]?\"info-weighted\":\"unmodified\",r,f,a;return{c(){t=F(\"input\"),n=T(),i=F(\"span\"),o=R(`Attention is\n",
              "            `),l=F(\"b\"),r=R(s),y(t,\"class\",\"info-mode svelte-xqk9oe\"),y(t,\"type\",\"checkbox\"),y(i,\"class\",\"info-mode-text svelte-xqk9oe\"),S(i,\"white-space\",\"nowrap\")},m(c,d){C(c,t,d),t.checked=e[3],C(c,n,d),C(c,i,d),E(i,o),E(i,l),E(l,r),f||(a=[z(t,\"change\",e[26]),z(i,\"click\",e[27])],f=!0)},p(c,d){d[0]&8&&(t.checked=c[3]),d[0]&8&&s!==(s=c[3]?\"info-weighted\":\"unmodified\")&&ke(r,s)},d(c){c&&j(t),c&&j(n),c&&j(i),f=!1,H(a)}}}function _n(e){let t,n,i,o,l,s,r,f,a,c,d,h,p=e[10]!=null&&gt(e),m=e[7]!==void 0&&vt(e);r=new qe({props:{array:e[6],width:\"200\",height:\"200\",focus_token:e[9],hover_token_is_target:e[2],isolate_channel:e[10]}});let g=e[13]>1&&kt(e),u=e[8]&&yt(e);return{c(){t=F(\"div\"),n=F(\"div\"),i=R(`Attention Pattern\n",
              "        `),p&&p.c(),o=T(),m&&m.c(),l=T(),s=F(\"div\"),se(r.$$.fragment),a=T(),g&&g.c(),c=T(),u&&u.c(),d=tt(),y(n,\"class\",\"figcaption svelte-xqk9oe\"),S(n,\"grid-column\",\"big-attn\"),y(s,\"style\",f=\"grid-column: big-attn; grid-row: main; \"+(e[11]?\"display:none\":\"\")),y(t,\"class\",\"attn-container svelte-xqk9oe\")},m(_,k){C(_,t,k),E(t,n),E(n,i),p&&p.m(n,null),E(t,o),m&&m.m(t,null),E(t,l),E(t,s),Z(r,s,null),E(t,a),g&&g.m(t,null),C(_,c,k),u&&u.m(_,k),C(_,d,k),h=!0},p(_,k){_[10]!=null?p?p.p(_,k):(p=gt(_),p.c(),p.m(n,null)):p&&(p.d(1),p=null),_[7]!==void 0?m?(m.p(_,k),k[0]&128&&O(m,1)):(m=vt(_),m.c(),O(m,1),m.m(t,l)):m&&(oe(),L(m,1,1,()=>{m=null}),le());const w={};k[0]&64&&(w.array=_[6]),k[0]&512&&(w.focus_token=_[9]),k[0]&4&&(w.hover_token_is_target=_[2]),k[0]&1024&&(w.isolate_channel=_[10]),r.$set(w),(!h||k[0]&2048&&f!==(f=\"grid-column: big-attn; grid-row: main; \"+(_[11]?\"display:none\":\"\")))&&y(s,\"style\",f),_[13]>1?g?(g.p(_,k),k[0]&8192&&O(g,1)):(g=kt(_),g.c(),O(g,1),g.m(t,null)):g&&(oe(),L(g,1,1,()=>{g=null}),le()),_[8]?u?(u.p(_,k),k[0]&256&&O(u,1)):(u=yt(_),u.c(),O(u,1),u.m(d.parentNode,d)):u&&(oe(),L(u,1,1,()=>{u=null}),le())},i(_){h||(O(m),O(r.$$.fragment,_),O(g),O(u),h=!0)},o(_){L(m),L(r.$$.fragment,_),L(g),L(u),h=!1},d(_){_&&j(t),p&&p.d(),m&&m.d(),X(r),g&&g.d(),_&&j(c),u&&u.d(_),_&&j(d)}}}function re(e){return[...Array(e).keys()]}function qt(e){if(e!==void 0){for(var t=[],n=0;n<e.shape[0];n++){t.push([]);for(var i=0;i<e.shape[2];i++){for(var o=0,l=0;l<e.shape[1];l++)o=Math.max(o,e.pick(n,l,i));t[n].push(o)}}return t}}function Et(e){if(e!==void 0){for(var t=[],n=0;n<e.shape[0];n++){t.push([]);for(var i=0;i<e.shape[2];i++){for(var o=0,l=0;l<e.shape[1];l++)o=Math.max(o,e.pick(l,n,i));t[n].push(o)}}return t}}function hn(e,t,n){let i,o,l,s,r,f,a,c,d,h,p,m,g,{tokens:u}=t,{attention:_}=t,{info_weighted:k}=t,{head_labels:w}=t,{show_tokens:M=!0}=t,{focus_token_lock:N={value:void 0,mode:\"soft\"}}=t,{focus_head_lock:b={value:void 0,mode:\"soft\"}}=t,{hover_token_is_target:v=!1}=t,{_show_info_weighted:q=!1}=t;function D(A,Y,G,$=void 0){if(Y<G)return\"#FFF\";var ue=A.pick(Y,G,null);return We(ue,void 0,$)}function J(A,Y,G){if(Y==null)var $=1;else var ue=G?h:d,$=Math.max(0,Math.min(1,ue[Y][A]));return\"\"+$}function gn(A,Y,G,$=void 0,ue){if(Y==null){var qn=ue?h:d;return We(qn[G],void 0,$)}let Ue,Je;ue?(Ue=G,Je=Y):(Ue=Y,Je=G);let Xe=D(A,Ue,Je,$);return Xe===\"#FFF\"&&(Xe=\"#DDD\"),Xe}let{all_token_colors:Ye}=t;function vn(A){b=A,n(1,b)}function kn(A){N=A,n(0,N)}function bn(){v=this.checked,n(2,v)}const yn=()=>n(2,v=v^!0);function wn(){q=this.checked,n(3,q)}const Fn=()=>n(3,q=q^!0);return e.$$set=A=>{\"tokens\"in A&&n(5,u=A.tokens),\"attention\"in A&&n(6,_=A.attention),\"info_weighted\"in A&&n(7,k=A.info_weighted),\"head_labels\"in A&&n(17,w=A.head_labels),\"show_tokens\"in A&&n(8,M=A.show_tokens),\"focus_token_lock\"in A&&n(0,N=A.focus_token_lock),\"focus_head_lock\"in A&&n(1,b=A.focus_head_lock),\"hover_token_is_target\"in A&&n(2,v=A.hover_token_is_target),\"_show_info_weighted\"in A&&n(3,q=A._show_info_weighted),\"all_token_colors\"in A&&n(4,Ye=A.all_token_colors)},e.$$.update=()=>{if(e.$$.dirty[0]&1){e:n(9,i=N.value)}if(e.$$.dirty[0]&2){e:n(10,o=b.value)}if(e.$$.dirty[0]&136){e:n(11,l=q&&k!==void 0)}if(e.$$.dirty[0]&2240){e:n(12,s=l?k:_)}if(e.$$.dirty[0]&4096){e:window.attention_show=s}if(e.$$.dirty[0]&64){e:n(18,r=qt(_))}if(e.$$.dirty[0]&64){e:n(19,f=Et(_))}if(e.$$.dirty[0]&128){e:n(20,a=qt(k))}if(e.$$.dirty[0]&128){e:n(21,c=Et(k))}if(e.$$.dirty[0]&1312768){e:d=l?a:r}if(e.$$.dirty[0]&2623488){e:h=l?c:f}if(e.$$.dirty[0]&64){e:n(13,p=_.shape[2])}if(e.$$.dirty[0]&8192){e:n(14,m=re(p).map(A=>We(re(p).map(Y=>1),void 0,A)))}if(e.$$.dirty[0]&139264){e:n(15,g=w!=null?w:re(p))}if(e.$$.dirty[0]&5668){e:n(4,Ye=re(u.length).map(A=>gn(s,i,A,o,v)))}},[N,b,v,q,Ye,u,_,k,M,i,o,l,s,p,m,g,J,w,r,f,a,c,vn,kn,bn,yn,wn,Fn]}class pn extends xe{constructor(t){super(),document.getElementById(\"svelte-xqk9oe-style\")||an(),He(this,t,hn,_n,je,{tokens:5,attention:6,info_weighted:7,head_labels:17,show_tokens:8,focus_token_lock:0,focus_head_lock:1,hover_token_is_target:2,_show_info_weighted:3,all_token_colors:4},[-1,-1])}}const mn=pn}},Ee={};function ee(x){if(Ee[x])return Ee[x].exports;var B=Ee[x]={exports:{}};return St[x](B,B.exports,ee),B.exports}return ee.d=(x,B)=>{for(var te in B)ee.o(B,te)&&!ee.o(x,te)&&Object.defineProperty(x,te,{enumerable:!0,get:B[te]})},ee.o=(x,B)=>Object.prototype.hasOwnProperty.call(x,B),ee(143)})().default;\n",
              "</script>\n",
              "        \n",
              "        <div id=\"AttentionMulti_3ea3db3\"></div>\n",
              "        <script>\n",
              "        ( () => {\n",
              "            var data = {\n",
              "\"tokens\": [\n",
              "\"<|endoftext|>\",\n",
              "\"I\",\n",
              "\" am\",\n",
              "\" an\",\n",
              "\" amazing\",\n",
              "\" aut\",\n",
              "\"ore\",\n",
              "\"gressive\",\n",
              "\",\",\n",
              "\" dec\",\n",
              "\"oder\",\n",
              "\"-\",\n",
              "\"only\",\n",
              "\",\",\n",
              "\" G\",\n",
              "\"PT\",\n",
              "\"-\",\n",
              "\"2\",\n",
              "\" style\",\n",
              "\" transformer\",\n",
              "\".\",\n",
              "\" One\",\n",
              "\" day\",\n",
              "\" I\",\n",
              "\" will\",\n",
              "\" exceed\",\n",
              "\" human\",\n",
              "\" level\",\n",
              "\" intelligence\",\n",
              "\" and\",\n",
              "\" take\",\n",
              "\" over\",\n",
              "\" the\",\n",
              "\" world\",\n",
              "\"!\"\n",
              "],\n",
              "\"attention\": {\n",
              "\"__type__\": \"npy\",\n",
              "\"zdata\": \"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\",\n",
              "\"min\": 0.0,\n",
              "\"max\": 1.0\n",
              "}\n",
              "};\n",
              "            data = loader.unpack_obj(data);\n",
              "            window.AttentionMulti_data = data;\n",
              "            var AttentionMulti_inst = new AttentionMulti({\n",
              "                \"target\": document.getElementById(\"AttentionMulti_3ea3db3\"),\n",
              "                \"props\": data\n",
              "                });\n",
              "        })();\n",
              "        </script>\n",
              "        \n",
              "        "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pysvelte\n",
        "pysvelte.AttentionMulti(tokens=reference_gpt2.to_str_tokens(reference_text), attention=cache['blocks.0.attn.hook_attn'][0].permute(1, 2, 0)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SilqwTsgx621",
        "outputId": "49cae508-8e7b-4bd4-dfb8-76042ab3f04b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG - Reference] Reference Attention output (first 5 elements): tensor([ 0.7966,  0.0170,  0.0348, -0.0554,  0.0905], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "Input shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "Output shape: torch.Size([2, 4, 768])\n",
            "\n",
            "Input shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "Output shape: torch.Size([1, 35, 768])\n",
            "Reference output shape: torch.Size([1, 35, 768])\n",
            "100.00% of the values are correct\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 7.9663e-01,  1.6986e-02,  3.4781e-02,  ...,  3.3120e-02,\n",
              "          -2.3129e-02,  1.8103e-01],\n",
              "         [ 1.3167e-03,  1.5750e-01, -1.4059e-01,  ..., -8.1997e-03,\n",
              "           5.3076e-03,  1.3511e-01],\n",
              "         [ 8.9738e-02, -7.2411e-01, -6.9866e-01,  ...,  5.5321e-02,\n",
              "           2.7959e-03,  9.0785e-02],\n",
              "         ...,\n",
              "         [-3.0286e-01,  4.9638e-02, -6.0990e-01,  ..., -3.7084e-02,\n",
              "          -4.9522e-04, -8.6008e-03],\n",
              "         [-1.0844e+00, -6.1457e-02,  2.2966e-01,  ..., -2.6688e-02,\n",
              "          -1.4368e-02,  3.3245e-02],\n",
              "         [ 3.7947e-01, -4.9886e-01,  2.6434e-01,  ..., -2.7894e-02,\n",
              "          -8.9028e-03,  4.8796e-02]]], device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
        "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
        "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
        "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
        "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
        "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
        "\n",
        "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
        "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
        "\n",
        "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
        "\n",
        "    def forward(self, normalized_resid_pre):\n",
        "        # normalized_resid_pre: [batch, position, d_model]\n",
        "        if self.cfg.debug: print(\"Normalized_resid_pre:\", normalized_resid_pre.shape)\n",
        "\n",
        "        q = einsum(\"batch query_pos d_model, n_heads d_model d_head -> batch query_pos n_heads d_head\", normalized_resid_pre, self.W_Q) + self.b_Q\n",
        "        k = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_K) + self.b_K\n",
        "\n",
        "        attn_scores = einsum(\"batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos\", q, k)\n",
        "        attn_scores = attn_scores / math.sqrt(self.cfg.d_head)\n",
        "        attn_scores = self.apply_causal_mask(attn_scores)\n",
        "\n",
        "        pattern = attn_scores.softmax(dim=-1) # [batch, n_head, query_pos, key_pos]\n",
        "\n",
        "        v = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_V) + self.b_V\n",
        "\n",
        "        z = einsum(\"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\", pattern, v)\n",
        "\n",
        "        attn_out = einsum(\"batch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos d_model\", z, self.W_O) + self.b_O\n",
        "        return attn_out\n",
        "\n",
        "\n",
        "    def apply_causal_mask(self, attn_scores):\n",
        "          mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device), diagonal=1).bool()\n",
        "          attn_scores.masked_fill_(mask, self.IGNORE)\n",
        "          return attn_scores\n",
        "\n",
        "\n",
        "reference_attn_out = reference_gpt2.blocks[0].attn(cache[\"blocks.0.ln1.hook_normalized\"])\n",
        "print(f\"[DEBUG - Reference] Reference Attention output (first 5 elements): {reference_attn_out.flatten()[:5]}\")\n",
        "\n",
        "rand_float_test(Attention, [2, 4, 768])\n",
        "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"blocks.0.ln1.hook_normalized\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# # Input sequence (batch=1, pos=3, d_model=4)\n",
        "# x = torch.tensor([[[1.0, 2.0, 3.0, 4.0],\n",
        "#                    [5.0, 6.0, 7.0, 8.0],\n",
        "#                    [9.0, 10.0, 11.0, 12.0]]])  # [1, 3, 4]\n",
        "\n",
        "# # Weight matrix (n_heads=2, d_model=4, d_head=2)\n",
        "# W_Q = torch.tensor([[[0.1, 0.2], [0.3, 0.4], [0.5, 0.6], [0.7, 0.8]],\n",
        "#                     [[-0.1, -0.2], [-0.3, -0.4], [-0.5, -0.6], [-0.7, -0.8]]])  # [2, 4, 2]\n",
        "\n",
        "# # Linear transformation\n",
        "# Q = torch.einsum('bpd,hdq->bphq', x, W_Q)\n",
        "# print(\"Q shape:\", Q.shape)  # [1, 3, 2, 2]\n",
        "# print(Q)\n"
      ],
      "metadata": {
        "id": "oeoSX2KPCrKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Simulate attention output (batch=1, n_heads=2, pos=3, d_head=2)\n",
        "# attn_output = torch.tensor([[[[1.0, 1.1], [1.2, 1.3], [1.4, 1.5]],\n",
        "#                              [[2.0, 2.1], [2.2, 2.3], [2.4, 2.5]]]])  # [1, 2, 3, 2]\n",
        "# print(attn_output)\n",
        "\n",
        "# # Permute\n",
        "# attn_output = attn_output.permute(0, 2, 1, 3)  # [1, 3, 2, 2]\n",
        "# print(\"Permuted shape:\", attn_output.shape)\n",
        "\n",
        "# # Reshape\n",
        "# attn_output = attn_output.reshape(1, 3, -1)  # [1, 3, 4]\n",
        "# print(\"Final shape:\", attn_output.shape)\n",
        "# print(attn_output)\n"
      ],
      "metadata": {
        "id": "r8uVE4dKJF1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Dimensionality Collapse"
      ],
      "metadata": {
        "id": "8SP5GjoYKw_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attn_output = torch.tensor([[[[1.0, 1.1], [1.2, 1.3], [1.4, 1.5]],\n",
        "#                              [[2.0, 2.1], [2.2, 2.3], [2.4, 2.5]]]])  # [1, 2, 3, 2]\n"
      ],
      "metadata": {
        "id": "6eACFnzhKynV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attn_output = attn_output.permute(0, 2, 1, 3)  # [1, 3, 2, 2]\n"
      ],
      "metadata": {
        "id": "r747cYvALE2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attn_output = attn_output.reshape(1, 3, -1)  # [1, 3, 4]\n"
      ],
      "metadata": {
        "id": "ahgRw9A_Lyuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kE-1hBgiKI0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# # Dimensions\n",
        "# batch = 2\n",
        "# pos = 4\n",
        "# d_model = 8\n",
        "# n_heads = 2\n",
        "# d_head = 4\n",
        "\n",
        "# # Step 1: Linear Transformation\n",
        "# W_Q = torch.randn(n_heads, d_model, d_head)  # [n_heads, d_model, d_head]\n",
        "# x = torch.randn(batch, pos, d_model)  # [batch, pos, d_model]\n",
        "\n",
        "# # Apply linear transformation\n",
        "# Q = torch.einsum('bpd,hdq->bphq', x, W_Q)  # [batch, pos, n_heads, d_head]\n",
        "# K = Q.clone()  # [batch, pos, n_heads, d_head]\n",
        "# V = Q.clone()  # [batch, pos, n_heads, d_head]\n",
        "\n",
        "# # Step 2: Dot Product for Attention Scores\n",
        "# Q = Q.permute(0, 2, 1, 3)  # [batch, n_heads, pos, d_head]\n",
        "# K = K.permute(0, 2, 3, 1)  # [batch, n_heads, d_head, pos]\n",
        "\n",
        "# attn_scores = torch.einsum('bhqd,bhdk->bhqk', Q, K)  # [batch, n_heads, pos, pos]\n",
        "# print(\"Attention scores shape:\", attn_scores.shape)\n",
        "\n",
        "# # Step 3: Weighted Sum\n",
        "# attn_probs = torch.softmax(attn_scores, dim=-1)  # [batch, n_heads, pos, pos]\n",
        "# V = V.permute(0, 2, 1, 3)  # [batch, n_heads, pos, d_head]\n",
        "\n",
        "# # Weighted sum\n",
        "# attn_output = torch.einsum('bhqk,bhkd->bhqd', attn_probs, V)  # [batch, n_heads, pos, d_head]\n",
        "# print(\"Attention output shape:\", attn_output.shape)\n",
        "\n",
        "# # Step 4: Final Projection\n",
        "# attn_output = attn_output.permute(0, 2, 1, 3).reshape(batch, pos, -1)  # [batch, pos, d_model]\n",
        "# print(\"Final shape after combining heads:\", attn_output.shape)\n"
      ],
      "metadata": {
        "id": "fcwEzvYc8sDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Why Split d_model into n_heads and d_head ?\n",
        "\n",
        "# import torch\n",
        "# batch, pos, d_model, n_heads, d_head = 1, 3, 4, 2, 2  # Simplified for clarity\n",
        "# x = torch.tensor([[[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [9.0, 10.0, 11.0, 12.0]]])\n",
        "# print(\"Input shape:\", x.shape)\n",
        "# print(\"Input:\", x)\n",
        "\n",
        "# # Reshape to [batch, pos, n_heads, d_head]\n",
        "# x = x.reshape(batch, pos, n_heads, d_head)\n",
        "# print(\"Input shape After Reshape:\", x.shape)\n",
        "# print(\"After Reshape:\", x)\n",
        "\n",
        "# # Permute to [batch, n_heads, pos, d_head]\n",
        "# x = x.permute(0, 2, 1, 3)\n",
        "# print(\"After Permute:\", x)\n"
      ],
      "metadata": {
        "id": "zujAh1W0PMQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# # Dimensions\n",
        "# batch, pos, d_model, n_heads, d_head = 2, 4, 8, 2, 4  # d_model = n_heads * d_head\n",
        "\n",
        "# # Step 1: Linear Transformation for Q, K, V\n",
        "# W_Q = torch.randn(n_heads, d_model, d_head)  # [n_heads, d_model, d_head]\n",
        "# W_K = W_Q.clone()  # Using same weights for simplicity\n",
        "# W_V = W_Q.clone()\n",
        "\n",
        "# x = torch.randn(batch, pos, d_model)  # [batch, pos, d_model]\n",
        "\n",
        "# # Transform Q\n",
        "# Q = torch.einsum('bpd,hdq->bphq', x, W_Q)  # [batch, pos, n_heads, d_head]\n",
        "# print(\"Q shape after transformation:\", Q.shape)\n",
        "\n",
        "# # Transform K\n",
        "# K = torch.einsum('bpd,hdq->bphq', x, W_K)  # [batch, pos, n_heads, d_head]\n",
        "# print(\"K shape after transformation:\", K.shape)\n",
        "\n",
        "# # Transform V\n",
        "# V = torch.einsum('bpd,hdq->bphq', x, W_V)  # [batch, pos, n_heads, d_head]\n",
        "# print(\"V shape after transformation:\", V.shape)\n",
        "\n",
        "# # Step 2: Prepare Q and K for Attention Scores\n",
        "# # Permute Q to [batch, n_heads, pos, d_head]\n",
        "# Q = Q.permute(0, 2, 1, 3)\n",
        "# print(\"Q shape after permute:\", Q.shape)\n",
        "\n",
        "# # Permute K to [batch, n_heads, d_head, pos]\n",
        "# K = K.permute(0, 2, 3, 1)\n",
        "# print(\"K shape after permute:\", K.shape)\n",
        "\n",
        "# # Step 3: Compute Attention Scores\n",
        "# # Dot product of Q and K -> [batch, n_heads, pos, pos]\n",
        "# attn_scores = torch.einsum('bhqd,bhdk->bhqk', Q, K)\n",
        "# print(\"Attention scores shape:\", attn_scores.shape)\n",
        "\n",
        "# # Step 4: Apply Softmax to Attention Scores\n",
        "# attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "# print(\"Attention probabilities shape:\", attn_probs.shape)\n",
        "\n",
        "# # Step 5: Weighted Sum of Values\n",
        "# # Permute V to [batch, n_heads, pos, d_head]\n",
        "# V = V.permute(0, 2, 1, 3)\n",
        "# print(\"V shape after permute:\", V.shape)\n",
        "\n",
        "# # Compute weighted sum -> [batch, n_heads, pos, d_head]\n",
        "# attn_output = torch.einsum('bhqk,bhkd->bhqd', attn_probs, V)\n",
        "# print(\"Attention output shape:\", attn_output.shape)\n",
        "# print(\"Attention output [batch, pos, n_heads, d_head]\", attn_output)\n",
        "\n",
        "# # Step 6: Combine Heads\n",
        "# # Permute to [batch, pos, n_heads, d_head] and reshape to [batch, pos, d_model]\n",
        "# attn_output = attn_output.permute(0, 2, 1, 3).reshape(batch, pos, -1)\n",
        "# print(\"Final output shape:\", attn_output.shape)\n",
        "# print(\"Attention output reshaped to [batch, pos, d_model]\", attn_output)\n",
        "\n"
      ],
      "metadata": {
        "id": "HoSPZ1Yn_Lct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# # Initialize seq_len and create triangular matrix\n",
        "# seq_len = 4\n",
        "# batch = 2\n",
        "# n_heads = 3\n",
        "\n",
        "# # Step 1: Create lower triangular matrix\n",
        "# mask = torch.tril(torch.ones(seq_len, seq_len))\n",
        "# print(\"Step 1: Lower triangular matrix\")\n",
        "# print(mask.shape)\n",
        "# print(mask)\n",
        "\n",
        "# # Step 2: Add two dimensions with unsqueeze\n",
        "# mask = mask.unsqueeze(0).unsqueeze(0)  # Shape becomes [1, 1, seq_len, seq_len]\n",
        "# print(\"\\nStep 2: After unsqueeze\")\n",
        "# print(mask.shape)\n",
        "# print(mask)\n",
        "\n",
        "# # Step 3: Expand to [batch, n_heads, seq_len, seq_len]\n",
        "# mask = mask.expand(batch, n_heads, seq_len, seq_len)  # Expands to [batch, n_heads, seq_len, seq_len]\n",
        "# print(\"\\nStep 3: After expand\")\n",
        "# print(mask.shape)\n",
        "# print(mask)\n"
      ],
      "metadata": {
        "id": "YJhZgHNRv0Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# # Toy Example Setup\n",
        "# batch_size = 1       # Single example\n",
        "# seq_len = 2          # Sequence length (number of positions)\n",
        "# d_model = 3          # Model hidden size\n",
        "# n_heads = 1          # Single attention head\n",
        "# d_head = 2           # Size of each attention head\n",
        "\n",
        "# # Real Numbers for Inputs and Parameters\n",
        "# normalized_resid_pre = torch.tensor([[[1.0, 2.0, 3.0],  # Token 1\n",
        "#                                       [4.0, 5.0, 6.0]]],  # Token 2\n",
        "#                                     dtype=torch.float32)  # Shape: [batch, position, d_model]\n",
        "\n",
        "# W_Q = torch.tensor([[[0.1, 0.2],   # d_model=3, d_head=2\n",
        "#                       [0.3, 0.4],\n",
        "#                       [0.5, 0.6]]], dtype=torch.float32)  # Shape: [n_heads, d_model, d_head]\n",
        "\n",
        "# b_Q = torch.tensor([[0.01, 0.02]], dtype=torch.float32)  # Shape: [n_heads, d_head]\n",
        "\n",
        "# # Log inputs\n",
        "# print(\"Inputs:\")\n",
        "# print(\"normalized_resid_pre (shape):\", normalized_resid_pre.shape)\n",
        "# print(normalized_resid_pre)\n",
        "# print(\"W_Q (shape):\", W_Q.shape)\n",
        "# print(W_Q)\n",
        "# print(\"b_Q (shape):\", b_Q.shape)\n",
        "# print(b_Q)\n",
        "# print(\"\\n\")\n",
        "\n",
        "# # Manual Computation Without Einsum\n",
        "# # Step 1: Perform matrix multiplication\n",
        "# # Reshape W_Q to [d_model, d_head] for multiplication\n",
        "# W_Q_reshaped = W_Q[0]  # Extract weights for the single head: [d_model, d_head]\n",
        "# print(\"Step 1: Reshape W_Q for the single head\")\n",
        "# print(\"W_Q_reshaped (shape):\", W_Q_reshaped.shape)\n",
        "# print(W_Q_reshaped)\n",
        "# print(\"\\n\")\n",
        "\n",
        "# # Multiply normalized_resid_pre by W_Q_reshaped\n",
        "# Q = torch.matmul(normalized_resid_pre, W_Q_reshaped)  # [batch, position, d_head]\n",
        "# print(\"Step 2: Matrix multiplication\")\n",
        "# print(\"Q (shape):\", Q.shape)\n",
        "# print(Q)\n",
        "# print(\"\\n\")\n",
        "\n",
        "# # Add the bias term\n",
        "# Q_manual = Q + b_Q  # Broadcasting over [n_heads, d_head]\n",
        "# print(\"Step 3: Add bias\")\n",
        "# print(\"Q_manual (shape):\", Q_manual.shape)\n",
        "# print(Q_manual)\n",
        "# print(\"\\n\")\n",
        "\n",
        "# # Computation Using Einsum\n",
        "# Q_einsum = torch.einsum(\"bpd,hdm->bhpm\", normalized_resid_pre, W_Q) + b_Q\n",
        "# print(\"Computation Using Einsum\")\n",
        "# print(\"Q_einsum (shape):\", Q_einsum.shape)\n",
        "# print(Q_einsum)\n",
        "# print(\"\\n\")\n",
        "\n",
        "# # Compare Results\n",
        "# difference = torch.abs(Q_manual - Q_einsum.squeeze(1))  # Remove the n_heads dimension for comparison\n",
        "# print(\"Comparison of Manual and Einsum\")\n",
        "# print(\"Difference (shape):\", difference.shape)\n",
        "# print(difference)\n"
      ],
      "metadata": {
        "id": "AgHiMinQ5tt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# class Config:\n",
        "#     d_model: int = 768\n",
        "#     debug: bool = True\n",
        "#     layer_norm_eps: float = 1e-5\n",
        "#     d_vocab: int = 50257\n",
        "#     init_range: float = 0.02\n",
        "#     n_ctx: int = 1024\n",
        "#     d_head: int = 64\n",
        "#     d_mlp: int = 3072\n",
        "#     n_heads: int = 12\n",
        "#     n_layers: int = 12\n",
        "\n",
        "# cfg = Config()\n",
        "# print(cfg)"
      ],
      "metadata": {
        "id": "lu4AF9NgrDuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oBjF3MTx621"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk_d2UK-x621",
        "outputId": "72e323c5-0d59-4b1f-f2d9-17ef8067051f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "Output shape: torch.Size([2, 4, 768])\n",
            "\n",
            "Input shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "Output shape: torch.Size([1, 35, 768])\n",
            "Reference output shape: torch.Size([1, 35, 768])\n",
            "100.00% of the values are correct\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.4380,  0.3624,  0.5117,  ...,  1.7227,  1.5761,  0.0368],\n",
              "         [-1.0766, -0.0438,  0.3276,  ..., -0.5437,  0.4033,  0.3717],\n",
              "         [-1.2182, -1.5481, -0.9702,  ...,  1.0737,  0.7199,  0.5080],\n",
              "         ...,\n",
              "         [-0.4004,  0.8475,  0.2047,  ...,  0.3789,  0.0455, -0.4744],\n",
              "         [-0.0862,  0.7839,  0.9046,  ..., -0.2175, -0.5953,  0.8555],\n",
              "         [ 0.8448, -0.3743,  1.0397,  ...,  0.0296,  0.3405,  0.3585]]],\n",
              "       device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
        "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
        "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
        "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
        "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
        "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
        "\n",
        "    def forward(self, normalized_resid_mid):\n",
        "        print(\"cfg.d_model\", cfg.d_model)\n",
        "        print(\"cfg.d_mlp\", cfg.d_mlp)\n",
        "        print(\"normalized_resid_mid.shape\", normalized_resid_mid.shape)\n",
        "        print(\"self.W_in.shape\", self.W_in.shape)\n",
        "        print(\"self.b_in.shape\", self.b_in.shape)\n",
        "        print(\"self.W_out.shape\", self.W_out.shape)\n",
        "        print(\"self.b_out.shape\", self.b_out.shape)\n",
        "        # normalized_resid_mid: [batch, position, d_model]\n",
        "        # what I want here is normalized_resid_stream. matmul\n",
        "        output_in = torch.einsum(\"bpd, dm -> bpm\", normalized_resid_mid, self.W_in) + self.b_in\n",
        "        hidden_activation = gelu_new(output_in)\n",
        "        output_out = torch.einsum(\"bpm, md -> bpd\", hidden_activation, self.W_out) + self.b_out\n",
        "\n",
        "        print(\"output.shape\", output_in.shape)\n",
        "        print(\"hidden_activtaion.shape\", hidden_activation.shape)\n",
        "        print(\"output_out.shape\", output_out.shape)\n",
        "        return output_out\n",
        "        \"YOUR CODE HERE\"\n",
        "\n",
        "rand_float_test(MLP, [2, 4, 768])\n",
        "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"blocks.0.ln2.hook_normalized\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqc9kMJnx622"
      },
      "source": [
        "## Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmAuzb8tx622",
        "outputId": "b2072605-4238-4345-834b-44358744aa86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0324, -0.1436,  0.5158, -0.0915, -1.0564], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "Output shape: torch.Size([2, 4, 768])\n",
            "\n",
            "Input shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "Output shape: torch.Size([1, 35, 768])\n",
            "Reference output shape: torch.Size([1, 35, 768])\n",
            "93.40% of the values are correct\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.3911,  0.1543,  0.6005,  ...,  1.7199,  1.7365,  0.3930],\n",
              "         [-0.9041, -0.0358,  0.2350,  ..., -0.4148,  0.3562,  0.3938],\n",
              "         [-0.9647, -2.4819, -1.4997,  ...,  1.4047,  0.7615,  0.5920],\n",
              "         ...,\n",
              "         [-0.7422,  0.9252, -0.3217,  ...,  0.2921,  0.1096, -0.5344],\n",
              "         [-1.3219,  0.8960,  1.1798,  ..., -0.5545, -0.4072,  0.9254],\n",
              "         [ 1.1209, -0.8915,  1.3741,  ..., -0.1354,  0.3431,  0.4516]]],\n",
              "       device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.ln1 = LayerNorm(cfg)\n",
        "        self.attn = Attention(cfg)\n",
        "        self.ln2 = LayerNorm(cfg)\n",
        "        self.mlp = MLP(cfg)\n",
        "\n",
        "    def forward(self, resid_pre):\n",
        "        # resid_pre [batch, position, d_model]\n",
        "        print(f\"[DEBUG - TransformerBlock] resid_pre.shape: {resid_pre.shape}\")\n",
        "        normalized_resid_pre = self.ln1(resid_pre)\n",
        "        print(f\"[DEBUG - TransformerBlock] normalized_resid_pre.shape: {normalized_resid_pre.shape}\")\n",
        "        attn_out = self.attn(normalized_resid_pre)\n",
        "        print(f\"[DEBUG - TransformerBlock] attn_out.shape: {attn_out.shape}\")\n",
        "        resid_mid = resid_pre + attn_out\n",
        "        print(f\"[DEBUG - TransformerBlock] resid_mid.shape: {resid_mid.shape}\")\n",
        "\n",
        "        normalized_resid_mid = self.ln2(resid_mid)\n",
        "        print(f\"[DEBUG - TransformerBlock] normalized_resid_mid.shape: {normalized_resid_mid.shape}\")\n",
        "        mlp_out = self.mlp(normalized_resid_mid)\n",
        "        print(f\"[DEBUG - TransformerBlock] mlp_out.shape: {mlp_out.shape}\")\n",
        "\n",
        "        resid_post = resid_mid + mlp_out\n",
        "        print(f\"[DEBUG - TransformerBlock] resid_post.shape: {resid_post.shape}\")\n",
        "        print(f\"[DEBUG - TransformerBlock] resid_post (first 5 elements): {resid_post[0, 0, :5]}\")  # Add this line\n",
        "\n",
        "        return resid_post\n",
        "\n",
        "\n",
        "\n",
        "# # Reference values for debugging\n",
        "# reference_block = reference_gpt2.blocks[0]\n",
        "# reference_resid_pre = cache[\"resid_pre\", 0]\n",
        "\n",
        "# ref_normalized_resid_pre = reference_block.ln1(reference_resid_pre)\n",
        "# ref_attn_out = reference_block.attn(ref_normalized_resid_pre)\n",
        "# ref_resid_mid = reference_resid_pre + ref_attn_out\n",
        "# ref_normalized_resid_mid = reference_block.ln2(ref_resid_mid)\n",
        "# ref_mlp_out = reference_block.mlp(ref_normalized_resid_mid)\n",
        "# ref_resid_post = ref_resid_mid + ref_mlp_out\n",
        "\n",
        "# print(\"[DEBUG - Reference] Reference resid_post (first 5 elements):\", ref_resid_post[0, 0, :5])\n",
        "\n",
        "\n",
        "rand_float_test(TransformerBlock, [2, 4, 768])\n",
        "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4CKS5h7x622"
      },
      "source": [
        "## Unembedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6LLvUhpx622",
        "outputId": "c205c27a-21a3-40fb-a551-425458a3c7f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_final: torch.Size([2, 4, 768])\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "\n",
            "Input shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_final: torch.Size([1, 35, 768])\n",
            "Output shape: torch.Size([1, 35, 50257])\n",
            "Reference output shape: torch.Size([1, 35, 50257])\n",
            "100.00% of the values are correct\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ -43.4317,  -39.8364,  -43.0660,  ...,  -54.0878,  -54.3452,\n",
              "           -42.3645],\n",
              "         [-128.0392, -127.9936, -130.7011,  ..., -136.7121, -129.9261,\n",
              "          -129.3965],\n",
              "         [-119.8521, -121.0064, -123.8820,  ..., -128.5180, -126.6027,\n",
              "          -121.9060],\n",
              "         ...,\n",
              "         [-112.9815, -112.7749, -117.0633,  ..., -121.2914, -117.6574,\n",
              "          -114.5005],\n",
              "         [ -98.6725, -104.4888, -108.7361,  ..., -118.3552, -113.8766,\n",
              "          -106.3604],\n",
              "         [-126.8285, -128.9596, -128.3941,  ..., -140.1970, -138.5883,\n",
              "          -122.3697]]], device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "class Unembed(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
        "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
        "        self.b_U = nn.Parameter(torch.zeros((cfg.d_vocab), requires_grad=False))\n",
        "\n",
        "    def forward(self, normalized_resid_final):\n",
        "        # normalized_resid_final [batch, position, d_model]\n",
        "        if self.cfg.debug: print(\"Normalized_resid_final:\", normalized_resid_final.shape)\n",
        "        logits = einsum(\"batch position d_model, d_model d_vocab -> batch position d_vocab\", normalized_resid_final, self.W_U) + self.b_U\n",
        "        return logits\n",
        "\n",
        "rand_float_test(Unembed, [2, 4, 768])\n",
        "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osc-vVWTx622"
      },
      "source": [
        "## Full Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM5OP92zx622",
        "outputId": "d9cc046f-b3a8-417c-dc03-6b4ccc78ae32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_pos shape: torch.Size([1024, 768])\n",
            "Input shape: torch.Size([2, 4])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([2, 4])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([2, 4, 768])\n",
            "tokens shape: torch.Size([2, 4])\n",
            "tokens: tensor([[275, 909, 359, 176],\n",
            "        [720, 458, 933, 103]], device='cuda:0')\n",
            "positions shape: torch.Size([4])\n",
            "positions: tensor([0, 1, 2, 3], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-0.0032,  0.0108,  0.0139,  ...,  0.0052, -0.0050, -0.0062],\n",
            "        [ 0.0060,  0.0031,  0.0022,  ...,  0.0020, -0.0221, -0.0167],\n",
            "        [ 0.0005, -0.0242, -0.0138,  ..., -0.0076,  0.0168, -0.0177],\n",
            "        [-0.0201,  0.0337,  0.0039,  ..., -0.0137,  0.0047, -0.0139],\n",
            "        [-0.0139, -0.0241,  0.0326,  ..., -0.0137,  0.0150, -0.0025]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([4, 768])\n",
            "pos_embeddings (first few positions): tensor([[-0.0032,  0.0108,  0.0139,  ...,  0.0052, -0.0050, -0.0062],\n",
            "        [ 0.0060,  0.0031,  0.0022,  ...,  0.0020, -0.0221, -0.0167],\n",
            "        [ 0.0005, -0.0242, -0.0138,  ..., -0.0076,  0.0168, -0.0177]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([2, 4, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.4504, -0.0288,  0.2080,  0.8366, -0.1270], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.7084, -0.7296,  0.2532,  1.0053, -0.5628], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.6070, -0.7407,  0.1818,  0.5890, -1.1536], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.5295, -1.2767,  0.0644,  0.3966, -0.8448], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3017, -0.9384, -0.3938,  0.4976, -1.3275], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.7155, -1.1197,  0.0596, -0.0710, -2.0378], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-1.1488, -1.3298,  0.2279, -0.1234, -2.8088], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-1.4141, -2.0826, -0.3147, -0.2939, -3.0150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-1.7464, -1.8297, -1.0538,  0.1122, -2.5766], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-1.7198, -1.4179, -1.9223, -0.6839, -2.9791], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-2.1748, -2.6385, -2.1843, -0.7944, -2.5304], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([2, 4, 768])\n",
            "Normalized_resid_pre: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([2, 4, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([2, 4, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([2, 4, 3072])\n",
            "hidden_activtaion.shape torch.Size([2, 4, 3072])\n",
            "output_out.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([2, 4, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-1.4282, -3.0816, -1.6972, -0.2052, -1.4771], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([2, 4, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([2, 4, 768])\n",
            "Normalized_resid_final: torch.Size([2, 4, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([2, 4, 50257])\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "Input shape: torch.Size([1, 35])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 35])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 35, 768])\n",
            "tokens shape: torch.Size([1, 35])\n",
            "tokens: tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
            "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
            "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
            "          1011,   625,   262,   995,     0]], device='cuda:0')\n",
            "positions shape: torch.Size([35])\n",
            "positions: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34],\n",
            "       device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([35, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 35, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4088,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4659,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3464,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 35, 768])\n",
            "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 35, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 35, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 35, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 35, 3072])\n",
            "output_out.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 35, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 35, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 35, 768])\n",
            "Normalized_resid_final: torch.Size([1, 35, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 35, 50257])\n",
            "Output shape: torch.Size([1, 35, 50257])\n",
            "Reference output shape: torch.Size([1, 35, 50257])\n",
            "100.00% of the values are correct\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ -43.4315,  -39.8362,  -43.0657,  ...,  -54.0875,  -54.3449,\n",
              "           -42.3642],\n",
              "         [-128.0371, -127.9915, -130.6989,  ..., -136.7097, -129.9238,\n",
              "          -129.3942],\n",
              "         [-119.8520, -121.0062, -123.8818,  ..., -128.5176, -126.6023,\n",
              "          -121.9059],\n",
              "         ...,\n",
              "         [-112.9796, -112.7730, -117.0612,  ..., -121.2890, -117.6551,\n",
              "          -114.4986],\n",
              "         [ -98.6739, -104.4902, -108.7371,  ..., -118.3559, -113.8774,\n",
              "          -106.3616],\n",
              "         [-126.8269, -128.9581, -128.3925,  ..., -140.1947, -138.5858,\n",
              "          -122.3685]]], device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "class DemoTransformer(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.embed = Embed(cfg)\n",
        "        self.pos_embed = PosEmbed(cfg)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
        "        self.ln_final = LayerNorm(cfg)\n",
        "        self.unembed = Unembed(cfg)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # tokens [batch, position]\n",
        "        print(\"[DEBUG - DemoTransformer] tokens.shape:\", tokens.shape)  # [batch, position]\n",
        "\n",
        "        embed = self.embed(tokens)\n",
        "        print(\"[DEBUG - DemoTransformer] embed.shape:\", embed.shape)  # [batch, position, d_model]\n",
        "\n",
        "        pos_embed = self.pos_embed(tokens)\n",
        "        print(\"[DEBUG - DemoTransformer] pos_embed.shape:\", pos_embed.shape)  # [batch, position, d_model]\n",
        "\n",
        "        residual = embed + pos_embed\n",
        "        print(\"[DEBUG - DemoTransformer] x.shape after embedding:\", residual.shape)  # [batch, position, d_model]\n",
        "\n",
        "        for block in self.blocks:\n",
        "            residual = block(residual)\n",
        "            print(\"residual.shape\", residual.shape)  # [batch, position, d_model]\n",
        "\n",
        "        normalized_residual_final = self.ln_final(residual)\n",
        "        print(\"[DEBUG - DemoTransformer] normalized_residual_final:\", normalized_residual_final.shape)  # [batch, position, d_model]\n",
        "\n",
        "        logits = self.unembed(normalized_residual_final)\n",
        "        print(\"[DEBUG - DemoTransformer] logits.shape:\", logits.shape)  # [batch, position, d_vocab]\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "rand_int_test(DemoTransformer, [2, 4])\n",
        "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzIof1GOx623"
      },
      "source": [
        "# Try it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCXSJ3zsx623",
        "outputId": "f5368976-3ae8-4a3b-aeae-ae73b0e528ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_pos shape: torch.Size([1024, 768])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DemoTransformer(\n",
              "  (embed): Embed()\n",
              "  (pos_embed): PosEmbed()\n",
              "  (blocks): ModuleList(\n",
              "    (0-11): 12 x TransformerBlock(\n",
              "      (ln1): LayerNorm()\n",
              "      (attn): Attention()\n",
              "      (ln2): LayerNorm()\n",
              "      (mlp): MLP()\n",
              "    )\n",
              "  )\n",
              "  (ln_final): LayerNorm()\n",
              "  (unembed): Unembed()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "demo_gpt2 = DemoTransformer(Config(debug=False))\n",
        "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
        "demo_gpt2.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYth5iVSx623"
      },
      "source": [
        "Take a test string - the intro paragraph of today's featured Wikipedia article. Let's calculate the loss!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hg7MFbEx623"
      },
      "outputs": [],
      "source": [
        "test_string = \"\"\"Mini scule is a species of microhylid frog endemic to Madagascar that was described in 2019. The scientific name of the species refers to its size, being a pun on the word minuscule. It is very small, measuring only 8.4 to 10.8 mm (0.33 to 0.43 in) in snout–vent length. It has bronze underparts with a brown groin and back of the thigh, cream upperparts with brown flecking, a dark brown side of the head, and a red iris. On the hind feet, the first toe is absent and the second and fifth toes are strongly reduced. The frog is known only from the Sainte Luce Reserve, where it inhabits areas with deep leaf litter near semi-permanent water bodies. Specimens of frogs from Mandena, the Vohimena mountains, the southern Anosy Mountains, and Tsitongambarika may also be of this species. Along with Mini mum and Mini ature, the other two species in its genus, it received media attention when first described due to the wordplay in its scientific name. (Full article...)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQdoAMYEx623",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d4e8dd-09cf-4009-bb65-16903c82a843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 237])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 237, 768])\n",
            "tokens shape: torch.Size([1, 237])\n",
            "tokens: tensor([[50256, 39234,   629,  2261,   318,   257,  4693,   286,  4580,    71,\n",
            "          2645,   312, 21264, 42560,   284, 46694,   326,   373,  3417,   287,\n",
            "         13130,    13,   383,  5654,  1438,   286,   262,  4693, 10229,   284,\n",
            "           663,  2546,    11,   852,   257,  4000,   319,   262,  1573,   949,\n",
            "         16241,  2261,    13,   632,   318,   845,  1402,    11, 15964,   691,\n",
            "           807,    13,    19,   284,   838,    13,    23,  8085,   357,    15,\n",
            "            13,  2091,   284,   657,    13,  3559,   287,     8,   287,  3013,\n",
            "           448,  1906,  1151,  4129,    13,   632,   468, 22101,   739, 42632,\n",
            "           351,   257,  7586, 42247,   290,   736,   286,   262, 19341,    11,\n",
            "          8566,  6727, 42632,   351,  7586,  5104, 44377,    11,   257,  3223,\n",
            "          7586,  1735,   286,   262,  1182,    11,   290,   257,  2266,  4173,\n",
            "           271,    13,  1550,   262, 16222,  3625,    11,   262,   717, 21189,\n",
            "           318, 13717,   290,   262,  1218,   290,  8150, 23932,   389,  7634,\n",
            "          5322,    13,   383, 21264,   318,  1900,   691,   422,   262,   311,\n",
            "           391,   660,  6026,   344, 12224,    11,   810,   340, 11381,   896,\n",
            "          3006,   351,  2769, 12835, 25359,  1474, 10663,    12,   525, 44172,\n",
            "          1660,  5920,    13, 18291, 12117,   286, 37475,   422, 13314,  8107,\n",
            "            11,   262,   569,  1219,   320,  8107, 12269,    11,   262,  8372,\n",
            "          1052,   418,    88, 21124,    11,   290, 13146,   270,   506,  4131,\n",
            "           283,  9232,   743,   635,   307,   286,   428,  4693,    13, 17159,\n",
            "           351, 12558, 25682,   290, 12558,   379,   495,    11,   262,   584,\n",
            "           734,  4693,   287,   663, 34306,    11,   340,  2722,  2056,  3241,\n",
            "           618,   717,  3417,  2233,   284,   262,  1573,  1759,   287,   663,\n",
            "          5654,  1438,    13,   357, 13295,  2708, 23029]], device='cuda:0')\n",
            "positions shape: torch.Size([237])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236],\n",
            "       device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([237, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 237, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 237, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 237, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 237, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 237, 3072])\n",
            "output_out.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 237, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 237, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 237, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 237, 3072])\n",
            "output_out.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 237, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 237, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 237, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 237, 3072])\n",
            "output_out.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4089,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 237, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 237, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 237, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 237, 3072])\n",
            "output_out.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 237, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 237, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 237, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 237, 3072])\n",
            "output_out.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 237, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 237, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 237, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 237, 3072])\n",
            "output_out.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 237, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 237, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 237, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 237, 3072])\n",
            "output_out.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 237, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 237, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 237, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 237, 3072])\n",
            "output_out.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 237, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 237, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 237, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 237, 3072])\n",
            "output_out.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 237, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 237, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 237, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 237, 3072])\n",
            "output_out.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 237, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 237, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 237, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 237, 3072])\n",
            "output_out.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3465,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 237, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 237, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 237, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 237, 3072])\n",
            "output_out.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 237, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 237, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 237, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 237, 50257])\n"
          ]
        }
      ],
      "source": [
        "test_tokens = reference_gpt2.to_tokens(test_string).cuda()\n",
        "demo_logits = demo_gpt2(test_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgOsWfu3x623",
        "outputId": "a985fd76-6563-484c-86a4-69475d3cef2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.7187, device='cuda:0', grad_fn=<NegBackward0>)\n",
            "Loss as average prob tensor(0.0243, device='cuda:0', grad_fn=<ExpBackward0>)\n",
            "Loss as 'uniform over this many variables' tensor(41.2099, device='cuda:0', grad_fn=<ExpBackward0>)\n",
            "Uniform loss over the vocab 10.82490511970208\n"
          ]
        }
      ],
      "source": [
        "def lm_cross_entropy_loss(logits, tokens):\n",
        "    # Measure next token loss\n",
        "    # Logits have shape [batch, position, d_vocab]\n",
        "    # Tokens have shape [batch, position]\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    pred_log_probs = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
        "    return -pred_log_probs.mean()\n",
        "loss = lm_cross_entropy_loss(demo_logits, test_tokens)\n",
        "print(loss)\n",
        "print(\"Loss as average prob\", (-loss).exp())\n",
        "print(\"Loss as 'uniform over this many variables'\", (loss).exp())\n",
        "print(\"Uniform loss over the vocab\", math.log(demo_gpt2.cfg.d_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rzOtV6kx624"
      },
      "source": [
        "We can also greedily generate text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7fcb9775cdfb48a180c69449280f60a2",
            "a3987fc0774141fc87b3802ea88f91eb",
            "84dce524a4aa4c4883d1929a998046ef",
            "1b0f5d13471748b38254708274f47220",
            "04b8730d9e5d404a90eb874ab2e95f16",
            "30873368ed8a4152acd9bd4cebaaf7ef",
            "8a2373c0ee4943cbae94acdd0acc14a8",
            "9aa0783ece664599902d0cbe685f85bf",
            "7e4b48e528e04de499d360d2b56f6a32",
            "1164538d2def4a389778bfd68785af79",
            "6f81b426dca34afdb15127a0c14527d9"
          ]
        },
        "id": "0henqE37x624",
        "outputId": "617cb092-f9f7-4756-beda-768257983756"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fcb9775cdfb48a180c69449280f60a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 185, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 185, 3072])\n",
            "output_out.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 185, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 185, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 185, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 185, 3072])\n",
            "output_out.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 185, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 185, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 185, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 185, 3072])\n",
            "output_out.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 185, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 185, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 185, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 185, 3072])\n",
            "output_out.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 185, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 185, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 185, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 185, 3072])\n",
            "output_out.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 185, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 185, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 185, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 185, 3072])\n",
            "output_out.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 185, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 185, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 185, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 185, 3072])\n",
            "output_out.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3464,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 185, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 185, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 185, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 185, 3072])\n",
            "output_out.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 185, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 185, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 185, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 185, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 186])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 186, 768])\n",
            "tokens shape: torch.Size([1, 186])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262]], device='cuda:0')\n",
            "positions shape: torch.Size([186])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([186, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 186, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 186, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 186, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 186, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 186, 3072])\n",
            "output_out.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 186, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 186, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 186, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 186, 3072])\n",
            "output_out.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 186, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 186, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 186, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 186, 3072])\n",
            "output_out.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4088,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 186, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 186, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 186, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 186, 3072])\n",
            "output_out.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 186, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 186, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 186, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 186, 3072])\n",
            "output_out.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 186, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 186, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 186, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 186, 3072])\n",
            "output_out.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 186, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 186, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 186, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 186, 3072])\n",
            "output_out.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 186, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 186, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 186, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 186, 3072])\n",
            "output_out.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 186, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 186, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 186, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 186, 3072])\n",
            "output_out.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 186, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 186, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 186, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 186, 3072])\n",
            "output_out.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 186, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 186, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 186, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 186, 3072])\n",
            "output_out.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3464,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 186, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 186, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 186, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 186, 3072])\n",
            "output_out.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 186, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 186, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 186, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 186, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 187])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 187, 768])\n",
            "tokens shape: torch.Size([1, 187])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473]], device='cuda:0')\n",
            "positions shape: torch.Size([187])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([187, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 187, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 187, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 187, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 187, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 187, 3072])\n",
            "output_out.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 187, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 187, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 187, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 187, 3072])\n",
            "output_out.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 187, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 187, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 187, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 187, 3072])\n",
            "output_out.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4088,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 187, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 187, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 187, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 187, 3072])\n",
            "output_out.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 187, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 187, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 187, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 187, 3072])\n",
            "output_out.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 187, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 187, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 187, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 187, 3072])\n",
            "output_out.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 187, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 187, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 187, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 187, 3072])\n",
            "output_out.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 187, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 187, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 187, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 187, 3072])\n",
            "output_out.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 187, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 187, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 187, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 187, 3072])\n",
            "output_out.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 187, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 187, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 187, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 187, 3072])\n",
            "output_out.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 187, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 187, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 187, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 187, 3072])\n",
            "output_out.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3464,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 187, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 187, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 187, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 187, 3072])\n",
            "output_out.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 187, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 187, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 187, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 187, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 188])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 188, 768])\n",
            "tokens shape: torch.Size([1, 188])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319]],\n",
            "       device='cuda:0')\n",
            "positions shape: torch.Size([188])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([188, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 188, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 188, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 188, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 188, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 188, 3072])\n",
            "output_out.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 188, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 188, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 188, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 188, 3072])\n",
            "output_out.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 188, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 188, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 188, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 188, 3072])\n",
            "output_out.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4088,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 188, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 188, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 188, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 188, 3072])\n",
            "output_out.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 188, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 188, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 188, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 188, 3072])\n",
            "output_out.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 188, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 188, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 188, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 188, 3072])\n",
            "output_out.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 188, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 188, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 188, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 188, 3072])\n",
            "output_out.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 188, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 188, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 188, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 188, 3072])\n",
            "output_out.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 188, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 188, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 188, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 188, 3072])\n",
            "output_out.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 188, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 188, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 188, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 188, 3072])\n",
            "output_out.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 188, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 188, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 188, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 188, 3072])\n",
            "output_out.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3464,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 188, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 188, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 188, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 188, 3072])\n",
            "output_out.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 188, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 188, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 188, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 188, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 189])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 189, 768])\n",
            "tokens shape: torch.Size([1, 189])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319,  3321]],\n",
            "       device='cuda:0')\n",
            "positions shape: torch.Size([189])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([189, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 189, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 189, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 189, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 189, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 189, 3072])\n",
            "output_out.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 189, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 189, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 189, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 189, 3072])\n",
            "output_out.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 189, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 189, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 189, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 189, 3072])\n",
            "output_out.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4088,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 189, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 189, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 189, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 189, 3072])\n",
            "output_out.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 189, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 189, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 189, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 189, 3072])\n",
            "output_out.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 189, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 189, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 189, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 189, 3072])\n",
            "output_out.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 189, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 189, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 189, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 189, 3072])\n",
            "output_out.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 189, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 189, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 189, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 189, 3072])\n",
            "output_out.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 189, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 189, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 189, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 189, 3072])\n",
            "output_out.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 189, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 189, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 189, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 189, 3072])\n",
            "output_out.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 189, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 189, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 189, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 189, 3072])\n",
            "output_out.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3464,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 189, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 189, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 189, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 189, 3072])\n",
            "output_out.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 189, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 189, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 189, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 189, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 190])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 190, 768])\n",
            "tokens shape: torch.Size([1, 190])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319,  3321,    13]],\n",
            "       device='cuda:0')\n",
            "positions shape: torch.Size([190])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([190, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 190, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 190, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 190, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 190, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 190, 3072])\n",
            "output_out.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 190, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 190, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 190, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 190, 3072])\n",
            "output_out.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 190, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 190, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 190, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 190, 3072])\n",
            "output_out.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4088,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 190, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 190, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 190, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 190, 3072])\n",
            "output_out.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 190, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 190, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 190, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 190, 3072])\n",
            "output_out.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 190, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 190, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 190, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 190, 3072])\n",
            "output_out.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 190, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 190, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 190, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 190, 3072])\n",
            "output_out.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 190, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 190, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 190, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 190, 3072])\n",
            "output_out.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 190, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 190, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 190, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 190, 3072])\n",
            "output_out.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 190, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 190, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 190, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 190, 3072])\n",
            "output_out.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 190, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 190, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 190, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 190, 3072])\n",
            "output_out.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3464,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 190, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 190, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 190, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 190, 3072])\n",
            "output_out.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 190, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 190, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 190, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 190, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 191])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 191, 768])\n",
            "tokens shape: torch.Size([1, 191])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319,  3321,    13,\n",
            "           628]], device='cuda:0')\n",
            "positions shape: torch.Size([191])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([191, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 191, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 191, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 191, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 191, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 191, 3072])\n",
            "output_out.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 191, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 191, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 191, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 191, 3072])\n",
            "output_out.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 191, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 191, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 191, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 191, 3072])\n",
            "output_out.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4088,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 191, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 191, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 191, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 191, 3072])\n",
            "output_out.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 191, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 191, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 191, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 191, 3072])\n",
            "output_out.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 191, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 191, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 191, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 191, 3072])\n",
            "output_out.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 191, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 191, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 191, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 191, 3072])\n",
            "output_out.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 191, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 191, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 191, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 191, 3072])\n",
            "output_out.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 191, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 191, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 191, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 191, 3072])\n",
            "output_out.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 191, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 191, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 191, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 191, 3072])\n",
            "output_out.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 191, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 191, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 191, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 191, 3072])\n",
            "output_out.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3464,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 191, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 191, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 191, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 191, 3072])\n",
            "output_out.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 191, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 191, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 191, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 191, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 192])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 192, 768])\n",
            "tokens shape: torch.Size([1, 192])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319,  3321,    13,\n",
            "           628,   198]], device='cuda:0')\n",
            "positions shape: torch.Size([192])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([192, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 192, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 192, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 192, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 192, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 192, 3072])\n",
            "output_out.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 192, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 192, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 192, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 192, 3072])\n",
            "output_out.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 192, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 192, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 192, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 192, 3072])\n",
            "output_out.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4088,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 192, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 192, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 192, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 192, 3072])\n",
            "output_out.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 192, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 192, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 192, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 192, 3072])\n",
            "output_out.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 192, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 192, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 192, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 192, 3072])\n",
            "output_out.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 192, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 192, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 192, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 192, 3072])\n",
            "output_out.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 192, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 192, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 192, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 192, 3072])\n",
            "output_out.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 192, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 192, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 192, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 192, 3072])\n",
            "output_out.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 192, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 192, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 192, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 192, 3072])\n",
            "output_out.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 192, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 192, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 192, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 192, 3072])\n",
            "output_out.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3464,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 192, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 192, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 192, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 192, 3072])\n",
            "output_out.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 192, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 192, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 192, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 192, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 193])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 193, 768])\n",
            "tokens shape: torch.Size([1, 193])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319,  3321,    13,\n",
            "           628,   198,   464]], device='cuda:0')\n",
            "positions shape: torch.Size([193])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192],\n",
            "       device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([193, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 193, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 193, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 193, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 193, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 193, 3072])\n",
            "output_out.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 193, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 193, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 193, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 193, 3072])\n",
            "output_out.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 193, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 193, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 193, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 193, 3072])\n",
            "output_out.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4089,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 193, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 193, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 193, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 193, 3072])\n",
            "output_out.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 193, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 193, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 193, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 193, 3072])\n",
            "output_out.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 193, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 193, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 193, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 193, 3072])\n",
            "output_out.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 193, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 193, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 193, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 193, 3072])\n",
            "output_out.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 193, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 193, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 193, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 193, 3072])\n",
            "output_out.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 193, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 193, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 193, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 193, 3072])\n",
            "output_out.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 193, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 193, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 193, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 193, 3072])\n",
            "output_out.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 193, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 193, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 193, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 193, 3072])\n",
            "output_out.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3465,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 193, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 193, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 193, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 193, 3072])\n",
            "output_out.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 193, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 193, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 193, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 193, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 194])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 194, 768])\n",
            "tokens shape: torch.Size([1, 194])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319,  3321,    13,\n",
            "           628,   198,   464,  2097]], device='cuda:0')\n",
            "positions shape: torch.Size([194])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193],\n",
            "       device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([194, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 194, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 194, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 194, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 194, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 194, 3072])\n",
            "output_out.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 194, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 194, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 194, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 194, 3072])\n",
            "output_out.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 194, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 194, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 194, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 194, 3072])\n",
            "output_out.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4089,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 194, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 194, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 194, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 194, 3072])\n",
            "output_out.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 194, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 194, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 194, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 194, 3072])\n",
            "output_out.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 194, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 194, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 194, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 194, 3072])\n",
            "output_out.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 194, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 194, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 194, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 194, 3072])\n",
            "output_out.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 194, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 194, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 194, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 194, 3072])\n",
            "output_out.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 194, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 194, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 194, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 194, 3072])\n",
            "output_out.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 194, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 194, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 194, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 194, 3072])\n",
            "output_out.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 194, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 194, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 194, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 194, 3072])\n",
            "output_out.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3465,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 194, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 194, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 194, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 194, 3072])\n",
            "output_out.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 194, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 194, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 194, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 194, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 195])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 195, 768])\n",
            "tokens shape: torch.Size([1, 195])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319,  3321,    13,\n",
            "           628,   198,   464,  2097,   286]], device='cuda:0')\n",
            "positions shape: torch.Size([195])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194],\n",
            "       device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([195, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 195, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 195, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 195, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 195, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 195, 3072])\n",
            "output_out.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 195, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 195, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 195, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 195, 3072])\n",
            "output_out.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 195, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 195, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 195, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 195, 3072])\n",
            "output_out.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4089,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 195, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 195, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 195, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 195, 3072])\n",
            "output_out.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 195, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 195, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 195, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 195, 3072])\n",
            "output_out.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 195, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 195, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 195, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 195, 3072])\n",
            "output_out.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 195, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 195, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 195, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 195, 3072])\n",
            "output_out.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 195, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 195, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 195, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 195, 3072])\n",
            "output_out.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 195, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 195, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 195, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 195, 3072])\n",
            "output_out.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 195, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 195, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 195, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 195, 3072])\n",
            "output_out.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 195, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 195, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 195, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 195, 3072])\n",
            "output_out.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3465,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 195, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 195, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 195, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 195, 3072])\n",
            "output_out.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 195, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 195, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 195, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 195, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 196])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 196, 768])\n",
            "tokens shape: torch.Size([1, 196])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319,  3321,    13,\n",
            "           628,   198,   464,  2097,   286, 17132]], device='cuda:0')\n",
            "positions shape: torch.Size([196])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195],\n",
            "       device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([196, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 196, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 196, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 196, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 196, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 196, 3072])\n",
            "output_out.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 196, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 196, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 196, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 196, 3072])\n",
            "output_out.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 196, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 196, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 196, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 196, 3072])\n",
            "output_out.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4089,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 196, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 196, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 196, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 196, 3072])\n",
            "output_out.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 196, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 196, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 196, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 196, 3072])\n",
            "output_out.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 196, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 196, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 196, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 196, 3072])\n",
            "output_out.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 196, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 196, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 196, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 196, 3072])\n",
            "output_out.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 196, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 196, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 196, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 196, 3072])\n",
            "output_out.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 196, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 196, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 196, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 196, 3072])\n",
            "output_out.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 196, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 196, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 196, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 196, 3072])\n",
            "output_out.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 196, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 196, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 196, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 196, 3072])\n",
            "output_out.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3465,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 196, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 196, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 196, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 196, 3072])\n",
            "output_out.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 196, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 196, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 196, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 196, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 197])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 197, 768])\n",
            "tokens shape: torch.Size([1, 197])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319,  3321,    13,\n",
            "           628,   198,   464,  2097,   286, 17132,   318]], device='cuda:0')\n",
            "positions shape: torch.Size([197])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([197, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 197, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 197, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 197, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 197, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 197, 3072])\n",
            "output_out.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 197, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 197, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 197, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 197, 3072])\n",
            "output_out.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 197, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 197, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 197, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 197, 3072])\n",
            "output_out.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4089,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 197, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 197, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 197, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 197, 3072])\n",
            "output_out.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 197, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 197, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 197, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 197, 3072])\n",
            "output_out.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 197, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 197, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 197, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 197, 3072])\n",
            "output_out.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 197, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 197, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 197, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 197, 3072])\n",
            "output_out.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 197, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 197, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 197, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 197, 3072])\n",
            "output_out.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 197, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 197, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 197, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 197, 3072])\n",
            "output_out.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 197, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 197, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 197, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 197, 3072])\n",
            "output_out.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 197, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 197, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 197, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 197, 3072])\n",
            "output_out.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3465,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 197, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 197, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 197, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 197, 3072])\n",
            "output_out.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 197, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 197, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 197, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 197, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 198])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 198, 768])\n",
            "tokens shape: torch.Size([1, 198])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319,  3321,    13,\n",
            "           628,   198,   464,  2097,   286, 17132,   318,  2938]],\n",
            "       device='cuda:0')\n",
            "positions shape: torch.Size([198])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([198, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 198, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 198, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 198, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 198, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 198, 3072])\n",
            "output_out.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 198, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 198, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 198, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 198, 3072])\n",
            "output_out.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 198, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 198, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 198, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 198, 3072])\n",
            "output_out.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4089,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 198, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 198, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 198, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 198, 3072])\n",
            "output_out.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 198, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 198, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 198, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 198, 3072])\n",
            "output_out.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 198, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 198, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 198, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 198, 3072])\n",
            "output_out.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 198, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 198, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 198, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 198, 3072])\n",
            "output_out.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 198, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 198, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 198, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 198, 3072])\n",
            "output_out.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 198, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 198, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 198, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 198, 3072])\n",
            "output_out.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 198, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 198, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 198, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 198, 3072])\n",
            "output_out.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 198, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 198, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 198, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 198, 3072])\n",
            "output_out.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3465,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 198, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 198, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 198, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 198, 3072])\n",
            "output_out.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 198, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 198, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 198, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 198, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 199])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 199, 768])\n",
            "tokens shape: torch.Size([1, 199])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319,  3321,    13,\n",
            "           628,   198,   464,  2097,   286, 17132,   318,  2938,   284]],\n",
            "       device='cuda:0')\n",
            "positions shape: torch.Size([199])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([199, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 199, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 199, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 199, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 199, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 199, 3072])\n",
            "output_out.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 199, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 199, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 199, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 199, 3072])\n",
            "output_out.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 199, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 199, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 199, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 199, 3072])\n",
            "output_out.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4089,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 199, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 199, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 199, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 199, 3072])\n",
            "output_out.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 199, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 199, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 199, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 199, 3072])\n",
            "output_out.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 199, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 199, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 199, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 199, 3072])\n",
            "output_out.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 199, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 199, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 199, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 199, 3072])\n",
            "output_out.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 199, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 199, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 199, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 199, 3072])\n",
            "output_out.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 199, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 199, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 199, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 199, 3072])\n",
            "output_out.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 199, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 199, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 199, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 199, 3072])\n",
            "output_out.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 199, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 199, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 199, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 199, 3072])\n",
            "output_out.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3465,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 199, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 199, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 199, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 199, 3072])\n",
            "output_out.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 199, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 199, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 199, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 199, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 200])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 200, 768])\n",
            "tokens shape: torch.Size([1, 200])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319,  3321,    13,\n",
            "           628,   198,   464,  2097,   286, 17132,   318,  2938,   284,  3015]],\n",
            "       device='cuda:0')\n",
            "positions shape: torch.Size([200])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([200, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 200, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 200, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 200, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 200, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 200, 3072])\n",
            "output_out.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 200, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 200, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 200, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 200, 3072])\n",
            "output_out.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 200, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 200, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 200, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 200, 3072])\n",
            "output_out.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4089,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 200, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 200, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 200, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 200, 3072])\n",
            "output_out.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 200, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 200, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 200, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 200, 3072])\n",
            "output_out.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 200, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 200, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 200, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 200, 3072])\n",
            "output_out.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 200, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 200, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 200, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 200, 3072])\n",
            "output_out.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 200, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 200, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 200, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 200, 3072])\n",
            "output_out.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 200, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 200, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 200, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 200, 3072])\n",
            "output_out.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 200, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 200, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 200, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 200, 3072])\n",
            "output_out.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 200, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 200, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 200, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 200, 3072])\n",
            "output_out.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3465,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 200, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 200, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 200, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 200, 3072])\n",
            "output_out.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 200, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 200, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 200, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 200, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([1, 201])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([1, 201, 768])\n",
            "tokens shape: torch.Size([1, 201])\n",
            "tokens: tensor([[50256, 29449,  3000,    25,  1992,  1301,   468,   587, 18516,  2317,\n",
            "           416,   262,  2097,   286, 17132,   329,  5076,   286,  1176,   290,\n",
            "         28118,   286,  3162,    13,   383,  3015,   373, 18395,   284, 29903,\n",
            "            11,   351,   838,  4734,  9679,   477,  4956,   287,  6709,   284,\n",
            "         18516,   620,    13,   383,  1893,   318,   783,   691,   262,  2368,\n",
            "           287,  1605,  2106,   284,   307, 18516,  2317,    11,   290,   262,\n",
            "           717,   284,   307, 18516,  2317,  5403,    13,   383,  2097,   481,\n",
            "           783,  3758,   262,  6685,   286, 30258,   284,   262,  3845,    11,\n",
            "           810,   257,  4473,   481,   307,  2714,   284,  5004,  1771,   284,\n",
            "          4781,   262,  1893,   422,  2607,    13,   383,  3845,   318,  2938,\n",
            "           284,  2221,   262,  4473,   319,  3321,    13,   628,   198,   464,\n",
            "          2097,   286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,\n",
            "           286,  1992,  1301,   319,  3431,    13,   628,   198,   464,  2097,\n",
            "           286, 17132,   318,  2938,   284,  3015,   319,   262, 30258,   286,\n",
            "          1992,  1301,   319,  3431,    13,   628,   198,   464,  3845,   318,\n",
            "          2938,   284,  2221,   262,  4473,   319,  3321,    13,   628,   198,\n",
            "           464,  2097,   286, 17132,   318,  2938,   284,  3015,   319,   262,\n",
            "         30258,   286,  1992,  1301,   319,  3431,    13,   628,   198,   464,\n",
            "          3845,   318,  2938,   284,  2221,   262,  4473,   319,  3321,    13,\n",
            "           628,   198,   464,  2097,   286, 17132,   318,  2938,   284,  3015,\n",
            "           319]], device='cuda:0')\n",
            "positions shape: torch.Size([201])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200], device='cuda:0')\n",
            "W_pos shape: torch.Size([1024, 768])\n",
            "W_pos (first few rows): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02],\n",
            "        [-2.8337e-04, -7.3803e-02,  1.0553e-01,  ...,  1.0157e-02,\n",
            "          1.7659e-02, -7.0854e-03],\n",
            "        [ 7.6374e-03, -2.5090e-02,  1.2696e-01,  ...,  8.4643e-03,\n",
            "          9.8542e-03, -7.0117e-03]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([201, 768])\n",
            "pos_embeddings (first few positions): tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "          2.8267e-02,  5.4490e-02],\n",
            "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "          1.0172e-02, -1.5573e-04],\n",
            "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "          1.9325e-02, -2.1424e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([1, 201, 768])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 201, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 201, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 201, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 201, 3072])\n",
            "output_out.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.3911, 0.1543, 0.6005, 0.5763, 0.2506], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 201, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 201, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 201, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 201, 3072])\n",
            "output_out.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([0.1763, 0.0512, 0.6984, 0.4973, 0.6422], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 201, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 201, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 201, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 201, 3072])\n",
            "output_out.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.0047, -0.2178,  0.4008,  0.4089,  0.7072], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 201, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 201, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 201, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 201, 3072])\n",
            "output_out.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0612, -0.2281,  0.4474,  0.3652,  0.9020], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 201, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 201, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 201, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 201, 3072])\n",
            "output_out.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1148, -0.2847,  0.3346,  0.3129,  0.9300], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 201, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 201, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 201, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 201, 3072])\n",
            "output_out.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1497, -0.3525,  0.4344,  0.2510,  1.1150], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 201, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 201, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 201, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 201, 3072])\n",
            "output_out.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2023, -0.3277,  0.4658,  0.1988,  1.2153], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 201, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 201, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 201, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 201, 3072])\n",
            "output_out.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2341, -0.2093,  0.3412,  0.1045,  1.3102], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 201, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 201, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 201, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 201, 3072])\n",
            "output_out.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.1842, -0.1084,  0.3064,  0.0314,  1.2508], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 201, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 201, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 201, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 201, 3072])\n",
            "output_out.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2680, -0.0414,  0.0577,  0.0301,  1.3124], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 201, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 201, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 201, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 201, 3072])\n",
            "output_out.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.3465,  0.1060, -0.2842,  0.0374,  1.2450], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([1, 201, 768])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([1, 201, 768])\n",
            "self.W_in.shape torch.Size([768, 3072])\n",
            "self.b_in.shape torch.Size([3072])\n",
            "self.W_out.shape torch.Size([3072, 768])\n",
            "self.b_out.shape torch.Size([768])\n",
            "output.shape torch.Size([1, 201, 3072])\n",
            "hidden_activtaion.shape torch.Size([1, 201, 3072])\n",
            "output_out.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([1, 201, 768])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([-0.2768,  1.0193, -1.4706,  2.0693,  0.7169], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([1, 201, 768])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([1, 201, 768])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([1, 201, 50257])\n",
            "Breaking News: President Trump has been impeached by the House of Representatives for abuse of power and obstruction of Congress. The vote was 230 to 197, with 10 Republicans joining all Democrats in voting to impeach. The president is now only the third in American history to be impeached, and the first to be impeached twice. The House will now send the articles of impeachment to the Senate, where a trial will be held to determine whether to remove the president from office. The Senate is expected to begin the trial on Monday.\n",
            "\n",
            "\n",
            "The House of Representatives is expected to vote on the impeachment of President Trump on Tuesday.\n",
            "\n",
            "\n",
            "The House of Representatives is expected to vote on the impeachment of President Trump on Tuesday.\n",
            "\n",
            "\n",
            "The Senate is expected to begin the trial on Monday.\n",
            "\n",
            "\n",
            "The House of Representatives is expected to vote on the impeachment of President Trump on Tuesday.\n",
            "\n",
            "\n",
            "The Senate is expected to begin the trial on Monday.\n",
            "\n",
            "\n",
            "The House of Representatives is expected to vote on the\n"
          ]
        }
      ],
      "source": [
        "test_string = \"Breaking News: President Trump has been impeached by the House of Representatives for abuse of power and obstruction of Congress. The vote was 230 to 197, with 10 Republicans joining all Democrats in voting to impeach. The president is now only the third in American history to be impeached, and the first to be impeached twice. The House will now send the articles of impeachment to the Senate, where a trial will be held to determine whether to remove the president from office. The Senate is expected to begin the trial on\"\n",
        "for i in tqdm.tqdm(range(100)):\n",
        "    test_tokens = reference_gpt2.to_tokens(test_string).cuda()\n",
        "    demo_logits = demo_gpt2(test_tokens)\n",
        "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
        "print(test_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY8VwaPWx624"
      },
      "source": [
        "# Training a Model!\n",
        "\n",
        "This is a lightweight demonstration of how you can actually train your own GPT-2 with this code! Here we train a tiny model on a tiny dataset, but it's fundamentally the same code for training a larger/more real model (though you'll need beefier GPUs and data parallelism to do it remotely efficiently, and fancier parallelism for much bigger ones).\n",
        "\n",
        "For our purposes, we'll train 2L 4 heads per layer model, with context length 256, for 1000 steps of batch size 8, just to show what it looks like (and so the notebook doesn't melt your colab lol)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDYsa7lnx624",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80709710-48d0-4f23-caa2-0db82260c0a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    %pip install datasets\n",
        "    %pip install transformers\n",
        "import datasets\n",
        "import transformers\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og6jrAcXx624"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1SxKYt7x624"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "num_epochs = 1\n",
        "max_steps = 1000\n",
        "log_every = 10\n",
        "lr = 1e-3\n",
        "weight_decay = 1e-2\n",
        "model_cfg = Config(debug=False, d_model=256, n_heads=4, d_head=64, d_mlp=1024, n_layers=2, n_ctx=256, d_vocab=reference_gpt2.cfg.d_vocab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T8rewpax624"
      },
      "source": [
        "\n",
        "## Create Data\n",
        "\n",
        "We load in a tiny dataset I made, with the first 10K entries in the Pile (inspired by Stas' version for OpenWebText!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc6YYRVix624",
        "outputId": "2e47a078-c105-4b9d-9b29-776b0844d08c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386,
          "referenced_widgets": [
            "57ce05c380aa4fc586299e839032533c",
            "bb9356d803314a418dafc742a7df01dc",
            "3b5c7a80c9844431ad9b729e07856403",
            "9488f664974b4f93b5df25c53d3e14ff",
            "bfcd9f574d4e43c0afd20f945decedc1",
            "11cf68a2605d44ceba0d9a2751545497",
            "dc6ff916e7a740b68adbc04cc4631b43",
            "690a692fe7f6402c91a92de51ebfe963",
            "4cb087f2d82a431d807922c8556ed736",
            "c4f059fd0c894935ab3a2bc3f4b7f4dd",
            "8e26ed2b56174cb88aff2acbed31f169",
            "44d5ca7a36704342b8c2e35c4e5600f0",
            "164ab06aeeaf43cbbf629ff4c56378f7",
            "df7e9ff1e0bb49f889628da9571a2700",
            "297b0ec4b424475a8659ee68855df4d8",
            "6847db13a51e449e93ccadbd52700f4c",
            "9170a15997a640808c3d4abc129046a4",
            "e7de58f368ca4cc896086af800c910c9",
            "9c7e8d3dc7de475bb6d96013961db218",
            "c2d5074eb8074b3db607a32d40226c1a",
            "8689cb223b014289a59147617c9be304",
            "a0291a8674d1413e85a092e1035eeb73",
            "002d84fa3d7a41fc9d75da0b091c2cd7",
            "b42ea6d105ce4ae485c493b482da9eb4",
            "90750f51430a4def9bf55bbe3e708736",
            "c00846c5d527401795a798ebbfe861d2",
            "e496233771874cf1a3f569c2989730cb",
            "bbcff679ce4347369eb39dab18311fff",
            "85e5b876d3a04be3af67a4917d38b15f",
            "2e581698676d4a588e968c7f1afaac48",
            "fcc28140bcbb4527a29a9946ecb34cdb",
            "cc30edbb40af404cb8a833286269547c",
            "89c13c5167b947c5b7566cf719f1d70e",
            "af6b054347414a528773bbadb9121c26",
            "64057d667aba4ea989052bc4f4b43370",
            "305cfdbb940f4009a3a6a20923337ff4",
            "7c45a65c99b946a0b91bc49c656645b3",
            "a7257c08058b4600982cccfc28767eab",
            "40af14c86324459790df9b98ab9068b9",
            "0ba725b7ccfc470c824597474e3df80d",
            "bab323dea9764d39b83c754bde294dff",
            "f1f8863445f94f2eaf4306a1c82b7a7f",
            "70d351a824e04c588b5e421ba74961ae",
            "f971ff26807e417c9bde60367cec1d94",
            "2b3c103cb75d4d61b948ad4f963793b0",
            "36c5279d4aec4b11bb96913d085b67fe",
            "da82ef11204e46ddbf8e7d7e19fc2305",
            "aec0200fb9d04db69efb954ad463fa60",
            "5187df2b344542f1a7e6b2b0a45abdf8",
            "a06a6852e663415e9600365545b099b3",
            "fbf61fc29e394071a751219162f38eae",
            "d3abe1d3e67a4a1ba96e2a8f9386fdca",
            "b30f4f7b311349e993c28a32f2fb5415",
            "150df001338044d3bb115e556ba563eb",
            "2a82e85d106f49239c0724db3218a609"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/373 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57ce05c380aa4fc586299e839032533c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dataset_infos.json:   0%|          | 0.00/921 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44d5ca7a36704342b8c2e35c4e5600f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)-00000-of-00001-4746b8785c874cc7.parquet:   0%|          | 0.00/33.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "002d84fa3d7a41fc9d75da0b091c2cd7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af6b054347414a528773bbadb9121c26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'meta'],\n",
            "    num_rows: 10000\n",
            "})\n",
            "It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web. Playi\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b3c103cb75d4d61b948ad4f963793b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (80023 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (101051 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (155995 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (229134 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
        "print(dataset)\n",
        "print(dataset[0]['text'][:100])\n",
        "tokens_dataset = tokenize_and_concatenate(dataset, reference_gpt2.tokenizer, streaming=False, max_length=model_cfg.n_ctx, column_name=\"text\", add_bos_token=True, num_proc=4)\n",
        "data_loader = torch.utils.data.DataLoader(tokens_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgC9J59Kx624"
      },
      "source": [
        "## Create Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPv0mvxPx624",
        "outputId": "dc42d096-853c-43a2-fbb7-094a6b22ddca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_pos shape: torch.Size([256, 256])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DemoTransformer(\n",
              "  (embed): Embed()\n",
              "  (pos_embed): PosEmbed()\n",
              "  (blocks): ModuleList(\n",
              "    (0-1): 2 x TransformerBlock(\n",
              "      (ln1): LayerNorm()\n",
              "      (attn): Attention()\n",
              "      (ln2): LayerNorm()\n",
              "      (mlp): MLP()\n",
              "    )\n",
              "  )\n",
              "  (ln_final): LayerNorm()\n",
              "  (unembed): Unembed()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "model = DemoTransformer(model_cfg)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdhqAP2Rx625"
      },
      "source": [
        "## Create Optimizer\n",
        "We use AdamW - it's a pretty standard optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AgvM4MGx625"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58FLDMmLx625"
      },
      "source": [
        "## Run Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "acb4ceaedd7e40e290f0356894acb36f",
            "24a3f41ee07443ca989433257874c1d3",
            "c6d0c4a8c66c450cb7fa5dad05220e5f",
            "ec721abd2fa8475fa4d1dd5fde477d31",
            "9ae9059384d348cfbb79799e7ccb8fcb",
            "d72388df4a9845cf856b2557e5e9675e",
            "61f226e603db462eaad4eecdc81d393b",
            "e4521623748b4a21b43357e9b2884131",
            "2950972ce54a486baa01dfb2302fa5b3",
            "d8452bfb34604c0b8d2c69ac9216c46d",
            "b6622497e5c342418d86a8cdbace0f8e"
          ]
        },
        "id": "WAQSwxthx625",
        "outputId": "444e6e84-ff29-479c-b342-87c97e5b38ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches: 8506\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acb4ceaedd7e40e290f0356894acb36f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0120,  0.0722, -0.0182,  ..., -0.0029,  0.0358,  0.0341],\n",
            "        [-0.0200,  0.0172, -0.0114,  ..., -0.0480, -0.0126,  0.0178],\n",
            "        [-0.0148,  0.0459, -0.0371,  ...,  0.0173,  0.0151,  0.1084],\n",
            "        [-0.0102,  0.0577, -0.0588,  ..., -0.0170, -0.0328,  0.0272],\n",
            "        [ 0.0588, -0.0226, -0.0322,  ..., -0.0014,  0.0213,  0.0351]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1959e-02,  7.2168e-02, -1.8248e-02, -3.3852e-02, -9.8496e-03,\n",
            "         -5.2616e-03, -1.1343e-02,  3.6680e-03, -6.9077e-02, -1.3609e-02,\n",
            "          5.3550e-03,  1.5637e-02, -3.2391e-02, -2.7969e-02, -4.2734e-02,\n",
            "         -3.2186e-02, -2.9987e-03,  2.1513e-02,  1.4423e-02,  2.2351e-02,\n",
            "         -2.7239e-02,  4.4074e-02, -1.1589e-02, -3.3363e-02, -1.2960e-02,\n",
            "          2.7360e-02, -3.0757e-02,  3.4232e-02,  7.4625e-03, -1.0288e-02,\n",
            "         -7.3793e-02,  2.3163e-02,  1.4379e-02,  1.6257e-02, -3.2679e-02,\n",
            "         -8.6258e-03,  6.2563e-02,  3.1089e-02,  3.2444e-02,  1.5713e-03,\n",
            "         -4.2983e-02, -2.4562e-03,  3.0011e-02, -1.3549e-02,  1.0258e-02,\n",
            "          9.4311e-03,  2.7771e-02, -7.5834e-02, -3.7924e-02, -6.9733e-02,\n",
            "         -2.2922e-02,  1.4826e-02, -2.1761e-02,  4.6816e-02,  2.7453e-02,\n",
            "          2.7672e-03, -2.4666e-02, -1.6615e-02,  4.3781e-02,  1.9713e-02,\n",
            "         -3.3417e-03, -8.4712e-05, -4.9488e-03, -1.7445e-02, -3.6002e-02,\n",
            "          2.1273e-02,  5.0179e-02, -1.8082e-02,  2.3302e-02,  4.2526e-02,\n",
            "          2.4392e-02, -1.8052e-02, -1.3375e-02,  2.9333e-02, -4.5891e-02,\n",
            "         -3.6241e-02, -1.7681e-02,  7.0921e-02,  6.2707e-02,  2.4639e-02,\n",
            "         -1.2912e-02, -2.6554e-02,  3.4277e-02, -3.4201e-02, -9.3284e-03,\n",
            "          9.9218e-02,  6.2518e-03,  3.3194e-02, -1.4946e-02,  3.4296e-03,\n",
            "          2.2637e-03, -5.5971e-02,  2.2145e-02, -9.4818e-04, -2.1642e-02,\n",
            "         -1.3722e-02, -7.3298e-02, -4.5355e-02,  1.5539e-02, -1.4655e-02,\n",
            "         -4.5489e-02,  4.8946e-03,  1.0823e-02,  1.7079e-02, -1.3603e-02,\n",
            "         -8.4753e-03,  5.2353e-02, -2.5236e-02, -4.9985e-02,  4.7926e-02,\n",
            "         -3.5037e-02, -1.6239e-02, -5.2739e-03, -6.3359e-02, -8.4387e-03,\n",
            "         -1.0175e-02, -4.5645e-03,  3.6103e-02,  4.1530e-02,  6.8039e-02,\n",
            "          1.7394e-02, -6.5215e-03, -3.5248e-02,  1.1104e-01,  3.7738e-02,\n",
            "         -7.4248e-03, -3.0978e-02, -8.2958e-03,  2.3787e-02, -8.0794e-03,\n",
            "          6.3456e-02,  9.9736e-03, -4.0655e-03,  2.2458e-02, -3.8968e-02,\n",
            "          2.3285e-04, -4.7133e-02,  4.1211e-02, -1.0262e-02,  7.6120e-03,\n",
            "         -1.7112e-02, -1.3714e-02,  3.8116e-02, -4.4706e-02, -2.5316e-02,\n",
            "          3.7808e-02, -2.3145e-02, -3.1525e-02,  3.9148e-02,  5.0892e-04,\n",
            "         -1.2068e-02,  5.9924e-02,  3.7714e-03,  7.2369e-03,  8.1242e-03,\n",
            "         -1.8806e-02,  1.1489e-02, -1.0193e-02, -6.1768e-03,  2.7424e-02,\n",
            "         -7.2560e-03, -3.1076e-02, -2.6024e-02, -5.7251e-02, -1.1881e-02,\n",
            "          5.8636e-02, -9.5776e-03, -4.9896e-02, -2.1303e-02, -3.7512e-02,\n",
            "         -2.1300e-03, -9.0485e-03, -5.0802e-03, -2.4678e-02, -3.4460e-02,\n",
            "         -2.2531e-02,  4.6462e-03, -1.2654e-02, -1.5076e-02, -8.6955e-02,\n",
            "          3.0021e-02,  4.4388e-02, -1.2888e-02, -6.7502e-03, -1.7988e-02,\n",
            "         -3.8401e-03,  8.9218e-03,  3.8405e-02,  4.8886e-02,  4.0167e-02,\n",
            "          7.8551e-03, -4.2394e-02, -6.5309e-03,  3.0156e-02, -3.8233e-04,\n",
            "         -2.5452e-02, -1.5260e-01, -2.4444e-02, -5.1424e-02, -3.5686e-02,\n",
            "          7.8553e-03, -1.2122e-02, -2.6476e-02,  4.7473e-02,  4.6368e-02,\n",
            "          3.3484e-02,  8.7363e-03,  2.1778e-02,  2.5406e-02, -3.1580e-02,\n",
            "         -1.3911e-02, -3.3367e-02, -1.0025e-02,  6.2769e-03, -3.8804e-02,\n",
            "         -4.2375e-03, -1.6376e-02,  1.8181e-02,  2.2090e-02,  3.6296e-02,\n",
            "          1.2087e-01,  4.5560e-02,  3.5494e-02,  1.3844e-02,  3.0808e-02,\n",
            "         -9.9392e-02,  7.9435e-03, -2.9655e-03, -1.0098e-02, -8.0303e-03,\n",
            "          1.2590e-02, -2.9643e-03, -1.0543e-02, -3.9314e-02,  4.9720e-02,\n",
            "          2.4894e-02, -2.0511e-02,  1.7675e-02,  1.8190e-02, -2.8037e-02,\n",
            "          1.5030e-02,  1.4524e-02,  2.3802e-04, -4.5114e-03,  8.9906e-03,\n",
            "         -1.2260e-01,  7.4496e-03,  7.3949e-03,  4.6444e-02, -2.4910e-02,\n",
            "          1.6363e-02, -4.9841e-03,  1.8465e-02, -2.8629e-03,  3.5785e-02,\n",
            "          3.4145e-02],\n",
            "        [-2.0015e-02,  1.7177e-02, -1.1422e-02,  3.3823e-02,  1.1741e-01,\n",
            "         -4.3188e-02, -2.1965e-02,  4.1782e-02,  1.8324e-02, -9.8519e-03,\n",
            "          4.8789e-03, -3.8582e-03, -6.1573e-03,  2.4935e-02,  1.0532e-01,\n",
            "          4.4139e-02,  2.9790e-03, -1.6241e-02, -1.0661e-02, -3.8593e-02,\n",
            "          9.9652e-02,  1.7794e-02, -6.5380e-03, -8.7889e-02, -4.3349e-02,\n",
            "         -4.7232e-02,  8.8880e-04, -6.1176e-02,  2.0963e-02, -8.3917e-03,\n",
            "          8.1457e-02, -3.1509e-02, -2.1444e-02,  8.3060e-03,  3.0542e-02,\n",
            "         -6.2358e-02,  1.0986e-03, -5.5006e-02, -4.1330e-03,  2.0781e-02,\n",
            "          2.4270e-02, -1.1058e-02, -3.1109e-02,  5.6513e-03, -3.0159e-02,\n",
            "         -1.3012e-02, -2.7220e-04,  8.3030e-02,  2.0667e-02,  1.1212e-03,\n",
            "          8.0347e-02, -9.9538e-03, -4.4658e-02, -3.9311e-02,  8.0737e-02,\n",
            "          8.8388e-02, -2.3172e-03, -9.4836e-02, -6.4923e-03,  3.1561e-02,\n",
            "          7.2525e-02, -2.0603e-02, -1.3975e-02,  9.1293e-02,  1.9017e-03,\n",
            "          5.3347e-02,  2.8127e-02, -5.7377e-02, -1.9074e-02, -1.1225e-02,\n",
            "          4.8980e-02,  2.8447e-02,  3.5234e-02,  9.0310e-02,  6.1721e-02,\n",
            "          2.8961e-02, -1.2514e-01,  8.9399e-02,  1.4161e-02, -4.5562e-02,\n",
            "         -4.6029e-02, -3.7366e-02, -6.8916e-02,  7.9900e-02, -9.8769e-04,\n",
            "         -3.7852e-02,  7.4646e-02, -3.3123e-02,  1.2300e-02,  2.1572e-02,\n",
            "         -2.2324e-02,  1.8329e-02, -3.9286e-02, -1.1951e-02, -1.6252e-02,\n",
            "         -5.5267e-02, -6.4724e-02, -2.8307e-02, -6.2495e-02, -2.4803e-02,\n",
            "         -5.0546e-02, -1.9468e-02,  1.7404e-02, -1.5857e-02,  6.1746e-03,\n",
            "         -3.1331e-03, -3.7432e-02, -2.1846e-02,  3.8275e-02, -9.4537e-05,\n",
            "          7.1257e-03,  8.3879e-03, -8.8632e-02,  3.7598e-02,  7.6535e-02,\n",
            "         -3.5067e-02, -7.8756e-02, -3.5103e-02, -5.6965e-02, -6.0831e-02,\n",
            "         -1.4688e-02, -1.5089e-02, -1.9366e-02, -4.0478e-02,  1.0413e-02,\n",
            "          1.2873e-02,  2.8977e-02,  2.7989e-02, -5.2766e-02,  3.8841e-03,\n",
            "          2.6737e-02,  2.7871e-02,  2.9571e-02, -2.4254e-02,  3.7783e-03,\n",
            "          4.2141e-03, -3.5446e-02,  2.0935e-02,  5.8943e-02,  4.4832e-02,\n",
            "         -2.4601e-02, -7.6633e-02,  8.9427e-02, -4.9248e-02, -1.1954e-02,\n",
            "         -1.2539e-02,  3.1543e-02, -2.4895e-02,  1.2173e-02,  2.9956e-03,\n",
            "         -1.1450e-03,  5.3038e-02,  2.4212e-02,  3.0249e-03, -3.2107e-02,\n",
            "          3.7070e-02,  1.6368e-02, -3.5915e-02,  7.7156e-02, -6.8882e-02,\n",
            "         -4.8598e-02, -1.9992e-02, -1.9915e-02, -6.8572e-02, -3.5874e-02,\n",
            "          3.9452e-02, -6.7737e-02, -8.1595e-03, -5.5584e-02, -1.4153e-02,\n",
            "          2.8875e-02, -3.7642e-02, -1.9049e-02, -2.7725e-02, -1.0847e-02,\n",
            "         -1.2120e-02,  1.0125e-02,  6.7836e-02,  1.8498e-02, -1.7654e-02,\n",
            "         -4.5180e-02, -1.7523e-02, -8.2353e-03, -5.9134e-02, -8.2406e-03,\n",
            "         -9.7548e-03, -6.5312e-03,  1.8497e-02, -3.8260e-02,  3.2133e-02,\n",
            "         -5.5799e-03, -2.3923e-02,  6.1521e-02, -1.0818e-01,  7.5439e-03,\n",
            "          7.5118e-03,  7.1339e-02, -1.5725e-02,  7.6277e-03, -4.1188e-02,\n",
            "          4.8948e-02, -1.8093e-02, -3.6023e-02,  2.7607e-02, -9.2606e-02,\n",
            "         -3.6086e-02, -1.0955e-02, -4.9353e-02, -1.3286e-02, -3.9458e-02,\n",
            "         -6.5384e-02,  6.3162e-02, -2.0339e-02, -6.2542e-02, -8.3241e-02,\n",
            "          3.3031e-02,  9.0339e-03,  3.7594e-02,  3.4849e-02, -2.2588e-03,\n",
            "          3.0058e-02, -1.7586e-03,  1.4001e-02,  8.3710e-02, -3.2433e-02,\n",
            "          3.7284e-02,  3.6202e-02, -3.6823e-04,  4.8713e-02, -4.3701e-03,\n",
            "         -2.5606e-02, -3.1480e-03,  7.6532e-02,  1.7223e-03,  6.4929e-02,\n",
            "         -7.8502e-02,  2.4820e-02, -2.4021e-02, -2.2034e-02,  4.8009e-02,\n",
            "         -8.0753e-02, -2.7988e-02,  6.2312e-02, -2.6458e-03, -1.0584e-02,\n",
            "         -2.0337e-02, -2.3564e-02,  3.0777e-02, -2.1601e-03, -1.1045e-02,\n",
            "         -1.5642e-02,  5.8140e-02, -1.2294e-02, -4.8029e-02, -1.2576e-02,\n",
            "          1.7758e-02],\n",
            "        [-1.4756e-02,  4.5928e-02, -3.7140e-02, -3.9524e-03, -2.7385e-04,\n",
            "         -5.7345e-02, -2.3728e-02, -4.2278e-02, -7.9668e-04,  4.3979e-02,\n",
            "         -6.8451e-03,  2.3407e-03,  1.7200e-02, -3.4379e-02, -2.3963e-02,\n",
            "         -2.0158e-02, -1.6150e-02,  1.3536e-02, -1.6761e-02, -2.0732e-02,\n",
            "          3.0717e-03, -3.2796e-03, -8.8488e-03, -3.3682e-02, -4.0930e-02,\n",
            "          4.9705e-02,  1.8244e-02, -5.2820e-02, -4.9861e-02,  4.9708e-04,\n",
            "         -6.6904e-02,  3.3395e-02, -2.1700e-02, -1.8123e-02, -1.1581e-03,\n",
            "         -4.0467e-02, -2.2967e-03,  5.5933e-02,  1.9131e-02, -4.5904e-02,\n",
            "         -6.6385e-02, -1.9134e-02, -4.5687e-02,  3.5528e-02, -2.3184e-02,\n",
            "         -4.3740e-02, -3.1890e-02,  1.2583e-02,  4.7969e-02, -2.5138e-02,\n",
            "         -1.1366e-02, -3.9282e-02, -4.0238e-02,  5.4910e-02,  8.7742e-03,\n",
            "          2.4348e-02, -5.4024e-02,  7.0409e-02,  2.3083e-02,  1.8863e-02,\n",
            "          2.3234e-02,  1.9181e-02, -2.3994e-02, -3.1713e-02, -1.1946e-02,\n",
            "          3.3956e-02,  1.1928e-02, -3.0821e-03, -6.1252e-03,  6.2224e-02,\n",
            "          1.3761e-02,  5.1617e-03,  1.4496e-02, -1.7629e-02, -5.7610e-02,\n",
            "         -5.7957e-02, -4.8932e-03,  2.0694e-02, -3.3703e-03,  1.7610e-02,\n",
            "         -5.4355e-02,  2.7610e-02,  1.5094e-03, -1.0621e-02,  7.6664e-03,\n",
            "         -1.0485e-02,  3.4303e-02,  1.6449e-02,  2.1875e-02, -7.0391e-03,\n",
            "         -9.8579e-03, -2.5843e-02,  2.2471e-02, -4.1677e-02, -5.4272e-02,\n",
            "          3.5588e-02,  3.1620e-02, -1.2983e-02, -1.5800e-02,  2.1759e-02,\n",
            "         -4.2718e-02, -4.4137e-02, -1.5661e-02, -4.9404e-02, -1.5766e-02,\n",
            "         -3.8626e-02,  2.6514e-02, -3.8822e-02, -2.3550e-03,  2.7145e-03,\n",
            "         -1.8564e-02,  2.5485e-02, -1.0410e-02,  3.7956e-03, -2.7443e-03,\n",
            "         -3.3785e-02,  1.8261e-02,  9.8546e-02, -4.0328e-03, -3.7355e-02,\n",
            "          2.1838e-02,  1.6512e-02,  7.0836e-02,  1.4335e-02, -3.1754e-02,\n",
            "         -2.0125e-02,  6.6409e-03,  4.3805e-02,  1.2797e-03, -7.7822e-03,\n",
            "         -7.3543e-02, -3.8874e-03,  8.5357e-03, -3.9375e-02, -1.7581e-03,\n",
            "          2.0638e-02,  1.9387e-02,  2.3260e-02,  3.2850e-02,  2.9236e-02,\n",
            "          2.7825e-02, -3.1735e-02,  1.5811e-02,  2.3846e-02,  2.0194e-02,\n",
            "         -9.4660e-03,  3.2861e-02,  4.7874e-02, -2.8648e-02,  1.7515e-02,\n",
            "         -2.1438e-03, -1.8524e-02,  5.0637e-02,  2.7624e-02,  2.3568e-02,\n",
            "          1.3417e-02, -3.5311e-03, -3.3407e-02,  6.4261e-03, -8.0702e-02,\n",
            "          4.6537e-02,  1.1048e-02,  1.8849e-02, -4.7784e-02, -7.4035e-03,\n",
            "          5.4981e-02, -4.0353e-02, -1.8897e-02, -6.1320e-02, -3.8581e-02,\n",
            "          3.3721e-02, -2.4862e-02,  3.3082e-02, -3.8755e-03,  2.8614e-02,\n",
            "         -4.1179e-02,  1.0301e-01,  3.2291e-02,  3.0774e-02, -8.2057e-02,\n",
            "         -2.5212e-02,  8.2874e-03, -4.4552e-02,  2.6938e-02,  2.9211e-02,\n",
            "         -2.3791e-02, -1.0823e-02,  1.0045e-02,  6.3989e-02,  1.1324e-02,\n",
            "          1.2253e-02, -3.9248e-02,  2.5494e-02, -2.4982e-02,  1.0536e-02,\n",
            "         -4.9270e-02,  4.4175e-02,  1.6949e-03,  8.0100e-02,  9.7229e-02,\n",
            "         -5.6939e-02, -1.8012e-02, -5.8335e-02, -6.3097e-02, -6.9745e-02,\n",
            "         -1.3191e-02,  3.6121e-02, -2.3653e-02,  4.1935e-02,  8.2821e-03,\n",
            "          2.3714e-02,  7.2019e-03,  6.6172e-02,  1.1137e-02, -2.8426e-02,\n",
            "         -2.4698e-02, -1.7816e-02,  3.4974e-02,  6.4591e-02,  2.0533e-02,\n",
            "          3.6701e-02,  2.2958e-02,  4.3859e-02,  2.9429e-02, -1.2580e-02,\n",
            "         -5.2338e-02, -3.1592e-02, -2.4425e-02, -3.4330e-02,  3.0783e-02,\n",
            "          2.3802e-02,  1.9229e-02,  2.9936e-02,  8.3385e-03,  7.7329e-03,\n",
            "         -4.3302e-02, -1.8456e-02,  2.2919e-02, -3.1145e-02,  3.0245e-02,\n",
            "          5.4295e-02, -3.5373e-03,  5.2304e-02, -8.2308e-03, -1.7427e-03,\n",
            "         -1.2781e-02, -4.4252e-02,  8.2693e-02,  7.6244e-02, -8.2948e-03,\n",
            "         -2.1071e-02,  3.3150e-02,  8.1905e-03,  1.7268e-02,  1.5067e-02,\n",
            "          1.0838e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.1090,  0.3880,  4.9948, -3.5393, -4.1882], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4765, -0.0828,  4.1118, -3.3089, -4.4415], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256, 26224,    90,  ...,    62,    74,  1976],\n",
            "        [50256,   307, 10944,  ...,  5224,   694,   338],\n",
            "        [50256,   284,  6070,  ..., 13937,   291, 10145],\n",
            "        ...,\n",
            "        [50256,  1610,  3723,  ...,  2753,   691,  1178],\n",
            "        [50256,   318,    11,  ...,  5149,   502,   339],\n",
            "        [50256, 13141,   546,  ...,   351, 12183, 21748]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0118,  0.0718, -0.0183,  ..., -0.0028,  0.0358,  0.0341],\n",
            "        [-0.0199,  0.0171, -0.0113,  ..., -0.0482, -0.0127,  0.0178],\n",
            "        [-0.0147,  0.0464, -0.0368,  ...,  0.0173,  0.0149,  0.1086],\n",
            "        [-0.0105,  0.0581, -0.0588,  ..., -0.0170, -0.0326,  0.0272],\n",
            "        [ 0.0589, -0.0227, -0.0321,  ..., -0.0016,  0.0214,  0.0351]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1848e-02,  7.1834e-02, -1.8261e-02, -3.3868e-02, -9.8254e-03,\n",
            "         -5.1632e-03, -1.1284e-02,  3.6450e-03, -6.9161e-02, -1.3683e-02,\n",
            "          5.3719e-03,  1.5666e-02, -3.2344e-02, -2.7945e-02, -4.3083e-02,\n",
            "         -3.2224e-02, -3.0764e-03,  2.1323e-02,  1.4685e-02,  2.2187e-02,\n",
            "         -2.7428e-02,  4.4224e-02, -1.1687e-02, -3.3414e-02, -1.2966e-02,\n",
            "          2.7367e-02, -3.0792e-02,  3.4387e-02,  7.2930e-03, -1.0118e-02,\n",
            "         -7.3949e-02,  2.3336e-02,  1.4433e-02,  1.6120e-02, -3.2744e-02,\n",
            "         -8.7093e-03,  6.2485e-02,  3.1246e-02,  3.2490e-02,  1.4408e-03,\n",
            "         -4.2784e-02, -2.4893e-03,  2.9938e-02, -1.3577e-02,  1.0495e-02,\n",
            "          9.5791e-03,  2.8023e-02, -7.5890e-02, -3.7956e-02, -6.9644e-02,\n",
            "         -2.2960e-02,  1.4862e-02, -2.1815e-02,  4.6818e-02,  2.7353e-02,\n",
            "          2.8554e-03, -2.4615e-02, -1.6689e-02,  4.3762e-02,  1.9863e-02,\n",
            "         -3.5031e-03, -3.2968e-05, -5.0244e-03, -1.7545e-02, -3.5842e-02,\n",
            "          2.1224e-02,  5.0225e-02, -1.8033e-02,  2.3269e-02,  4.2466e-02,\n",
            "          2.4212e-02, -1.8058e-02, -1.3267e-02,  2.9343e-02, -4.5811e-02,\n",
            "         -3.6154e-02, -1.7753e-02,  7.1052e-02,  6.2665e-02,  2.4623e-02,\n",
            "         -1.2848e-02, -2.6783e-02,  3.4276e-02, -3.4428e-02, -9.5759e-03,\n",
            "          9.9198e-02,  6.3302e-03,  3.3152e-02, -1.5000e-02,  3.2796e-03,\n",
            "          2.5148e-03, -5.5889e-02,  2.2196e-02, -7.4328e-04, -2.1516e-02,\n",
            "         -1.3695e-02, -7.3176e-02, -4.5417e-02,  1.5458e-02, -1.4566e-02,\n",
            "         -4.5368e-02,  4.8279e-03,  1.0838e-02,  1.7172e-02, -1.3638e-02,\n",
            "         -8.6066e-03,  5.2454e-02, -2.5320e-02, -5.0130e-02,  4.7970e-02,\n",
            "         -3.5183e-02, -1.6372e-02, -5.3223e-03, -6.3381e-02, -8.4110e-03,\n",
            "         -1.0092e-02, -4.4600e-03,  3.5936e-02,  4.1342e-02,  6.8033e-02,\n",
            "          1.7439e-02, -6.5009e-03, -3.5258e-02,  1.1115e-01,  3.7747e-02,\n",
            "         -7.4175e-03, -3.0977e-02, -8.4404e-03,  2.3719e-02, -8.1531e-03,\n",
            "          6.3430e-02,  1.0002e-02, -3.9504e-03,  2.2441e-02, -3.8997e-02,\n",
            "          3.8727e-04, -4.7440e-02,  4.1307e-02, -1.0226e-02,  7.5170e-03,\n",
            "         -1.7094e-02, -1.3593e-02,  3.8206e-02, -4.4937e-02, -2.5331e-02,\n",
            "          3.7538e-02, -2.3186e-02, -3.1547e-02,  3.9038e-02,  3.8438e-04,\n",
            "         -1.2166e-02,  5.9768e-02,  3.7915e-03,  7.1851e-03,  8.0763e-03,\n",
            "         -1.8748e-02,  1.1676e-02, -1.0086e-02, -6.1797e-03,  2.7438e-02,\n",
            "         -7.1863e-03, -3.1066e-02, -2.6043e-02, -5.7155e-02, -1.1832e-02,\n",
            "          5.8714e-02, -9.2315e-03, -4.9874e-02, -2.1004e-02, -3.7704e-02,\n",
            "         -1.9865e-03, -9.0720e-03, -5.0872e-03, -2.4683e-02, -3.4619e-02,\n",
            "         -2.2480e-02,  4.6438e-03, -1.2586e-02, -1.5037e-02, -8.7034e-02,\n",
            "          3.0025e-02,  4.4699e-02, -1.2916e-02, -6.8011e-03, -1.7767e-02,\n",
            "         -3.9375e-03,  9.1174e-03,  3.8424e-02,  4.8680e-02,  4.0032e-02,\n",
            "          7.7821e-03, -4.2529e-02, -6.5365e-03,  2.9994e-02, -4.5171e-04,\n",
            "         -2.5464e-02, -1.5263e-01, -2.4596e-02, -5.1374e-02, -3.5632e-02,\n",
            "          8.0352e-03, -1.2100e-02, -2.6445e-02,  4.7646e-02,  4.6451e-02,\n",
            "          3.3419e-02,  8.7073e-03,  2.1922e-02,  2.5412e-02, -3.1693e-02,\n",
            "         -1.3876e-02, -3.3281e-02, -9.9716e-03,  6.4030e-03, -3.8677e-02,\n",
            "         -4.2447e-03, -1.6411e-02,  1.8207e-02,  2.2252e-02,  3.6348e-02,\n",
            "          1.2098e-01,  4.5459e-02,  3.5674e-02,  1.3815e-02,  3.0748e-02,\n",
            "         -9.9422e-02,  8.0283e-03, -3.0951e-03, -1.0081e-02, -8.2394e-03,\n",
            "          1.2538e-02, -2.8729e-03, -1.0689e-02, -3.9362e-02,  4.9856e-02,\n",
            "          2.4964e-02, -2.0626e-02,  1.7475e-02,  1.8236e-02, -2.7967e-02,\n",
            "          1.5205e-02,  1.4647e-02,  1.3736e-04, -4.5558e-03,  8.9101e-03,\n",
            "         -1.2245e-01,  7.6470e-03,  7.5073e-03,  4.6429e-02, -2.4766e-02,\n",
            "          1.6280e-02, -5.1095e-03,  1.8448e-02, -2.7625e-03,  3.5815e-02,\n",
            "          3.4067e-02],\n",
            "        [-1.9930e-02,  1.7073e-02, -1.1271e-02,  3.4291e-02,  1.1776e-01,\n",
            "         -4.3388e-02, -2.2023e-02,  4.1511e-02,  1.8408e-02, -9.8949e-03,\n",
            "          5.1514e-03, -4.1964e-03, -6.5150e-03,  2.5203e-02,  1.0556e-01,\n",
            "          4.4222e-02,  2.5761e-03, -1.6616e-02, -1.0814e-02, -3.8600e-02,\n",
            "          9.9308e-02,  1.7712e-02, -6.5854e-03, -8.7738e-02, -4.3140e-02,\n",
            "         -4.6749e-02,  1.1540e-03, -6.1312e-02,  2.0765e-02, -8.4270e-03,\n",
            "          8.1573e-02, -3.1400e-02, -2.1246e-02,  8.3737e-03,  3.0556e-02,\n",
            "         -6.2093e-02,  1.1461e-03, -5.5203e-02, -4.5065e-03,  2.0635e-02,\n",
            "          2.4567e-02, -1.0933e-02, -3.1056e-02,  5.4126e-03, -3.0050e-02,\n",
            "         -1.3076e-02, -2.8392e-04,  8.3218e-02,  2.0712e-02,  1.1195e-03,\n",
            "          8.0317e-02, -1.0160e-02, -4.4355e-02, -3.9475e-02,  8.0660e-02,\n",
            "          8.8774e-02, -2.2538e-03, -9.4734e-02, -6.6667e-03,  3.1285e-02,\n",
            "          7.2789e-02, -2.0669e-02, -1.3937e-02,  9.1322e-02,  1.9531e-03,\n",
            "          5.3636e-02,  2.7961e-02, -5.7371e-02, -1.9178e-02, -1.1254e-02,\n",
            "          4.8806e-02,  2.8714e-02,  3.5312e-02,  9.0078e-02,  6.1828e-02,\n",
            "          2.9045e-02, -1.2508e-01,  8.9621e-02,  1.4374e-02, -4.5599e-02,\n",
            "         -4.6225e-02, -3.7430e-02, -6.9113e-02,  8.0101e-02, -8.1054e-04,\n",
            "         -3.7767e-02,  7.4670e-02, -3.3293e-02,  1.2092e-02,  2.1570e-02,\n",
            "         -2.2452e-02,  1.8438e-02, -3.9540e-02, -1.1997e-02, -1.6133e-02,\n",
            "         -5.5187e-02, -6.4670e-02, -2.7865e-02, -6.2715e-02, -2.5052e-02,\n",
            "         -5.0722e-02, -1.9459e-02,  1.7659e-02, -1.5619e-02,  6.3168e-03,\n",
            "         -3.3064e-03, -3.7397e-02, -2.1957e-02,  3.8640e-02, -3.3554e-04,\n",
            "          6.9147e-03,  8.4612e-03, -8.8311e-02,  3.7827e-02,  7.6595e-02,\n",
            "         -3.5249e-02, -7.8934e-02, -3.5057e-02, -5.6810e-02, -6.1153e-02,\n",
            "         -1.4866e-02, -1.4795e-02, -1.9368e-02, -4.0648e-02,  1.0494e-02,\n",
            "          1.2759e-02,  2.9109e-02,  2.8172e-02, -5.2384e-02,  3.9553e-03,\n",
            "          2.6694e-02,  2.8043e-02,  2.9258e-02, -2.4031e-02,  3.2886e-03,\n",
            "          4.3195e-03, -3.5545e-02,  2.1107e-02,  5.9183e-02,  4.4881e-02,\n",
            "         -2.4728e-02, -7.6610e-02,  8.9694e-02, -4.9274e-02, -1.2001e-02,\n",
            "         -1.2333e-02,  3.1473e-02, -2.5337e-02,  1.2241e-02,  3.0377e-03,\n",
            "         -1.1155e-03,  5.2775e-02,  2.4474e-02,  2.9492e-03, -3.2122e-02,\n",
            "          3.7203e-02,  1.6233e-02, -3.6166e-02,  7.7482e-02, -6.9091e-02,\n",
            "         -4.8564e-02, -1.9977e-02, -1.9872e-02, -6.8632e-02, -3.6145e-02,\n",
            "          3.9410e-02, -6.7896e-02, -8.1948e-03, -5.5625e-02, -1.4176e-02,\n",
            "          2.9042e-02, -3.7766e-02, -1.8941e-02, -2.7715e-02, -1.1037e-02,\n",
            "         -1.2082e-02,  9.6630e-03,  6.7850e-02,  1.8553e-02, -1.7482e-02,\n",
            "         -4.5012e-02, -1.7463e-02, -8.2950e-03, -5.8889e-02, -8.4076e-03,\n",
            "         -9.7095e-03, -6.4999e-03,  1.8419e-02, -3.8342e-02,  3.1981e-02,\n",
            "         -5.7772e-03, -2.3798e-02,  6.1862e-02, -1.0807e-01,  7.3051e-03,\n",
            "          7.7805e-03,  7.1615e-02, -1.5734e-02,  7.5420e-03, -4.1002e-02,\n",
            "          4.9127e-02, -1.8372e-02, -3.6124e-02,  2.7653e-02, -9.2795e-02,\n",
            "         -3.6511e-02, -1.0797e-02, -4.9436e-02, -1.2916e-02, -3.9519e-02,\n",
            "         -6.5719e-02,  6.3015e-02, -2.0611e-02, -6.2432e-02, -8.3756e-02,\n",
            "          3.2782e-02,  8.7806e-03,  3.7186e-02,  3.4531e-02, -2.3020e-03,\n",
            "          3.0095e-02, -1.8760e-03,  1.3929e-02,  8.3762e-02, -3.2483e-02,\n",
            "          3.7450e-02,  3.6245e-02, -5.0244e-04,  4.8748e-02, -4.1961e-03,\n",
            "         -2.5158e-02, -2.8493e-03,  7.6463e-02,  2.0066e-03,  6.4666e-02,\n",
            "         -7.8639e-02,  2.4656e-02, -2.4017e-02, -2.2331e-02,  4.8394e-02,\n",
            "         -8.0932e-02, -2.7942e-02,  6.2045e-02, -2.3802e-03, -1.0789e-02,\n",
            "         -2.0132e-02, -2.3511e-02,  3.0972e-02, -2.0270e-03, -1.0789e-02,\n",
            "         -1.5891e-02,  5.8091e-02, -1.2337e-02, -4.8228e-02, -1.2724e-02,\n",
            "          1.7782e-02],\n",
            "        [-1.4672e-02,  4.6380e-02, -3.6842e-02, -3.7738e-03, -5.9628e-04,\n",
            "         -5.7256e-02, -2.3545e-02, -4.2273e-02, -8.4102e-04,  4.4070e-02,\n",
            "         -6.5973e-03,  2.1362e-03,  1.7182e-02, -3.4367e-02, -2.3747e-02,\n",
            "         -1.9892e-02, -1.6265e-02,  1.3142e-02, -1.7113e-02, -2.0720e-02,\n",
            "          2.7548e-03, -3.1361e-03, -8.6373e-03, -3.3794e-02, -4.0843e-02,\n",
            "          5.0476e-02,  1.8407e-02, -5.2921e-02, -5.0296e-02,  3.4146e-04,\n",
            "         -6.6864e-02,  3.3808e-02, -2.1478e-02, -1.8113e-02, -1.0711e-03,\n",
            "         -4.0295e-02, -2.2288e-03,  5.5785e-02,  1.8899e-02, -4.6116e-02,\n",
            "         -6.6656e-02, -1.8930e-02, -4.5617e-02,  3.5368e-02, -2.3271e-02,\n",
            "         -4.3650e-02, -3.1646e-02,  1.2528e-02,  4.8148e-02, -2.5233e-02,\n",
            "         -1.1360e-02, -3.9200e-02, -4.0132e-02,  5.5176e-02,  8.7197e-03,\n",
            "          2.4476e-02, -5.4164e-02,  7.0630e-02,  2.2948e-02,  1.8675e-02,\n",
            "          2.3412e-02,  1.9072e-02, -2.3957e-02, -3.2134e-02, -1.1966e-02,\n",
            "          3.4077e-02,  1.1510e-02, -2.5836e-03, -5.9783e-03,  6.2074e-02,\n",
            "          1.3761e-02,  5.4898e-03,  1.4544e-02, -1.7586e-02, -5.7679e-02,\n",
            "         -5.8059e-02, -4.6951e-03,  2.0685e-02, -3.6448e-03,  1.7419e-02,\n",
            "         -5.4250e-02,  2.7484e-02,  1.5193e-03, -1.0611e-02,  8.0036e-03,\n",
            "         -1.0562e-02,  3.4366e-02,  1.6501e-02,  2.1763e-02, -6.9530e-03,\n",
            "         -9.9122e-03, -2.6186e-02,  2.2379e-02, -4.1992e-02, -5.4384e-02,\n",
            "          3.5822e-02,  3.1677e-02, -1.2708e-02, -1.5719e-02,  2.1684e-02,\n",
            "         -4.3054e-02, -4.4012e-02, -1.5282e-02, -4.9165e-02, -1.5853e-02,\n",
            "         -3.8676e-02,  2.6547e-02, -3.9112e-02, -2.0903e-03,  2.6122e-03,\n",
            "         -1.8791e-02,  2.5610e-02, -9.9296e-03,  3.7580e-03, -2.5330e-03,\n",
            "         -3.4211e-02,  1.8132e-02,  9.8730e-02, -3.7215e-03, -3.7283e-02,\n",
            "          2.1576e-02,  1.6629e-02,  7.0867e-02,  1.4469e-02, -3.1915e-02,\n",
            "         -2.0352e-02,  6.7264e-03,  4.4187e-02,  1.5493e-03, -7.6306e-03,\n",
            "         -7.3848e-02, -3.6040e-03,  8.1454e-03, -3.9666e-02, -1.9563e-03,\n",
            "          2.1018e-02,  1.9514e-02,  2.3297e-02,  3.2747e-02,  2.8780e-02,\n",
            "          2.7844e-02, -3.1435e-02,  1.5839e-02,  2.3782e-02,  2.0372e-02,\n",
            "         -9.2727e-03,  3.2967e-02,  4.7782e-02, -2.8692e-02,  1.7487e-02,\n",
            "         -2.2857e-03, -1.8505e-02,  5.0582e-02,  2.7631e-02,  2.3790e-02,\n",
            "          1.3591e-02, -3.6111e-03, -3.3650e-02,  6.4803e-03, -8.0835e-02,\n",
            "          4.6620e-02,  1.0793e-02,  1.8796e-02, -4.8103e-02, -7.9392e-03,\n",
            "          5.4758e-02, -4.0794e-02, -1.8931e-02, -6.1539e-02, -3.8634e-02,\n",
            "          3.3749e-02, -2.4943e-02,  3.3279e-02, -3.7196e-03,  2.9001e-02,\n",
            "         -4.1122e-02,  1.0284e-01,  3.2562e-02,  3.0632e-02, -8.2034e-02,\n",
            "         -2.5212e-02,  8.3908e-03, -4.4259e-02,  2.6996e-02,  2.9180e-02,\n",
            "         -2.3959e-02, -1.0901e-02,  9.9222e-03,  6.4268e-02,  1.1356e-02,\n",
            "          1.1918e-02, -3.9219e-02,  2.5843e-02, -2.4931e-02,  1.0515e-02,\n",
            "         -4.9435e-02,  4.4257e-02,  1.7941e-03,  7.9833e-02,  9.7628e-02,\n",
            "         -5.7216e-02, -1.8283e-02, -5.8580e-02, -6.3041e-02, -6.9790e-02,\n",
            "         -1.3652e-02,  3.6370e-02, -2.3873e-02,  4.2345e-02,  7.9344e-03,\n",
            "          2.3663e-02,  7.3565e-03,  6.6330e-02,  1.1148e-02, -2.8866e-02,\n",
            "         -2.5309e-02, -1.7983e-02,  3.4800e-02,  6.4387e-02,  2.0531e-02,\n",
            "          3.6804e-02,  2.2961e-02,  4.3863e-02,  2.9681e-02, -1.2278e-02,\n",
            "         -5.2524e-02, -3.1690e-02, -2.4385e-02, -3.4647e-02,  3.0894e-02,\n",
            "          2.4185e-02,  1.9355e-02,  2.9897e-02,  8.8723e-03,  7.7855e-03,\n",
            "         -4.3457e-02, -1.8602e-02,  2.3189e-02, -3.1308e-02,  3.0381e-02,\n",
            "          5.4360e-02, -3.7096e-03,  5.2235e-02, -8.2252e-03, -1.7928e-03,\n",
            "         -1.2956e-02, -4.4470e-02,  8.3000e-02,  7.6204e-02, -8.0716e-03,\n",
            "         -2.1283e-02,  3.2969e-02,  8.0900e-03,  1.7266e-02,  1.4864e-02,\n",
            "          1.0859e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.0779,  0.3410,  4.9943, -3.5216, -4.2023], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4968, -0.1655,  4.1359, -3.2628, -4.4118], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,   314,   761,  ...,   329,  5448,  2877],\n",
            "        [50256,   338,  1762,  ...,   220,   220,   220],\n",
            "        [50256,   460,   393,  ...,   220,   220,   220],\n",
            "        ...,\n",
            "        [50256,    12,   346,  ..., 35979, 28141,  2754],\n",
            "        [50256,  5097,    17,  ...,   326, 21082,    17],\n",
            "        [50256,   398,   485,  ...,   220,   220,  7618]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0117,  0.0718, -0.0184,  ..., -0.0026,  0.0358,  0.0341],\n",
            "        [-0.0199,  0.0170, -0.0111,  ..., -0.0483, -0.0128,  0.0178],\n",
            "        [-0.0145,  0.0469, -0.0366,  ...,  0.0174,  0.0147,  0.1086],\n",
            "        [-0.0109,  0.0584, -0.0588,  ..., -0.0170, -0.0324,  0.0274],\n",
            "        [ 0.0589, -0.0227, -0.0322,  ..., -0.0016,  0.0215,  0.0351]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1709e-02,  7.1752e-02, -1.8386e-02, -3.3898e-02, -9.8729e-03,\n",
            "         -5.0618e-03, -1.1269e-02,  3.7127e-03, -6.9190e-02, -1.3582e-02,\n",
            "          5.2487e-03,  1.5650e-02, -3.2396e-02, -2.8102e-02, -4.3243e-02,\n",
            "         -3.2269e-02, -3.1168e-03,  2.1135e-02,  1.4835e-02,  2.2021e-02,\n",
            "         -2.7672e-02,  4.4239e-02, -1.1726e-02, -3.3437e-02, -1.3066e-02,\n",
            "          2.7292e-02, -3.0726e-02,  3.4617e-02,  7.1930e-03, -1.0061e-02,\n",
            "         -7.4012e-02,  2.3435e-02,  1.4589e-02,  1.6092e-02, -3.2878e-02,\n",
            "         -8.7338e-03,  6.2468e-02,  3.1470e-02,  3.2556e-02,  1.3754e-03,\n",
            "         -4.2542e-02, -2.6036e-03,  2.9947e-02, -1.3714e-02,  1.0763e-02,\n",
            "          9.5428e-03,  2.8240e-02, -7.5952e-02, -3.7999e-02, -6.9545e-02,\n",
            "         -2.3032e-02,  1.4889e-02, -2.1917e-02,  4.6878e-02,  2.7401e-02,\n",
            "          2.9228e-03, -2.4411e-02, -1.6775e-02,  4.3809e-02,  2.0116e-02,\n",
            "         -3.6716e-03,  4.7242e-05, -5.0459e-03, -1.7566e-02, -3.5651e-02,\n",
            "          2.1051e-02,  5.0291e-02, -1.7974e-02,  2.3330e-02,  4.2541e-02,\n",
            "          2.4077e-02, -1.8014e-02, -1.3286e-02,  2.9434e-02, -4.5710e-02,\n",
            "         -3.6189e-02, -1.7820e-02,  7.1079e-02,  6.2579e-02,  2.4562e-02,\n",
            "         -1.2880e-02, -2.6925e-02,  3.4274e-02, -3.4632e-02, -9.8359e-03,\n",
            "          9.9210e-02,  6.3172e-03,  3.3184e-02, -1.4899e-02,  3.2239e-03,\n",
            "          2.7285e-03, -5.5877e-02,  2.2172e-02, -5.2710e-04, -2.1479e-02,\n",
            "         -1.3632e-02, -7.3257e-02, -4.5315e-02,  1.5484e-02, -1.4586e-02,\n",
            "         -4.5271e-02,  4.8001e-03,  1.0701e-02,  1.7177e-02, -1.3599e-02,\n",
            "         -8.5544e-03,  5.2457e-02, -2.5446e-02, -5.0238e-02,  4.7818e-02,\n",
            "         -3.5227e-02, -1.6558e-02, -5.3688e-03, -6.3530e-02, -8.4187e-03,\n",
            "         -1.0048e-02, -4.3564e-03,  3.5900e-02,  4.1142e-02,  6.8082e-02,\n",
            "          1.7563e-02, -6.6234e-03, -3.5122e-02,  1.1113e-01,  3.7716e-02,\n",
            "         -7.3370e-03, -3.0867e-02, -8.6327e-03,  2.3635e-02, -8.1871e-03,\n",
            "          6.3389e-02,  1.0072e-02, -3.8602e-03,  2.2476e-02, -3.9086e-02,\n",
            "          4.3831e-04, -4.7553e-02,  4.1369e-02, -1.0337e-02,  7.5317e-03,\n",
            "         -1.7086e-02, -1.3508e-02,  3.8286e-02, -4.5074e-02, -2.5233e-02,\n",
            "          3.7317e-02, -2.3210e-02, -3.1690e-02,  3.9011e-02,  2.9728e-04,\n",
            "         -1.2327e-02,  5.9689e-02,  3.7830e-03,  7.2658e-03,  8.1145e-03,\n",
            "         -1.8766e-02,  1.1976e-02, -9.9570e-03, -6.2887e-03,  2.7483e-02,\n",
            "         -7.1631e-03, -3.0999e-02, -2.6045e-02, -5.6989e-02, -1.1686e-02,\n",
            "          5.8844e-02, -9.0373e-03, -4.9932e-02, -2.0867e-02, -3.7739e-02,\n",
            "         -2.0003e-03, -9.0868e-03, -4.9746e-03, -2.4646e-02, -3.4672e-02,\n",
            "         -2.2428e-02,  4.6598e-03, -1.2568e-02, -1.5022e-02, -8.7102e-02,\n",
            "          3.0016e-02,  4.4852e-02, -1.2877e-02, -6.9648e-03, -1.7667e-02,\n",
            "         -3.9572e-03,  9.3651e-03,  3.8427e-02,  4.8534e-02,  3.9954e-02,\n",
            "          7.7112e-03, -4.2531e-02, -6.4322e-03,  2.9996e-02, -5.3616e-04,\n",
            "         -2.5529e-02, -1.5266e-01, -2.4825e-02, -5.1333e-02, -3.5542e-02,\n",
            "          8.0881e-03, -1.2121e-02, -2.6321e-02,  4.7758e-02,  4.6591e-02,\n",
            "          3.3275e-02,  8.9064e-03,  2.1974e-02,  2.5435e-02, -3.1735e-02,\n",
            "         -1.3855e-02, -3.3334e-02, -9.9668e-03,  6.4864e-03, -3.8585e-02,\n",
            "         -4.2374e-03, -1.6358e-02,  1.8330e-02,  2.2391e-02,  3.6401e-02,\n",
            "          1.2099e-01,  4.5321e-02,  3.5758e-02,  1.3733e-02,  3.0849e-02,\n",
            "         -9.9478e-02,  8.0197e-03, -3.0861e-03, -1.0214e-02, -8.4153e-03,\n",
            "          1.2523e-02, -2.8936e-03, -1.0689e-02, -3.9465e-02,  4.9967e-02,\n",
            "          2.5154e-02, -2.0651e-02,  1.7276e-02,  1.8281e-02, -2.7880e-02,\n",
            "          1.5340e-02,  1.4691e-02,  1.1344e-05, -4.6026e-03,  8.7093e-03,\n",
            "         -1.2246e-01,  7.8682e-03,  7.6517e-03,  4.6393e-02, -2.4780e-02,\n",
            "          1.6087e-02, -5.2489e-03,  1.8456e-02, -2.5665e-03,  3.5779e-02,\n",
            "          3.4071e-02],\n",
            "        [-1.9865e-02,  1.7027e-02, -1.1139e-02,  3.4728e-02,  1.1797e-01,\n",
            "         -4.3599e-02, -2.1987e-02,  4.1331e-02,  1.8469e-02, -9.9362e-03,\n",
            "          5.2637e-03, -4.4307e-03, -6.8736e-03,  2.5323e-02,  1.0582e-01,\n",
            "          4.4231e-02,  2.2373e-03, -1.6987e-02, -1.1004e-02, -3.8636e-02,\n",
            "          9.8975e-02,  1.7618e-02, -6.6101e-03, -8.7655e-02, -4.2943e-02,\n",
            "         -4.6274e-02,  1.4750e-03, -6.1409e-02,  2.0654e-02, -8.5458e-03,\n",
            "          8.1711e-02, -3.1404e-02, -2.0981e-02,  8.4376e-03,  3.0576e-02,\n",
            "         -6.1834e-02,  1.2452e-03, -5.5347e-02, -4.8573e-03,  2.0449e-02,\n",
            "          2.4801e-02, -1.0814e-02, -3.0916e-02,  5.0943e-03, -2.9931e-02,\n",
            "         -1.3197e-02, -2.7684e-04,  8.3395e-02,  2.0791e-02,  1.1101e-03,\n",
            "          8.0330e-02, -1.0304e-02, -4.4098e-02, -3.9600e-02,  8.0600e-02,\n",
            "          8.9222e-02, -2.1728e-03, -9.4595e-02, -6.8507e-03,  3.1052e-02,\n",
            "          7.3069e-02, -2.0739e-02, -1.3937e-02,  9.1319e-02,  2.0299e-03,\n",
            "          5.3826e-02,  2.7794e-02, -5.7350e-02, -1.9222e-02, -1.1245e-02,\n",
            "          4.8648e-02,  2.8989e-02,  3.5382e-02,  8.9933e-02,  6.1897e-02,\n",
            "          2.9026e-02, -1.2501e-01,  8.9810e-02,  1.4511e-02, -4.5715e-02,\n",
            "         -4.6434e-02, -3.7506e-02, -6.9267e-02,  8.0209e-02, -6.6606e-04,\n",
            "         -3.7725e-02,  7.4655e-02, -3.3397e-02,  1.1951e-02,  2.1618e-02,\n",
            "         -2.2590e-02,  1.8483e-02, -3.9839e-02, -1.2042e-02, -1.6086e-02,\n",
            "         -5.5032e-02, -6.4699e-02, -2.7388e-02, -6.2915e-02, -2.5302e-02,\n",
            "         -5.0872e-02, -1.9391e-02,  1.7845e-02, -1.5420e-02,  6.4325e-03,\n",
            "         -3.3748e-03, -3.7401e-02, -2.2112e-02,  3.8989e-02, -5.9042e-04,\n",
            "          6.7502e-03,  8.5140e-03, -8.7923e-02,  3.7994e-02,  7.6669e-02,\n",
            "         -3.5487e-02, -7.9147e-02, -3.4989e-02, -5.6672e-02, -6.1424e-02,\n",
            "         -1.5003e-02, -1.4650e-02, -1.9369e-02, -4.0856e-02,  1.0498e-02,\n",
            "          1.2694e-02,  2.9282e-02,  2.8317e-02, -5.2047e-02,  4.0832e-03,\n",
            "          2.6653e-02,  2.8272e-02,  2.8977e-02, -2.3887e-02,  2.7809e-03,\n",
            "          4.4210e-03, -3.5523e-02,  2.1220e-02,  5.9305e-02,  4.4918e-02,\n",
            "         -2.4800e-02, -7.6572e-02,  8.9997e-02, -4.9347e-02, -1.1990e-02,\n",
            "         -1.2080e-02,  3.1473e-02, -2.5797e-02,  1.2290e-02,  3.1397e-03,\n",
            "         -1.0588e-03,  5.2604e-02,  2.4641e-02,  2.9142e-03, -3.2098e-02,\n",
            "          3.7308e-02,  1.6124e-02, -3.6404e-02,  7.7718e-02, -6.9206e-02,\n",
            "         -4.8538e-02, -1.9966e-02, -1.9834e-02, -6.8658e-02, -3.6419e-02,\n",
            "          3.9225e-02, -6.8078e-02, -8.2409e-03, -5.5697e-02, -1.4095e-02,\n",
            "          2.9154e-02, -3.7873e-02, -1.8731e-02, -2.7663e-02, -1.1129e-02,\n",
            "         -1.2049e-02,  9.1789e-03,  6.7850e-02,  1.8635e-02, -1.7280e-02,\n",
            "         -4.4905e-02, -1.7430e-02, -8.3158e-03, -5.8698e-02, -8.5551e-03,\n",
            "         -9.6873e-03, -6.5070e-03,  1.8335e-02, -3.8369e-02,  3.1835e-02,\n",
            "         -5.9692e-03, -2.3669e-02,  6.2232e-02, -1.0790e-01,  7.0762e-03,\n",
            "          7.9349e-03,  7.1947e-02, -1.5796e-02,  7.4185e-03, -4.0827e-02,\n",
            "          4.9258e-02, -1.8624e-02, -3.6211e-02,  2.7633e-02, -9.2925e-02,\n",
            "         -3.7040e-02, -1.0513e-02, -4.9630e-02, -1.2570e-02, -3.9610e-02,\n",
            "         -6.5983e-02,  6.2876e-02, -2.0857e-02, -6.2362e-02, -8.4223e-02,\n",
            "          3.2587e-02,  8.5692e-03,  3.6839e-02,  3.4207e-02, -2.3057e-03,\n",
            "          3.0139e-02, -2.0081e-03,  1.3881e-02,  8.3811e-02, -3.2448e-02,\n",
            "          3.7554e-02,  3.6229e-02, -5.1621e-04,  4.8696e-02, -4.0791e-03,\n",
            "         -2.4671e-02, -2.5985e-03,  7.6438e-02,  2.2490e-03,  6.4515e-02,\n",
            "         -7.8652e-02,  2.4514e-02, -2.4014e-02, -2.2574e-02,  4.8802e-02,\n",
            "         -8.1196e-02, -2.7904e-02,  6.1771e-02, -2.1706e-03, -1.0992e-02,\n",
            "         -1.9974e-02, -2.3358e-02,  3.1162e-02, -1.9056e-03, -1.0597e-02,\n",
            "         -1.6150e-02,  5.8091e-02, -1.2334e-02, -4.8329e-02, -1.2840e-02,\n",
            "          1.7788e-02],\n",
            "        [-1.4539e-02,  4.6890e-02, -3.6614e-02, -3.7279e-03, -1.0238e-03,\n",
            "         -5.7038e-02, -2.3247e-02, -4.2125e-02, -7.7870e-04,  4.4178e-02,\n",
            "         -6.5533e-03,  1.8758e-03,  1.7034e-02, -3.4569e-02, -2.3451e-02,\n",
            "         -1.9683e-02, -1.6283e-02,  1.2666e-02, -1.7533e-02, -2.0638e-02,\n",
            "          2.5063e-03, -3.0540e-03, -8.3113e-03, -3.3893e-02, -4.0748e-02,\n",
            "          5.1135e-02,  1.8524e-02, -5.2969e-02, -5.0748e-02,  3.0058e-04,\n",
            "         -6.6740e-02,  3.4148e-02, -2.1147e-02, -1.8271e-02, -1.0068e-03,\n",
            "         -4.0175e-02, -2.2459e-03,  5.5573e-02,  1.8774e-02, -4.6333e-02,\n",
            "         -6.6794e-02, -1.8770e-02, -4.5569e-02,  3.5127e-02, -2.3363e-02,\n",
            "         -4.3648e-02, -3.1311e-02,  1.2440e-02,  4.8267e-02, -2.5232e-02,\n",
            "         -1.1389e-02, -3.8901e-02, -4.0072e-02,  5.5410e-02,  8.7751e-03,\n",
            "          2.4559e-02, -5.4116e-02,  7.0566e-02,  2.2847e-02,  1.8585e-02,\n",
            "          2.3550e-02,  1.8926e-02, -2.3945e-02, -3.2447e-02, -1.2010e-02,\n",
            "          3.4037e-02,  1.1139e-02, -1.8429e-03, -5.7691e-03,  6.1984e-02,\n",
            "          1.3740e-02,  5.7906e-03,  1.4442e-02, -1.7520e-02, -5.7760e-02,\n",
            "         -5.8193e-02, -4.5482e-03,  2.0624e-02, -4.0222e-03,  1.7235e-02,\n",
            "         -5.4096e-02,  2.7363e-02,  1.5809e-03, -1.0845e-02,  8.2385e-03,\n",
            "         -1.0565e-02,  3.4461e-02,  1.6711e-02,  2.1786e-02, -6.6531e-03,\n",
            "         -9.8929e-03, -2.6581e-02,  2.2188e-02, -4.2195e-02, -5.4536e-02,\n",
            "          3.6218e-02,  3.1623e-02, -1.2149e-02, -1.5486e-02,  2.1543e-02,\n",
            "         -4.3288e-02, -4.3790e-02, -1.5069e-02, -4.9085e-02, -1.5862e-02,\n",
            "         -3.8520e-02,  2.6523e-02, -3.9410e-02, -1.9120e-03,  2.4409e-03,\n",
            "         -1.8874e-02,  2.5764e-02, -9.3846e-03,  3.5335e-03, -2.4742e-03,\n",
            "         -3.4751e-02,  1.7836e-02,  9.8735e-02, -3.5767e-03, -3.7043e-02,\n",
            "          2.1249e-02,  1.6474e-02,  7.0929e-02,  1.4467e-02, -3.2122e-02,\n",
            "         -2.0327e-02,  6.9949e-03,  4.4269e-02,  1.6829e-03, -7.5788e-03,\n",
            "         -7.3982e-02, -3.4407e-03,  7.7699e-03, -3.9896e-02, -2.0073e-03,\n",
            "          2.1339e-02,  1.9762e-02,  2.3215e-02,  3.2452e-02,  2.8374e-02,\n",
            "          2.8037e-02, -3.1157e-02,  1.5980e-02,  2.3603e-02,  2.0624e-02,\n",
            "         -9.0960e-03,  3.3164e-02,  4.7531e-02, -2.8620e-02,  1.7571e-02,\n",
            "         -2.3895e-03, -1.8348e-02,  5.0443e-02,  2.7704e-02,  2.3975e-02,\n",
            "          1.3515e-02, -3.5827e-03, -3.3689e-02,  6.3140e-03, -8.0701e-02,\n",
            "          4.6655e-02,  1.0595e-02,  1.8977e-02, -4.8123e-02, -8.3326e-03,\n",
            "          5.4389e-02, -4.1254e-02, -1.8861e-02, -6.1778e-02, -3.8658e-02,\n",
            "          3.3572e-02, -2.4885e-02,  3.3466e-02, -3.3097e-03,  2.9344e-02,\n",
            "         -4.0986e-02,  1.0264e-01,  3.2873e-02,  3.0559e-02, -8.1942e-02,\n",
            "         -2.5199e-02,  8.3937e-03, -4.3716e-02,  2.6978e-02,  2.9149e-02,\n",
            "         -2.4066e-02, -1.0833e-02,  9.7943e-03,  6.4494e-02,  1.1339e-02,\n",
            "          1.1603e-02, -3.9111e-02,  2.6121e-02, -2.4803e-02,  1.0694e-02,\n",
            "         -4.9643e-02,  4.4283e-02,  1.7519e-03,  7.9567e-02,  9.7812e-02,\n",
            "         -5.7430e-02, -1.8408e-02, -5.8648e-02, -6.2978e-02, -6.9691e-02,\n",
            "         -1.4251e-02,  3.6780e-02, -2.4139e-02,  4.2582e-02,  7.4743e-03,\n",
            "          2.3748e-02,  7.4071e-03,  6.6498e-02,  1.1004e-02, -2.9166e-02,\n",
            "         -2.5849e-02, -1.8055e-02,  3.4620e-02,  6.4169e-02,  2.0561e-02,\n",
            "          3.6762e-02,  2.2993e-02,  4.3780e-02,  2.9964e-02, -1.1947e-02,\n",
            "         -5.2794e-02, -3.1978e-02, -2.4186e-02, -3.5148e-02,  3.0896e-02,\n",
            "          2.4592e-02,  1.9313e-02,  2.9931e-02,  9.2236e-03,  7.9353e-03,\n",
            "         -4.3454e-02, -1.8585e-02,  2.3315e-02, -3.1561e-02,  3.0404e-02,\n",
            "          5.4261e-02, -3.6352e-03,  5.1989e-02, -8.4207e-03, -1.8995e-03,\n",
            "         -1.3064e-02, -4.4608e-02,  8.3229e-02,  7.6097e-02, -8.0150e-03,\n",
            "         -2.1286e-02,  3.2746e-02,  8.0649e-03,  1.7410e-02,  1.4731e-02,\n",
            "          1.0858e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.1298,  0.3245,  4.9465, -3.5919, -4.2192], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4829, -0.1974,  4.1331, -3.4089, -4.4269], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,   705,    55,  ...,    11,   314,  1612],\n",
            "        [50256, 30071,    25,  ...,    13,    56,    13],\n",
            "        [50256,    12,  1507,  ...,    76,   329, 17751],\n",
            "        ...,\n",
            "        [50256,  5705, 38565,  ...,  2091,   198,  2061],\n",
            "        [50256,  1466,   319,  ...,  2371,   198,   198],\n",
            "        [50256,   261, 29978,  ...,    75,   482,   287]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0115,  0.0717, -0.0186,  ..., -0.0023,  0.0357,  0.0341],\n",
            "        [-0.0198,  0.0172, -0.0112,  ..., -0.0484, -0.0131,  0.0179],\n",
            "        [-0.0143,  0.0474, -0.0365,  ...,  0.0174,  0.0146,  0.1087],\n",
            "        [-0.0111,  0.0587, -0.0587,  ..., -0.0168, -0.0323,  0.0274],\n",
            "        [ 0.0590, -0.0227, -0.0319,  ..., -0.0018,  0.0216,  0.0352]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1534e-02,  7.1694e-02, -1.8596e-02, -3.3914e-02, -9.9253e-03,\n",
            "         -4.9550e-03, -1.1282e-02,  3.7909e-03, -6.9286e-02, -1.3377e-02,\n",
            "          5.1239e-03,  1.5602e-02, -3.2570e-02, -2.8353e-02, -4.3185e-02,\n",
            "         -3.2393e-02, -3.1890e-03,  2.0932e-02,  1.4937e-02,  2.1857e-02,\n",
            "         -2.8042e-02,  4.4161e-02, -1.1774e-02, -3.3344e-02, -1.3264e-02,\n",
            "          2.7193e-02, -3.0523e-02,  3.4952e-02,  7.1342e-03, -1.0002e-02,\n",
            "         -7.3992e-02,  2.3407e-02,  1.4734e-02,  1.6133e-02, -3.3051e-02,\n",
            "         -8.6823e-03,  6.2548e-02,  3.1722e-02,  3.2626e-02,  1.4089e-03,\n",
            "         -4.2161e-02, -2.7896e-03,  2.9968e-02, -1.4035e-02,  1.1097e-02,\n",
            "          9.3499e-03,  2.8384e-02, -7.5933e-02, -3.8175e-02, -6.9468e-02,\n",
            "         -2.3153e-02,  1.4888e-02, -2.2003e-02,  4.7016e-02,  2.7528e-02,\n",
            "          2.9643e-03, -2.4091e-02, -1.6838e-02,  4.3917e-02,  2.0462e-02,\n",
            "         -3.8121e-03,  1.0798e-04, -5.0316e-03, -1.7512e-02, -3.5488e-02,\n",
            "          2.0846e-02,  5.0345e-02, -1.7779e-02,  2.3459e-02,  4.2750e-02,\n",
            "          2.3995e-02, -1.7941e-02, -1.3410e-02,  2.9502e-02, -4.5530e-02,\n",
            "         -3.6305e-02, -1.7804e-02,  7.1129e-02,  6.2554e-02,  2.4546e-02,\n",
            "         -1.2939e-02, -2.6982e-02,  3.4204e-02, -3.4833e-02, -1.0154e-02,\n",
            "          9.9291e-02,  6.2481e-03,  3.3247e-02, -1.4718e-02,  3.2398e-03,\n",
            "          2.9584e-03, -5.5886e-02,  2.2155e-02, -2.7043e-04, -2.1528e-02,\n",
            "         -1.3543e-02, -7.3411e-02, -4.5035e-02,  1.5539e-02, -1.4722e-02,\n",
            "         -4.5256e-02,  4.7894e-03,  1.0458e-02,  1.7068e-02, -1.3427e-02,\n",
            "         -8.3573e-03,  5.2468e-02, -2.5568e-02, -5.0356e-02,  4.7517e-02,\n",
            "         -3.5181e-02, -1.6791e-02, -5.4456e-03, -6.3736e-02, -8.6113e-03,\n",
            "         -1.0069e-02, -4.2757e-03,  3.5918e-02,  4.0988e-02,  6.8201e-02,\n",
            "          1.7739e-02, -6.7927e-03, -3.4874e-02,  1.1100e-01,  3.7731e-02,\n",
            "         -7.2582e-03, -3.0714e-02, -8.9234e-03,  2.3629e-02, -8.0442e-03,\n",
            "          6.3276e-02,  1.0144e-02, -3.8408e-03,  2.2641e-02, -3.9229e-02,\n",
            "          5.0671e-04, -4.7531e-02,  4.1385e-02, -1.0523e-02,  7.6705e-03,\n",
            "         -1.7121e-02, -1.3439e-02,  3.8305e-02, -4.5114e-02, -2.5036e-02,\n",
            "          3.7088e-02, -2.3289e-02, -3.1944e-02,  3.9044e-02,  2.6729e-04,\n",
            "         -1.2585e-02,  5.9649e-02,  3.8121e-03,  7.3398e-03,  8.1045e-03,\n",
            "         -1.8860e-02,  1.2371e-02, -9.7780e-03, -6.4414e-03,  2.7608e-02,\n",
            "         -7.1346e-03, -3.0948e-02, -2.5987e-02, -5.6747e-02, -1.1438e-02,\n",
            "          5.8946e-02, -8.8466e-03, -5.0006e-02, -2.0803e-02, -3.7676e-02,\n",
            "         -2.1321e-03, -9.1466e-03, -4.7739e-03, -2.4540e-02, -3.4743e-02,\n",
            "         -2.2410e-02,  4.6042e-03, -1.2655e-02, -1.5087e-02, -8.7110e-02,\n",
            "          3.0048e-02,  4.4885e-02, -1.2792e-02, -7.1913e-03, -1.7789e-02,\n",
            "         -3.8649e-03,  9.6173e-03,  3.8408e-02,  4.8412e-02,  3.9839e-02,\n",
            "          7.5720e-03, -4.2397e-02, -6.3762e-03,  3.0049e-02, -6.7227e-04,\n",
            "         -2.5557e-02, -1.5275e-01, -2.5144e-02, -5.1292e-02, -3.5497e-02,\n",
            "          8.1961e-03, -1.2294e-02, -2.6071e-02,  4.7904e-02,  4.6766e-02,\n",
            "          3.3056e-02,  9.2327e-03,  2.2041e-02,  2.5482e-02, -3.1716e-02,\n",
            "         -1.3889e-02, -3.3484e-02, -1.0081e-02,  6.5307e-03, -3.8545e-02,\n",
            "         -4.1538e-03, -1.6274e-02,  1.8497e-02,  2.2384e-02,  3.6505e-02,\n",
            "          1.2095e-01,  4.5040e-02,  3.5798e-02,  1.3536e-02,  3.1024e-02,\n",
            "         -9.9458e-02,  7.9351e-03, -3.0313e-03, -1.0391e-02, -8.6334e-03,\n",
            "          1.2675e-02, -2.9641e-03, -1.0629e-02, -3.9650e-02,  5.0019e-02,\n",
            "          2.5451e-02, -2.0626e-02,  1.7178e-02,  1.8300e-02, -2.7700e-02,\n",
            "          1.5480e-02,  1.4675e-02, -1.6756e-04, -4.5959e-03,  8.3844e-03,\n",
            "         -1.2259e-01,  8.1125e-03,  7.7649e-03,  4.6405e-02, -2.4879e-02,\n",
            "          1.5876e-02, -5.4523e-03,  1.8520e-02, -2.3224e-03,  3.5713e-02,\n",
            "          3.4076e-02],\n",
            "        [-1.9815e-02,  1.7228e-02, -1.1161e-02,  3.5173e-02,  1.1798e-01,\n",
            "         -4.3708e-02, -2.2007e-02,  4.1289e-02,  1.8474e-02, -9.8020e-03,\n",
            "          5.2683e-03, -4.6198e-03, -7.4110e-03,  2.5280e-02,  1.0629e-01,\n",
            "          4.4415e-02,  1.9719e-03, -1.7199e-02, -1.1621e-02, -3.8549e-02,\n",
            "          9.8778e-02,  1.7445e-02, -6.4511e-03, -8.7609e-02, -4.2820e-02,\n",
            "         -4.5870e-02,  1.8181e-03, -6.1643e-02,  2.0455e-02, -8.8141e-03,\n",
            "          8.2074e-02, -3.1298e-02, -2.0537e-02,  8.5080e-03,  3.0602e-02,\n",
            "         -6.1538e-02,  1.3383e-03, -5.5490e-02, -5.1083e-03,  2.0156e-02,\n",
            "          2.4981e-02, -1.0828e-02, -3.0619e-02,  4.6014e-03, -2.9925e-02,\n",
            "         -1.3580e-02, -2.3347e-04,  8.3607e-02,  2.0970e-02,  1.1047e-03,\n",
            "          8.0404e-02, -1.0320e-02, -4.3941e-02, -3.9734e-02,  8.0794e-02,\n",
            "          8.9657e-02, -2.1524e-03, -9.4417e-02, -7.0021e-03,  3.1014e-02,\n",
            "          7.3346e-02, -2.0764e-02, -1.3861e-02,  9.1402e-02,  2.0320e-03,\n",
            "          5.3897e-02,  2.7602e-02, -5.7261e-02, -1.9121e-02, -1.1036e-02,\n",
            "          4.8622e-02,  2.9390e-02,  3.5289e-02,  8.9946e-02,  6.2012e-02,\n",
            "          2.9126e-02, -1.2509e-01,  8.9825e-02,  1.4560e-02, -4.5917e-02,\n",
            "         -4.6561e-02, -3.7376e-02, -6.9335e-02,  8.0424e-02, -2.4347e-04,\n",
            "         -3.7734e-02,  7.4548e-02, -3.3291e-02,  1.1976e-02,  2.1967e-02,\n",
            "         -2.2865e-02,  1.8338e-02, -4.0143e-02, -1.2165e-02, -1.6328e-02,\n",
            "         -5.4844e-02, -6.4928e-02, -2.6860e-02, -6.2897e-02, -2.5750e-02,\n",
            "         -5.1152e-02, -1.9364e-02,  1.7958e-02, -1.5379e-02,  6.7134e-03,\n",
            "         -3.0910e-03, -3.7612e-02, -2.2190e-02,  3.9432e-02, -9.1044e-04,\n",
            "          6.8340e-03,  8.5645e-03, -8.7595e-02,  3.8025e-02,  7.6877e-02,\n",
            "         -3.5938e-02, -7.9371e-02, -3.4728e-02, -5.6495e-02, -6.1590e-02,\n",
            "         -1.5149e-02, -1.4786e-02, -1.9303e-02, -4.1269e-02,  1.0369e-02,\n",
            "          1.2728e-02,  2.9609e-02,  2.8424e-02, -5.1790e-02,  4.2096e-03,\n",
            "          2.6615e-02,  2.8573e-02,  2.8681e-02, -2.3900e-02,  2.2053e-03,\n",
            "          4.3788e-03, -3.5257e-02,  2.1188e-02,  5.9236e-02,  4.5092e-02,\n",
            "         -2.4841e-02, -7.6711e-02,  9.0209e-02, -4.9374e-02, -1.1928e-02,\n",
            "         -1.1579e-02,  3.1594e-02, -2.6120e-02,  1.2478e-02,  3.3185e-03,\n",
            "         -1.1013e-03,  5.2484e-02,  2.4686e-02,  3.0826e-03, -3.1924e-02,\n",
            "          3.7286e-02,  1.6026e-02, -3.6747e-02,  7.7853e-02, -6.9315e-02,\n",
            "         -4.8555e-02, -1.9983e-02, -1.9696e-02, -6.8739e-02, -3.6590e-02,\n",
            "          3.8997e-02, -6.8519e-02, -8.4710e-03, -5.5960e-02, -1.3865e-02,\n",
            "          2.8979e-02, -3.7890e-02, -1.8416e-02, -2.7480e-02, -1.1099e-02,\n",
            "         -1.1950e-02,  8.9680e-03,  6.7980e-02,  1.8706e-02, -1.7038e-02,\n",
            "         -4.4842e-02, -1.7682e-02, -8.2562e-03, -5.8550e-02, -8.7726e-03,\n",
            "         -9.5841e-03, -6.5362e-03,  1.8255e-02, -3.8214e-02,  3.1889e-02,\n",
            "         -6.1532e-03, -2.3349e-02,  6.2804e-02, -1.0762e-01,  6.8588e-03,\n",
            "          8.1338e-03,  7.2428e-02, -1.5907e-02,  7.3290e-03, -4.0512e-02,\n",
            "          4.9040e-02, -1.8909e-02, -3.6222e-02,  2.7424e-02, -9.3089e-02,\n",
            "         -3.7510e-02, -1.0111e-02, -5.0199e-02, -1.2232e-02, -3.9908e-02,\n",
            "         -6.6234e-02,  6.2746e-02, -2.1095e-02, -6.2421e-02, -8.4822e-02,\n",
            "          3.2419e-02,  8.5060e-03,  3.6682e-02,  3.3878e-02, -2.1270e-03,\n",
            "          3.0107e-02, -2.1278e-03,  1.3635e-02,  8.3893e-02, -3.2080e-02,\n",
            "          3.7559e-02,  3.6024e-02, -4.5012e-04,  4.8555e-02, -3.8414e-03,\n",
            "         -2.4140e-02, -2.5241e-03,  7.6652e-02,  2.3855e-03,  6.4143e-02,\n",
            "         -7.8700e-02,  2.4547e-02, -2.3885e-02, -2.2740e-02,  4.9200e-02,\n",
            "         -8.1579e-02, -2.8132e-02,  6.1570e-02, -1.9966e-03, -1.1213e-02,\n",
            "         -1.9916e-02, -2.3414e-02,  3.1396e-02, -2.0180e-03, -1.0613e-02,\n",
            "         -1.6469e-02,  5.8036e-02, -1.2283e-02, -4.8438e-02, -1.3075e-02,\n",
            "          1.7890e-02],\n",
            "        [-1.4312e-02,  4.7361e-02, -3.6518e-02, -3.8091e-03, -1.4411e-03,\n",
            "         -5.6776e-02, -2.3030e-02, -4.1903e-02, -7.0584e-04,  4.4270e-02,\n",
            "         -6.6762e-03,  1.6218e-03,  1.6742e-02, -3.4858e-02, -2.2967e-02,\n",
            "         -1.9560e-02, -1.6232e-02,  1.2331e-02, -1.7967e-02, -2.0607e-02,\n",
            "          2.2440e-03, -3.0894e-03, -8.0070e-03, -3.3956e-02, -4.0731e-02,\n",
            "          5.1562e-02,  1.8645e-02, -5.3068e-02, -5.1143e-02,  1.5605e-04,\n",
            "         -6.6665e-02,  3.4602e-02, -2.0703e-02, -1.8380e-02, -8.3557e-04,\n",
            "         -4.0117e-02, -2.2820e-03,  5.5314e-02,  1.8667e-02, -4.6515e-02,\n",
            "         -6.6939e-02, -1.8672e-02, -4.5501e-02,  3.5012e-02, -2.3485e-02,\n",
            "         -4.3738e-02, -3.1090e-02,  1.2261e-02,  4.8402e-02, -2.5259e-02,\n",
            "         -1.1423e-02, -3.8547e-02, -4.0099e-02,  5.5552e-02,  8.8587e-03,\n",
            "          2.4649e-02, -5.4108e-02,  7.0372e-02,  2.2618e-02,  1.8639e-02,\n",
            "          2.3566e-02,  1.8964e-02, -2.3906e-02, -3.2614e-02, -1.2137e-02,\n",
            "          3.4044e-02,  1.0972e-02, -1.2378e-03, -5.5875e-03,  6.1969e-02,\n",
            "          1.3781e-02,  6.0551e-03,  1.4125e-02, -1.7463e-02, -5.7774e-02,\n",
            "         -5.8318e-02, -4.3418e-03,  2.0511e-02, -4.4027e-03,  1.7102e-02,\n",
            "         -5.3991e-02,  2.7305e-02,  1.6154e-03, -1.0948e-02,  8.1990e-03,\n",
            "         -1.0624e-02,  3.4568e-02,  1.6883e-02,  2.1869e-02, -6.3222e-03,\n",
            "         -9.7565e-03, -2.6929e-02,  2.1878e-02, -4.2403e-02, -5.4575e-02,\n",
            "          3.6684e-02,  3.1446e-02, -1.1599e-02, -1.5254e-02,  2.1215e-02,\n",
            "         -4.3532e-02, -4.3552e-02, -1.5034e-02, -4.9057e-02, -1.5799e-02,\n",
            "         -3.8244e-02,  2.6403e-02, -3.9609e-02, -1.7355e-03,  2.2058e-03,\n",
            "         -1.8811e-02,  2.5874e-02, -8.8770e-03,  3.2775e-03, -2.5059e-03,\n",
            "         -3.5391e-02,  1.7585e-02,  9.8627e-02, -3.5532e-03, -3.6748e-02,\n",
            "          2.0946e-02,  1.6385e-02,  7.1014e-02,  1.4403e-02, -3.2330e-02,\n",
            "         -2.0323e-02,  7.3490e-03,  4.4282e-02,  1.6947e-03, -7.6393e-03,\n",
            "         -7.4097e-02, -3.2761e-03,  7.5936e-03, -4.0060e-02, -2.0027e-03,\n",
            "          2.1467e-02,  1.9969e-02,  2.3019e-02,  3.2223e-02,  2.8110e-02,\n",
            "          2.8261e-02, -3.0937e-02,  1.6112e-02,  2.3384e-02,  2.0742e-02,\n",
            "         -8.9907e-03,  3.3412e-02,  4.7440e-02, -2.8543e-02,  1.7683e-02,\n",
            "         -2.4270e-03, -1.8218e-02,  5.0322e-02,  2.7793e-02,  2.4165e-02,\n",
            "          1.3344e-02, -3.6655e-03, -3.3782e-02,  6.1088e-03, -8.0679e-02,\n",
            "          4.6635e-02,  1.0450e-02,  1.9188e-02, -4.8204e-02, -8.3771e-03,\n",
            "          5.4053e-02, -4.1710e-02, -1.8875e-02, -6.2083e-02, -3.8437e-02,\n",
            "          3.3339e-02, -2.4791e-02,  3.3619e-02, -2.9439e-03,  2.9753e-02,\n",
            "         -4.0873e-02,  1.0259e-01,  3.3202e-02,  3.0650e-02, -8.1922e-02,\n",
            "         -2.5056e-02,  8.3600e-03, -4.3191e-02,  2.7011e-02,  2.9041e-02,\n",
            "         -2.4066e-02, -1.0713e-02,  9.7648e-03,  6.4812e-02,  1.1263e-02,\n",
            "          1.1223e-02, -3.9042e-02,  2.6484e-02, -2.4685e-02,  1.0964e-02,\n",
            "         -4.9895e-02,  4.4271e-02,  1.7645e-03,  7.9291e-02,  9.7866e-02,\n",
            "         -5.7653e-02, -1.8456e-02, -5.8677e-02, -6.2986e-02, -6.9437e-02,\n",
            "         -1.4747e-02,  3.7270e-02, -2.4394e-02,  4.2862e-02,  6.9794e-03,\n",
            "          2.3865e-02,  7.3401e-03,  6.6683e-02,  1.0908e-02, -2.9365e-02,\n",
            "         -2.6346e-02, -1.8097e-02,  3.4462e-02,  6.4179e-02,  2.0770e-02,\n",
            "          3.6810e-02,  2.3013e-02,  4.3547e-02,  3.0253e-02, -1.1584e-02,\n",
            "         -5.3059e-02, -3.2312e-02, -2.3814e-02, -3.5689e-02,  3.1082e-02,\n",
            "          2.4955e-02,  1.9135e-02,  2.9986e-02,  9.2622e-03,  8.0107e-03,\n",
            "         -4.3542e-02, -1.8537e-02,  2.3389e-02, -3.1821e-02,  3.0352e-02,\n",
            "          5.4186e-02, -3.4956e-03,  5.1820e-02, -8.6588e-03, -1.9729e-03,\n",
            "         -1.3276e-02, -4.4814e-02,  8.3495e-02,  7.6023e-02, -8.0446e-03,\n",
            "         -2.1240e-02,  3.2504e-02,  8.1322e-03,  1.7435e-02,  1.4612e-02,\n",
            "          1.0870e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.2357,  0.3607,  4.8552, -3.6976, -4.2222], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4534, -0.1419,  4.1009, -3.6506, -4.4448], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256, 17755,    11,  ..., 20250,    11,   355],\n",
            "        [50256,   220,   220,  ...,   220,   220,   220],\n",
            "        [50256, 29337,   290,  ...,  4361, 14523,  1910],\n",
            "        ...,\n",
            "        [50256, 10100,  4008,  ...,    33, 31249, 15853],\n",
            "        [50256,   262, 10088,  ...,  4083, 38011,    26],\n",
            "        [50256,   327,    62,  ...,   683,   319,   262]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0114,  0.0716, -0.0188,  ..., -0.0021,  0.0357,  0.0341],\n",
            "        [-0.0197,  0.0175, -0.0113,  ..., -0.0486, -0.0134,  0.0180],\n",
            "        [-0.0140,  0.0480, -0.0366,  ...,  0.0174,  0.0143,  0.1090],\n",
            "        [-0.0114,  0.0590, -0.0586,  ..., -0.0166, -0.0322,  0.0273],\n",
            "        [ 0.0591, -0.0228, -0.0317,  ..., -0.0019,  0.0216,  0.0352]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1396e-02,  7.1640e-02, -1.8762e-02, -3.3949e-02, -9.9448e-03,\n",
            "         -4.9268e-03, -1.1303e-02,  3.8119e-03, -6.9420e-02, -1.3194e-02,\n",
            "          4.9752e-03,  1.5527e-02, -3.2767e-02, -2.8577e-02, -4.2935e-02,\n",
            "         -3.2518e-02, -3.3154e-03,  2.0727e-02,  1.5022e-02,  2.1688e-02,\n",
            "         -2.8501e-02,  4.4115e-02, -1.1866e-02, -3.3186e-02, -1.3474e-02,\n",
            "          2.7106e-02, -3.0226e-02,  3.5281e-02,  7.1283e-03, -1.0022e-02,\n",
            "         -7.3950e-02,  2.3289e-02,  1.4807e-02,  1.6269e-02, -3.3132e-02,\n",
            "         -8.6652e-03,  6.2725e-02,  3.1861e-02,  3.2659e-02,  1.5137e-03,\n",
            "         -4.1798e-02, -2.9609e-03,  2.9981e-02, -1.4281e-02,  1.1354e-02,\n",
            "          9.1970e-03,  2.8385e-02, -7.5835e-02, -3.8328e-02, -6.9511e-02,\n",
            "         -2.3300e-02,  1.4820e-02, -2.2026e-02,  4.7189e-02,  2.7677e-02,\n",
            "          2.9769e-03, -2.3774e-02, -1.6827e-02,  4.4020e-02,  2.0719e-02,\n",
            "         -3.8359e-03,  2.3965e-04, -4.9829e-03, -1.7374e-02, -3.5347e-02,\n",
            "          2.0687e-02,  5.0384e-02, -1.7586e-02,  2.3550e-02,  4.2918e-02,\n",
            "          2.3987e-02, -1.7837e-02, -1.3529e-02,  2.9557e-02, -4.5281e-02,\n",
            "         -3.6487e-02, -1.7799e-02,  7.1156e-02,  6.2616e-02,  2.4571e-02,\n",
            "         -1.3046e-02, -2.6963e-02,  3.4077e-02, -3.4888e-02, -1.0456e-02,\n",
            "          9.9303e-02,  6.1228e-03,  3.3248e-02, -1.4531e-02,  3.2443e-03,\n",
            "          3.1714e-03, -5.5883e-02,  2.2024e-02,  4.0390e-05, -2.1587e-02,\n",
            "         -1.3513e-02, -7.3593e-02, -4.4705e-02,  1.5556e-02, -1.4941e-02,\n",
            "         -4.5301e-02,  4.7185e-03,  1.0197e-02,  1.6928e-02, -1.3272e-02,\n",
            "         -8.1381e-03,  5.2495e-02, -2.5739e-02, -5.0422e-02,  4.7182e-02,\n",
            "         -3.5182e-02, -1.7055e-02, -5.5044e-03, -6.3839e-02, -8.8646e-03,\n",
            "         -1.0113e-02, -4.2333e-03,  3.5902e-02,  4.0835e-02,  6.8302e-02,\n",
            "          1.7945e-02, -6.9361e-03, -3.4588e-02,  1.1091e-01,  3.7721e-02,\n",
            "         -7.1819e-03, -3.0634e-02, -9.2614e-03,  2.3667e-02, -7.8159e-03,\n",
            "          6.3077e-02,  1.0166e-02, -3.8522e-03,  2.2896e-02, -3.9437e-02,\n",
            "          4.6769e-04, -4.7355e-02,  4.1451e-02, -1.0709e-02,  7.8776e-03,\n",
            "         -1.7197e-02, -1.3453e-02,  3.8264e-02, -4.5053e-02, -2.4824e-02,\n",
            "          3.6869e-02, -2.3367e-02, -3.2292e-02,  3.9058e-02,  3.5825e-04,\n",
            "         -1.2763e-02,  5.9712e-02,  3.8094e-03,  7.3940e-03,  8.1216e-03,\n",
            "         -1.8860e-02,  1.2735e-02, -9.7031e-03, -6.5604e-03,  2.7727e-02,\n",
            "         -7.1518e-03, -3.1060e-02, -2.6003e-02, -5.6593e-02, -1.1205e-02,\n",
            "          5.9106e-02, -8.8153e-03, -5.0102e-02, -2.0822e-02, -3.7521e-02,\n",
            "         -2.2483e-03, -9.2803e-03, -4.5070e-03, -2.4466e-02, -3.4845e-02,\n",
            "         -2.2458e-02,  4.5056e-03, -1.2784e-02, -1.5207e-02, -8.7049e-02,\n",
            "          3.0151e-02,  4.4892e-02, -1.2653e-02, -7.3744e-03, -1.7999e-02,\n",
            "         -3.7654e-03,  9.8204e-03,  3.8329e-02,  4.8402e-02,  3.9666e-02,\n",
            "          7.4977e-03, -4.2178e-02, -6.2735e-03,  3.0045e-02, -8.5618e-04,\n",
            "         -2.5614e-02, -1.5289e-01, -2.5483e-02, -5.1305e-02, -3.5441e-02,\n",
            "          8.2886e-03, -1.2494e-02, -2.5882e-02,  4.8015e-02,  4.7002e-02,\n",
            "          3.2857e-02,  9.5760e-03,  2.2135e-02,  2.5601e-02, -3.1607e-02,\n",
            "         -1.3971e-02, -3.3591e-02, -1.0248e-02,  6.5852e-03, -3.8599e-02,\n",
            "         -4.0884e-03, -1.6250e-02,  1.8601e-02,  2.2299e-02,  3.6564e-02,\n",
            "          1.2098e-01,  4.4655e-02,  3.5768e-02,  1.3384e-02,  3.1240e-02,\n",
            "         -9.9366e-02,  7.9235e-03, -2.8834e-03, -1.0610e-02, -8.7568e-03,\n",
            "          1.2831e-02, -3.0040e-03, -1.0534e-02, -3.9792e-02,  4.9973e-02,\n",
            "          2.5740e-02, -2.0640e-02,  1.7028e-02,  1.8198e-02, -2.7418e-02,\n",
            "          1.5544e-02,  1.4705e-02, -2.4368e-04, -4.4562e-03,  8.1099e-03,\n",
            "         -1.2284e-01,  8.4071e-03,  7.8250e-03,  4.6533e-02, -2.4982e-02,\n",
            "          1.5580e-02, -5.5837e-03,  1.8735e-02, -2.1007e-03,  3.5706e-02,\n",
            "          3.4072e-02],\n",
            "        [-1.9714e-02,  1.7461e-02, -1.1288e-02,  3.5645e-02,  1.1786e-01,\n",
            "         -4.3746e-02, -2.2097e-02,  4.1341e-02,  1.8532e-02, -9.6850e-03,\n",
            "          5.1760e-03, -4.8616e-03, -7.9603e-03,  2.5071e-02,  1.0670e-01,\n",
            "          4.4677e-02,  1.7866e-03, -1.7335e-02, -1.2201e-02, -3.8458e-02,\n",
            "          9.8627e-02,  1.7122e-02, -6.2263e-03, -8.7556e-02, -4.2728e-02,\n",
            "         -4.5519e-02,  2.0217e-03, -6.1784e-02,  2.0282e-02, -9.1533e-03,\n",
            "          8.2430e-02, -3.1144e-02, -2.0049e-02,  8.5222e-03,  3.0721e-02,\n",
            "         -6.1131e-02,  1.3330e-03, -5.5638e-02, -5.3265e-03,  1.9982e-02,\n",
            "          2.5165e-02, -1.0902e-02, -3.0252e-02,  4.1327e-03, -2.9956e-02,\n",
            "         -1.4069e-02, -1.8415e-04,  8.3831e-02,  2.1057e-02,  1.1460e-03,\n",
            "          8.0457e-02, -1.0269e-02, -4.3841e-02, -3.9970e-02,  8.1003e-02,\n",
            "          8.9977e-02, -2.1514e-03, -9.4351e-02, -7.1502e-03,  3.1058e-02,\n",
            "          7.3549e-02, -2.0778e-02, -1.3671e-02,  9.1516e-02,  2.0133e-03,\n",
            "          5.3916e-02,  2.7435e-02, -5.7121e-02, -1.8897e-02, -1.0726e-02,\n",
            "          4.8568e-02,  2.9783e-02,  3.5085e-02,  8.9971e-02,  6.2025e-02,\n",
            "          2.9151e-02, -1.2515e-01,  8.9713e-02,  1.4490e-02, -4.6145e-02,\n",
            "         -4.6664e-02, -3.7255e-02, -6.9333e-02,  8.0746e-02,  6.8140e-05,\n",
            "         -3.7855e-02,  7.4469e-02, -3.3134e-02,  1.2098e-02,  2.2381e-02,\n",
            "         -2.3095e-02,  1.8224e-02, -4.0364e-02, -1.2317e-02, -1.6594e-02,\n",
            "         -5.4613e-02, -6.5189e-02, -2.6296e-02, -6.2833e-02, -2.6177e-02,\n",
            "         -5.1409e-02, -1.9264e-02,  1.7943e-02, -1.5395e-02,  7.0355e-03,\n",
            "         -2.7276e-03, -3.7961e-02, -2.2266e-02,  3.9886e-02, -1.4012e-03,\n",
            "          6.9736e-03,  8.6557e-03, -8.7267e-02,  3.7941e-02,  7.7094e-02,\n",
            "         -3.6276e-02, -7.9496e-02, -3.4503e-02, -5.6296e-02, -6.1724e-02,\n",
            "         -1.5306e-02, -1.4933e-02, -1.9134e-02, -4.1755e-02,  1.0234e-02,\n",
            "          1.2732e-02,  3.0121e-02,  2.8540e-02, -5.1550e-02,  4.3261e-03,\n",
            "          2.6592e-02,  2.8911e-02,  2.8423e-02, -2.3981e-02,  1.7213e-03,\n",
            "          4.3779e-03, -3.4935e-02,  2.1050e-02,  5.9148e-02,  4.5252e-02,\n",
            "         -2.4785e-02, -7.6730e-02,  9.0349e-02, -4.9398e-02, -1.1881e-02,\n",
            "         -1.1100e-02,  3.1661e-02, -2.6477e-02,  1.2650e-02,  3.4761e-03,\n",
            "         -1.1857e-03,  5.2314e-02,  2.4834e-02,  3.1758e-03, -3.1649e-02,\n",
            "          3.7152e-02,  1.6023e-02, -3.6985e-02,  7.7880e-02, -6.9429e-02,\n",
            "         -4.8596e-02, -1.9936e-02, -1.9507e-02, -6.8749e-02, -3.6690e-02,\n",
            "          3.8740e-02, -6.8965e-02, -8.6820e-03, -5.6211e-02, -1.3457e-02,\n",
            "          2.8700e-02, -3.7818e-02, -1.8178e-02, -2.7344e-02, -1.1002e-02,\n",
            "         -1.1896e-02,  8.8064e-03,  6.8126e-02,  1.8813e-02, -1.6777e-02,\n",
            "         -4.4712e-02, -1.8041e-02, -8.2259e-03, -5.8434e-02, -8.9650e-03,\n",
            "         -9.3864e-03, -6.5149e-03,  1.8194e-02, -3.8054e-02,  3.1986e-02,\n",
            "         -6.4452e-03, -2.3061e-02,  6.3373e-02, -1.0730e-01,  6.7124e-03,\n",
            "          8.3272e-03,  7.2946e-02, -1.5997e-02,  7.2348e-03, -4.0298e-02,\n",
            "          4.8745e-02, -1.9186e-02, -3.6146e-02,  2.7234e-02, -9.3158e-02,\n",
            "         -3.8045e-02, -9.5527e-03, -5.0792e-02, -1.1927e-02, -4.0192e-02,\n",
            "         -6.6385e-02,  6.2538e-02, -2.1322e-02, -6.2444e-02, -8.5359e-02,\n",
            "          3.2207e-02,  8.5471e-03,  3.6615e-02,  3.3618e-02, -1.9931e-03,\n",
            "          2.9946e-02, -2.2478e-03,  1.3361e-02,  8.3865e-02, -3.1613e-02,\n",
            "          3.7554e-02,  3.5726e-02, -3.4149e-04,  4.8446e-02, -3.5693e-03,\n",
            "         -2.3643e-02, -2.5657e-03,  7.6906e-02,  2.4113e-03,  6.3814e-02,\n",
            "         -7.8756e-02,  2.4683e-02, -2.3708e-02, -2.2743e-02,  4.9457e-02,\n",
            "         -8.1900e-02, -2.8501e-02,  6.1249e-02, -1.9498e-03, -1.1420e-02,\n",
            "         -1.9884e-02, -2.3524e-02,  3.1686e-02, -2.1927e-03, -1.0704e-02,\n",
            "         -1.6750e-02,  5.7905e-02, -1.2301e-02, -4.8559e-02, -1.3400e-02,\n",
            "          1.8030e-02],\n",
            "        [-1.3995e-02,  4.8010e-02, -3.6640e-02, -3.7246e-03, -1.8691e-03,\n",
            "         -5.6502e-02, -2.2944e-02, -4.1655e-02, -6.3034e-04,  4.4431e-02,\n",
            "         -6.8900e-03,  1.3943e-03,  1.6422e-02, -3.5298e-02, -2.2543e-02,\n",
            "         -1.9358e-02, -1.6130e-02,  1.2077e-02, -1.8406e-02, -2.0643e-02,\n",
            "          2.0477e-03, -3.3406e-03, -7.6582e-03, -3.3991e-02, -4.0765e-02,\n",
            "          5.1913e-02,  1.8763e-02, -5.3144e-02, -5.1486e-02, -1.8199e-04,\n",
            "         -6.6698e-02,  3.5143e-02, -2.0230e-02, -1.8407e-02, -5.8018e-04,\n",
            "         -3.9992e-02, -2.2520e-03,  5.5133e-02,  1.8494e-02, -4.6644e-02,\n",
            "         -6.7179e-02, -1.8662e-02, -4.5412e-02,  3.4907e-02, -2.3708e-02,\n",
            "         -4.4006e-02, -3.0932e-02,  1.2020e-02,  4.8501e-02, -2.5399e-02,\n",
            "         -1.1297e-02, -3.8294e-02, -4.0229e-02,  5.5648e-02,  8.9216e-03,\n",
            "          2.4725e-02, -5.4102e-02,  7.0195e-02,  2.2406e-02,  1.8749e-02,\n",
            "          2.3537e-02,  1.9051e-02, -2.3767e-02, -3.2808e-02, -1.2218e-02,\n",
            "          3.4031e-02,  1.0776e-02, -7.3752e-04, -5.2726e-03,  6.2094e-02,\n",
            "          1.3812e-02,  6.3497e-03,  1.3798e-02, -1.7357e-02, -5.7892e-02,\n",
            "         -5.8589e-02, -4.1621e-03,  2.0310e-02, -4.7528e-03,  1.6850e-02,\n",
            "         -5.3987e-02,  2.7213e-02,  1.5667e-03, -1.0928e-02,  8.0822e-03,\n",
            "         -1.0653e-02,  3.4632e-02,  1.7040e-02,  2.2083e-02, -5.8908e-03,\n",
            "         -9.7100e-03, -2.7187e-02,  2.1540e-02, -4.2607e-02, -5.4652e-02,\n",
            "          3.7054e-02,  3.1078e-02, -1.1008e-02, -1.5007e-02,  2.0916e-02,\n",
            "         -4.3855e-02, -4.3388e-02, -1.5099e-02, -4.8974e-02, -1.5698e-02,\n",
            "         -3.7923e-02,  2.6187e-02, -3.9867e-02, -1.4063e-03,  1.8849e-03,\n",
            "         -1.8725e-02,  2.5929e-02, -8.3423e-03,  2.9328e-03, -2.4376e-03,\n",
            "         -3.5956e-02,  1.7403e-02,  9.8649e-02, -3.5386e-03, -3.6496e-02,\n",
            "          2.0774e-02,  1.6295e-02,  7.1237e-02,  1.4287e-02, -3.2471e-02,\n",
            "         -2.0399e-02,  7.7256e-03,  4.4346e-02,  1.6914e-03, -7.8003e-03,\n",
            "         -7.4256e-02, -3.0114e-03,  7.3546e-03, -4.0138e-02, -2.0127e-03,\n",
            "          2.1668e-02,  2.0101e-02,  2.2855e-02,  3.2027e-02,  2.7967e-02,\n",
            "          2.8402e-02, -3.0741e-02,  1.6259e-02,  2.3253e-02,  2.0693e-02,\n",
            "         -8.7702e-03,  3.3606e-02,  4.7328e-02, -2.8524e-02,  1.7841e-02,\n",
            "         -2.5545e-03, -1.8155e-02,  5.0373e-02,  2.7870e-02,  2.4428e-02,\n",
            "          1.3175e-02, -3.6518e-03, -3.3954e-02,  5.9519e-03, -8.0844e-02,\n",
            "          4.6619e-02,  1.0373e-02,  1.9302e-02, -4.8335e-02, -8.3452e-03,\n",
            "          5.3832e-02, -4.2322e-02, -1.9027e-02, -6.2525e-02, -3.8153e-02,\n",
            "          3.3018e-02, -2.4715e-02,  3.3823e-02, -2.7989e-03,  3.0284e-02,\n",
            "         -4.0732e-02,  1.0261e-01,  3.3476e-02,  3.0767e-02, -8.1945e-02,\n",
            "         -2.4889e-02,  8.3068e-03, -4.2706e-02,  2.7063e-02,  2.8883e-02,\n",
            "         -2.3909e-02, -1.0495e-02,  9.7629e-03,  6.5202e-02,  1.1316e-02,\n",
            "          1.0693e-02, -3.8897e-02,  2.6926e-02, -2.4405e-02,  1.1216e-02,\n",
            "         -5.0144e-02,  4.4290e-02,  1.7836e-03,  7.8961e-02,  9.7803e-02,\n",
            "         -5.8046e-02, -1.8537e-02, -5.8705e-02, -6.2984e-02, -6.9171e-02,\n",
            "         -1.5312e-02,  3.7943e-02, -2.4651e-02,  4.3192e-02,  6.6739e-03,\n",
            "          2.3925e-02,  7.1813e-03,  6.6856e-02,  1.0860e-02, -2.9539e-02,\n",
            "         -2.6946e-02, -1.8092e-02,  3.4409e-02,  6.4245e-02,  2.0983e-02,\n",
            "          3.6805e-02,  2.3071e-02,  4.3343e-02,  3.0495e-02, -1.1050e-02,\n",
            "         -5.3258e-02, -3.2685e-02, -2.3370e-02, -3.6082e-02,  3.1379e-02,\n",
            "          2.5307e-02,  1.8854e-02,  3.0237e-02,  9.2854e-03,  8.1314e-03,\n",
            "         -4.3588e-02, -1.8457e-02,  2.3472e-02, -3.1960e-02,  3.0219e-02,\n",
            "          5.4184e-02, -3.5257e-03,  5.1730e-02, -8.8405e-03, -1.9926e-03,\n",
            "         -1.3538e-02, -4.5100e-02,  8.3891e-02,  7.5913e-02, -8.0467e-03,\n",
            "         -2.1255e-02,  3.2162e-02,  8.1908e-03,  1.7354e-02,  1.4290e-02,\n",
            "          1.0899e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.3670,  0.4015,  4.7742, -3.8052, -4.1544], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4675, -0.0265,  4.0854, -3.8497, -4.3596], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,    80,  5662,  ...,    12, 27739, 10812],\n",
            "        [50256, 23220, 10096,  ...,   262,   938,  1049],\n",
            "        [50256, 21678,   290,  ..., 43107, 22037,   460],\n",
            "        ...,\n",
            "        [50256, 29006,  1433,  ...,   266,   307, 13841],\n",
            "        [50256, 14264,   531,  ...,   447,   247,   447],\n",
            "        [50256,  3037,   460,  ..., 45401,   274, 12690]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0113,  0.0716, -0.0190,  ..., -0.0021,  0.0356,  0.0342],\n",
            "        [-0.0196,  0.0177, -0.0115,  ..., -0.0488, -0.0137,  0.0183],\n",
            "        [-0.0138,  0.0486, -0.0368,  ...,  0.0172,  0.0139,  0.1093],\n",
            "        [-0.0116,  0.0594, -0.0588,  ..., -0.0165, -0.0321,  0.0274],\n",
            "        [ 0.0591, -0.0228, -0.0315,  ..., -0.0020,  0.0217,  0.0354]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1318e-02,  7.1604e-02, -1.8971e-02, -3.3948e-02, -9.8072e-03,\n",
            "         -4.8448e-03, -1.1442e-02,  3.7828e-03, -6.9622e-02, -1.3073e-02,\n",
            "          4.9206e-03,  1.5349e-02, -3.2934e-02, -2.8689e-02, -4.2743e-02,\n",
            "         -3.2507e-02, -3.4402e-03,  2.0678e-02,  1.5100e-02,  2.1687e-02,\n",
            "         -2.8886e-02,  4.4007e-02, -1.1938e-02, -3.2918e-02, -1.3635e-02,\n",
            "          2.6941e-02, -3.0069e-02,  3.5552e-02,  7.1939e-03, -1.0070e-02,\n",
            "         -7.3874e-02,  2.3335e-02,  1.4705e-02,  1.6592e-02, -3.3244e-02,\n",
            "         -8.5406e-03,  6.2861e-02,  3.1950e-02,  3.2667e-02,  1.7478e-03,\n",
            "         -4.1430e-02, -3.1110e-03,  3.0021e-02, -1.4393e-02,  1.1513e-02,\n",
            "          9.0734e-03,  2.8344e-02, -7.5806e-02, -3.8596e-02, -6.9568e-02,\n",
            "         -2.3395e-02,  1.4682e-02, -2.2077e-02,  4.7260e-02,  2.7863e-02,\n",
            "          2.8810e-03, -2.3593e-02, -1.6789e-02,  4.4202e-02,  2.0916e-02,\n",
            "         -3.9442e-03,  4.0488e-04, -4.8483e-03, -1.7190e-02, -3.5284e-02,\n",
            "          2.0623e-02,  5.0438e-02, -1.7521e-02,  2.3630e-02,  4.3095e-02,\n",
            "          2.4001e-02, -1.7701e-02, -1.3677e-02,  2.9509e-02, -4.5082e-02,\n",
            "         -3.6633e-02, -1.7784e-02,  7.1174e-02,  6.2840e-02,  2.4711e-02,\n",
            "         -1.3101e-02, -2.6913e-02,  3.3904e-02, -3.4768e-02, -1.0745e-02,\n",
            "          9.9339e-02,  5.9525e-03,  3.3201e-02, -1.4417e-02,  3.2744e-03,\n",
            "          3.3110e-03, -5.5849e-02,  2.2024e-02,  2.6296e-04, -2.1618e-02,\n",
            "         -1.3663e-02, -7.3699e-02, -4.4510e-02,  1.5615e-02, -1.5125e-02,\n",
            "         -4.5486e-02,  4.5834e-03,  9.9961e-03,  1.6794e-02, -1.3101e-02,\n",
            "         -7.9326e-03,  5.2467e-02, -2.5810e-02, -5.0454e-02,  4.6854e-02,\n",
            "         -3.5162e-02, -1.7275e-02, -5.6972e-03, -6.3932e-02, -9.1228e-03,\n",
            "         -9.9996e-03, -4.0436e-03,  3.5922e-02,  4.0725e-02,  6.8416e-02,\n",
            "          1.8101e-02, -6.9404e-03, -3.4325e-02,  1.1087e-01,  3.7896e-02,\n",
            "         -7.2345e-03, -3.0634e-02, -9.4708e-03,  2.3702e-02, -7.7303e-03,\n",
            "          6.2983e-02,  1.0122e-02, -3.9342e-03,  2.3227e-02, -3.9598e-02,\n",
            "          3.8089e-04, -4.7329e-02,  4.1502e-02, -1.0758e-02,  8.1946e-03,\n",
            "         -1.7384e-02, -1.3485e-02,  3.8080e-02, -4.4893e-02, -2.4818e-02,\n",
            "          3.6747e-02, -2.3606e-02, -3.2534e-02,  3.9171e-02,  4.0470e-04,\n",
            "         -1.2993e-02,  5.9630e-02,  3.9843e-03,  7.3562e-03,  8.1879e-03,\n",
            "         -1.8880e-02,  1.3026e-02, -9.6690e-03, -6.5884e-03,  2.7612e-02,\n",
            "         -7.1637e-03, -3.1128e-02, -2.6062e-02, -5.6583e-02, -1.1000e-02,\n",
            "          5.9465e-02, -8.8103e-03, -5.0260e-02, -2.0934e-02, -3.7409e-02,\n",
            "         -2.3724e-03, -9.4549e-03, -4.3826e-03, -2.4553e-02, -3.5038e-02,\n",
            "         -2.2488e-02,  4.4883e-03, -1.2942e-02, -1.5325e-02, -8.7073e-02,\n",
            "          3.0327e-02,  4.4843e-02, -1.2686e-02, -7.5153e-03, -1.8247e-02,\n",
            "         -3.5585e-03,  1.0022e-02,  3.8328e-02,  4.8470e-02,  3.9554e-02,\n",
            "          7.4442e-03, -4.1980e-02, -6.1332e-03,  3.0023e-02, -1.0300e-03,\n",
            "         -2.5576e-02, -1.5305e-01, -2.5724e-02, -5.1262e-02, -3.5405e-02,\n",
            "          8.3366e-03, -1.2669e-02, -2.5681e-02,  4.8152e-02,  4.7239e-02,\n",
            "          3.2863e-02,  9.8112e-03,  2.2305e-02,  2.5700e-02, -3.1429e-02,\n",
            "         -1.4141e-02, -3.3780e-02, -1.0501e-02,  6.6563e-03, -3.8660e-02,\n",
            "         -4.0979e-03, -1.6207e-02,  1.8706e-02,  2.2215e-02,  3.6584e-02,\n",
            "          1.2094e-01,  4.4376e-02,  3.5671e-02,  1.3137e-02,  3.1422e-02,\n",
            "         -9.9144e-02,  7.9957e-03, -2.8934e-03, -1.0583e-02, -8.7455e-03,\n",
            "          1.2956e-02, -3.0767e-03, -1.0368e-02, -3.9915e-02,  4.9814e-02,\n",
            "          2.5952e-02, -2.0595e-02,  1.6972e-02,  1.8120e-02, -2.7254e-02,\n",
            "          1.5766e-02,  1.4640e-02, -2.4734e-04, -4.1882e-03,  7.9107e-03,\n",
            "         -1.2310e-01,  8.4902e-03,  7.8714e-03,  4.6617e-02, -2.5112e-02,\n",
            "          1.5339e-02, -5.7582e-03,  1.8830e-02, -2.1255e-03,  3.5580e-02,\n",
            "          3.4166e-02],\n",
            "        [-1.9558e-02,  1.7726e-02, -1.1502e-02,  3.6049e-02,  1.1778e-01,\n",
            "         -4.3760e-02, -2.2307e-02,  4.1454e-02,  1.8587e-02, -9.5751e-03,\n",
            "          5.0692e-03, -4.9742e-03, -8.6271e-03,  2.4763e-02,  1.0716e-01,\n",
            "          4.4958e-02,  1.6263e-03, -1.7413e-02, -1.2683e-02, -3.8391e-02,\n",
            "          9.8558e-02,  1.6830e-02, -5.8856e-03, -8.7664e-02, -4.2738e-02,\n",
            "         -4.5320e-02,  2.2477e-03, -6.1972e-02,  2.0090e-02, -9.4819e-03,\n",
            "          8.2838e-02, -3.0722e-02, -1.9463e-02,  8.5336e-03,  3.0816e-02,\n",
            "         -6.0775e-02,  1.2620e-03, -5.5752e-02, -5.3632e-03,  1.9732e-02,\n",
            "          2.5309e-02, -1.1052e-02, -2.9837e-02,  3.7500e-03, -3.0030e-02,\n",
            "         -1.4625e-02, -1.0488e-04,  8.3905e-02,  2.1155e-02,  1.1684e-03,\n",
            "          8.0531e-02, -1.0129e-02, -4.3913e-02, -4.0215e-02,  8.1299e-02,\n",
            "          9.0418e-02, -2.1490e-03, -9.4329e-02, -7.2147e-03,  3.1138e-02,\n",
            "          7.3662e-02, -2.0696e-02, -1.3542e-02,  9.1559e-02,  1.9617e-03,\n",
            "          5.3983e-02,  2.7458e-02, -5.7106e-02, -1.8562e-02, -1.0316e-02,\n",
            "          4.8631e-02,  3.0095e-02,  3.4771e-02,  9.0124e-02,  6.2068e-02,\n",
            "          2.9218e-02, -1.2524e-01,  8.9698e-02,  1.4431e-02, -4.6344e-02,\n",
            "         -4.6743e-02, -3.7184e-02, -6.9258e-02,  8.1080e-02,  3.8677e-04,\n",
            "         -3.7979e-02,  7.4417e-02, -3.2888e-02,  1.2272e-02,  2.2784e-02,\n",
            "         -2.3225e-02,  1.7985e-02, -4.0531e-02, -1.2550e-02, -1.6900e-02,\n",
            "         -5.4418e-02, -6.5556e-02, -2.5835e-02, -6.2739e-02, -2.6584e-02,\n",
            "         -5.1732e-02, -1.9195e-02,  1.7941e-02, -1.5349e-02,  7.2995e-03,\n",
            "         -2.2929e-03, -3.8364e-02, -2.2321e-02,  4.0303e-02, -1.8399e-03,\n",
            "          7.1667e-03,  8.7813e-03, -8.7001e-02,  3.7787e-02,  7.7256e-02,\n",
            "         -3.6642e-02, -7.9633e-02, -3.4329e-02, -5.6077e-02, -6.1766e-02,\n",
            "         -1.5437e-02, -1.5143e-02, -1.9027e-02, -4.2238e-02,  1.0124e-02,\n",
            "          1.2816e-02,  3.0650e-02,  2.8694e-02, -5.1404e-02,  4.4159e-03,\n",
            "          2.6658e-02,  2.9211e-02,  2.8260e-02, -2.4191e-02,  1.2245e-03,\n",
            "          4.4379e-03, -3.4653e-02,  2.0872e-02,  5.9156e-02,  4.5490e-02,\n",
            "         -2.4635e-02, -7.6789e-02,  9.0544e-02, -4.9604e-02, -1.1831e-02,\n",
            "         -1.0714e-02,  3.1775e-02, -2.6751e-02,  1.2887e-02,  3.5929e-03,\n",
            "         -1.2969e-03,  5.2159e-02,  2.4996e-02,  3.3548e-03, -3.1270e-02,\n",
            "          3.6989e-02,  1.5955e-02, -3.7151e-02,  7.7871e-02, -6.9545e-02,\n",
            "         -4.8684e-02, -1.9938e-02, -1.9343e-02, -6.8843e-02, -3.6763e-02,\n",
            "          3.8551e-02, -6.9485e-02, -9.0250e-03, -5.6520e-02, -1.3003e-02,\n",
            "          2.8377e-02, -3.7707e-02, -1.7957e-02, -2.7147e-02, -1.0943e-02,\n",
            "         -1.1793e-02,  8.8092e-03,  6.8420e-02,  1.9029e-02, -1.6542e-02,\n",
            "         -4.4594e-02, -1.8428e-02, -8.2794e-03, -5.8337e-02, -9.2358e-03,\n",
            "         -9.2119e-03, -6.5242e-03,  1.8210e-02, -3.7818e-02,  3.2142e-02,\n",
            "         -6.7252e-03, -2.2876e-02,  6.4044e-02, -1.0698e-01,  6.4792e-03,\n",
            "          8.4697e-03,  7.3401e-02, -1.6156e-02,  7.1952e-03, -4.0001e-02,\n",
            "          4.8350e-02, -1.9496e-02, -3.6102e-02,  2.6939e-02, -9.3200e-02,\n",
            "         -3.8464e-02, -9.0181e-03, -5.1533e-02, -1.1582e-02, -4.0604e-02,\n",
            "         -6.6494e-02,  6.2306e-02, -2.1455e-02, -6.2499e-02, -8.5964e-02,\n",
            "          3.2028e-02,  8.7036e-03,  3.6602e-02,  3.3529e-02, -1.7828e-03,\n",
            "          2.9913e-02, -2.3715e-03,  1.3040e-02,  8.3847e-02, -3.1133e-02,\n",
            "          3.7470e-02,  3.5405e-02, -2.4528e-04,  4.8377e-02, -3.3590e-03,\n",
            "         -2.3286e-02, -2.6998e-03,  7.7224e-02,  2.2032e-03,  6.3458e-02,\n",
            "         -7.8941e-02,  2.4872e-02, -2.3484e-02, -2.2726e-02,  4.9743e-02,\n",
            "         -8.2151e-02, -2.8860e-02,  6.0939e-02, -1.9550e-03, -1.1645e-02,\n",
            "         -2.0104e-02, -2.3683e-02,  3.2000e-02, -2.2812e-03, -1.0872e-02,\n",
            "         -1.7042e-02,  5.7732e-02, -1.2279e-02, -4.8775e-02, -1.3707e-02,\n",
            "          1.8299e-02],\n",
            "        [-1.3761e-02,  4.8574e-02, -3.6849e-02, -3.5142e-03, -2.2782e-03,\n",
            "         -5.6209e-02, -2.2938e-02, -4.1436e-02, -6.1770e-04,  4.4570e-02,\n",
            "         -7.0936e-03,  1.2291e-03,  1.6064e-02, -3.5718e-02, -2.2203e-02,\n",
            "         -1.9169e-02, -1.6110e-02,  1.1806e-02, -1.8804e-02, -2.0771e-02,\n",
            "          1.7926e-03, -3.6141e-03, -7.3898e-03, -3.3973e-02, -4.0754e-02,\n",
            "          5.2274e-02,  1.8905e-02, -5.3068e-02, -5.1833e-02, -4.8580e-04,\n",
            "         -6.6745e-02,  3.5708e-02, -1.9841e-02, -1.8385e-02, -3.0338e-04,\n",
            "         -3.9805e-02, -2.2017e-03,  5.4986e-02,  1.8312e-02, -4.6803e-02,\n",
            "         -6.7324e-02, -1.8686e-02, -4.5247e-02,  3.4720e-02, -2.3879e-02,\n",
            "         -4.4296e-02, -3.0811e-02,  1.1805e-02,  4.8541e-02, -2.5515e-02,\n",
            "         -1.1152e-02, -3.8118e-02, -4.0358e-02,  5.5676e-02,  8.9375e-03,\n",
            "          2.4829e-02, -5.4037e-02,  7.0076e-02,  2.2164e-02,  1.8932e-02,\n",
            "          2.3462e-02,  1.9231e-02, -2.3636e-02, -3.3090e-02, -1.2232e-02,\n",
            "          3.4025e-02,  1.0611e-02, -2.6007e-04, -4.9769e-03,  6.2266e-02,\n",
            "          1.3768e-02,  6.6878e-03,  1.3468e-02, -1.7338e-02, -5.7977e-02,\n",
            "         -5.8789e-02, -3.9731e-03,  2.0121e-02, -5.0771e-03,  1.6617e-02,\n",
            "         -5.3979e-02,  2.7080e-02,  1.5031e-03, -1.0830e-02,  7.9151e-03,\n",
            "         -1.0759e-02,  3.4659e-02,  1.7142e-02,  2.2243e-02, -5.4979e-03,\n",
            "         -9.6382e-03, -2.7373e-02,  2.1233e-02, -4.2813e-02, -5.4701e-02,\n",
            "          3.7337e-02,  3.0830e-02, -1.0451e-02, -1.4876e-02,  2.0579e-02,\n",
            "         -4.4141e-02, -4.3270e-02, -1.5196e-02, -4.8833e-02, -1.5560e-02,\n",
            "         -3.7575e-02,  2.5952e-02, -4.0110e-02, -1.0652e-03,  1.4615e-03,\n",
            "         -1.8689e-02,  2.5884e-02, -7.8571e-03,  2.6327e-03, -2.3283e-03,\n",
            "         -3.6532e-02,  1.7339e-02,  9.8733e-02, -3.4488e-03, -3.6328e-02,\n",
            "          2.0642e-02,  1.6272e-02,  7.1468e-02,  1.4110e-02, -3.2584e-02,\n",
            "         -2.0566e-02,  8.1434e-03,  4.4415e-02,  1.7594e-03, -7.8897e-03,\n",
            "         -7.4414e-02, -2.6661e-03,  7.2001e-03, -4.0196e-02, -2.1375e-03,\n",
            "          2.1895e-02,  2.0253e-02,  2.2687e-02,  3.1903e-02,  2.7839e-02,\n",
            "          2.8524e-02, -3.0483e-02,  1.6395e-02,  2.3046e-02,  2.0656e-02,\n",
            "         -8.6286e-03,  3.3714e-02,  4.7250e-02, -2.8593e-02,  1.7939e-02,\n",
            "         -2.6831e-03, -1.8172e-02,  5.0560e-02,  2.7866e-02,  2.4663e-02,\n",
            "          1.3016e-02, -3.5869e-03, -3.4092e-02,  5.8177e-03, -8.1011e-02,\n",
            "          4.6634e-02,  1.0300e-02,  1.9380e-02, -4.8467e-02, -8.2599e-03,\n",
            "          5.3627e-02, -4.2769e-02, -1.9264e-02, -6.2863e-02, -3.7795e-02,\n",
            "          3.2769e-02, -2.4698e-02,  3.4017e-02, -2.7015e-03,  3.0768e-02,\n",
            "         -4.0670e-02,  1.0254e-01,  3.3678e-02,  3.0931e-02, -8.1888e-02,\n",
            "         -2.4684e-02,  8.2983e-03, -4.2419e-02,  2.7155e-02,  2.8769e-02,\n",
            "         -2.3654e-02, -1.0293e-02,  9.7776e-03,  6.5496e-02,  1.1308e-02,\n",
            "          1.0066e-02, -3.8760e-02,  2.7448e-02, -2.4182e-02,  1.1320e-02,\n",
            "         -5.0301e-02,  4.4383e-02,  1.7789e-03,  7.8627e-02,  9.7760e-02,\n",
            "         -5.8291e-02, -1.8769e-02, -5.8679e-02, -6.2925e-02, -6.8887e-02,\n",
            "         -1.5848e-02,  3.8600e-02, -2.4890e-02,  4.3554e-02,  6.4341e-03,\n",
            "          2.3931e-02,  7.0337e-03,  6.6921e-02,  1.0895e-02, -2.9713e-02,\n",
            "         -2.7530e-02, -1.8081e-02,  3.4411e-02,  6.4288e-02,  2.1243e-02,\n",
            "          3.6834e-02,  2.2999e-02,  4.3203e-02,  3.0609e-02, -1.0461e-02,\n",
            "         -5.3373e-02, -3.3001e-02, -2.3019e-02, -3.6405e-02,  3.1632e-02,\n",
            "          2.5713e-02,  1.8605e-02,  3.0419e-02,  9.2165e-03,  8.1601e-03,\n",
            "         -4.3586e-02, -1.8445e-02,  2.3572e-02, -3.2058e-02,  3.0219e-02,\n",
            "          5.4241e-02, -3.5911e-03,  5.1671e-02, -8.9381e-03, -2.0851e-03,\n",
            "         -1.3761e-02, -4.5262e-02,  8.4346e-02,  7.5798e-02, -8.0237e-03,\n",
            "         -2.1362e-02,  3.1726e-02,  8.1649e-03,  1.7247e-02,  1.3916e-02,\n",
            "          1.0932e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4666,  0.4647,  4.7378, -3.9085, -4.0539], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4909,  0.1253,  4.1003, -4.0100, -4.2227], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,   220,   220,  ...,   220,   220,   220],\n",
            "        [50256,   262,  2576,  ...,   338,  3443,  1282],\n",
            "        [50256,   286, 10900,  ...,  7925,  3467, 30109],\n",
            "        ...,\n",
            "        [50256,  3265,   286,  ..., 11795,   198,  3880],\n",
            "        [50256,   416,   262,  ..., 32681,  6317,  1022],\n",
            "        [50256,   117, 43095,  ..., 11223,   304,   268]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0111,  0.0715, -0.0191,  ..., -0.0020,  0.0355,  0.0341],\n",
            "        [-0.0194,  0.0178, -0.0117,  ..., -0.0489, -0.0139,  0.0185],\n",
            "        [-0.0135,  0.0491, -0.0371,  ...,  0.0170,  0.0135,  0.1096],\n",
            "        [-0.0115,  0.0596, -0.0589,  ..., -0.0165, -0.0320,  0.0274],\n",
            "        [ 0.0592, -0.0230, -0.0312,  ..., -0.0020,  0.0219,  0.0354]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1079e-02,  7.1541e-02, -1.9105e-02, -3.3950e-02, -9.5212e-03,\n",
            "         -4.8570e-03, -1.1489e-02,  3.6739e-03, -6.9900e-02, -1.3005e-02,\n",
            "          4.9571e-03,  1.5078e-02, -3.3059e-02, -2.8767e-02, -4.2528e-02,\n",
            "         -3.2559e-02, -3.6698e-03,  2.0603e-02,  1.5159e-02,  2.1751e-02,\n",
            "         -2.9151e-02,  4.3869e-02, -1.2121e-02, -3.2595e-02, -1.3812e-02,\n",
            "          2.6780e-02, -2.9842e-02,  3.5741e-02,  7.2841e-03, -1.0066e-02,\n",
            "         -7.3741e-02,  2.3227e-02,  1.4608e-02,  1.6962e-02, -3.3501e-02,\n",
            "         -8.3839e-03,  6.3110e-02,  3.2041e-02,  3.2604e-02,  1.9969e-03,\n",
            "         -4.1070e-02, -3.2586e-03,  3.0000e-02, -1.4474e-02,  1.1769e-02,\n",
            "          8.9075e-03,  2.8293e-02, -7.5704e-02, -3.8843e-02, -6.9614e-02,\n",
            "         -2.3519e-02,  1.4510e-02, -2.2079e-02,  4.7352e-02,  2.8060e-02,\n",
            "          2.8045e-03, -2.3443e-02, -1.6605e-02,  4.4485e-02,  2.1060e-02,\n",
            "         -3.9188e-03,  3.7723e-04, -4.7227e-03, -1.6881e-02, -3.5076e-02,\n",
            "          2.0530e-02,  5.0329e-02, -1.7360e-02,  2.3651e-02,  4.3233e-02,\n",
            "          2.3999e-02, -1.7637e-02, -1.3771e-02,  2.9444e-02, -4.4829e-02,\n",
            "         -3.6908e-02, -1.7686e-02,  7.1242e-02,  6.3139e-02,  2.4920e-02,\n",
            "         -1.3168e-02, -2.6757e-02,  3.3687e-02, -3.4771e-02, -1.0934e-02,\n",
            "          9.9554e-02,  5.7479e-03,  3.3118e-02, -1.4273e-02,  3.3772e-03,\n",
            "          3.2744e-03, -5.5742e-02,  2.2048e-02,  5.1832e-04, -2.1690e-02,\n",
            "         -1.3807e-02, -7.3873e-02, -4.4240e-02,  1.5719e-02, -1.5268e-02,\n",
            "         -4.5647e-02,  4.4294e-03,  9.7207e-03,  1.6609e-02, -1.2883e-02,\n",
            "         -7.7291e-03,  5.2451e-02, -2.5786e-02, -5.0405e-02,  4.6693e-02,\n",
            "         -3.5159e-02, -1.7529e-02, -5.8378e-03, -6.3960e-02, -9.4559e-03,\n",
            "         -9.7993e-03, -3.9668e-03,  3.5932e-02,  4.0581e-02,  6.8525e-02,\n",
            "          1.8241e-02, -6.9706e-03, -3.4001e-02,  1.1087e-01,  3.8117e-02,\n",
            "         -7.2444e-03, -3.0820e-02, -9.7292e-03,  2.3672e-02, -7.5491e-03,\n",
            "          6.2888e-02,  1.0033e-02, -4.1476e-03,  2.3617e-02, -3.9706e-02,\n",
            "          3.8200e-04, -4.7316e-02,  4.1627e-02, -1.0808e-02,  8.4709e-03,\n",
            "         -1.7713e-02, -1.3623e-02,  3.7864e-02, -4.4602e-02, -2.4769e-02,\n",
            "          3.6768e-02, -2.3746e-02, -3.2828e-02,  3.9346e-02,  5.6625e-04,\n",
            "         -1.3257e-02,  5.9598e-02,  4.0929e-03,  7.3087e-03,  8.1654e-03,\n",
            "         -1.8848e-02,  1.3370e-02, -9.7489e-03, -6.5138e-03,  2.7496e-02,\n",
            "         -7.1533e-03, -3.1160e-02, -2.6141e-02, -5.6563e-02, -1.0813e-02,\n",
            "          5.9859e-02, -8.8387e-03, -5.0309e-02, -2.1069e-02, -3.7410e-02,\n",
            "         -2.5223e-03, -9.7730e-03, -4.1847e-03, -2.4638e-02, -3.5267e-02,\n",
            "         -2.2486e-02,  4.3881e-03, -1.3249e-02, -1.5576e-02, -8.7148e-02,\n",
            "          3.0377e-02,  4.4787e-02, -1.2593e-02, -7.7903e-03, -1.8498e-02,\n",
            "         -3.3795e-03,  1.0187e-02,  3.8261e-02,  4.8478e-02,  3.9453e-02,\n",
            "          7.5559e-03, -4.1733e-02, -6.1365e-03,  3.0047e-02, -1.2018e-03,\n",
            "         -2.5598e-02, -1.5328e-01, -2.6000e-02, -5.1250e-02, -3.5332e-02,\n",
            "          8.4922e-03, -1.2758e-02, -2.5472e-02,  4.8327e-02,  4.7445e-02,\n",
            "          3.2889e-02,  9.9457e-03,  2.2505e-02,  2.5800e-02, -3.1146e-02,\n",
            "         -1.4435e-02, -3.3959e-02, -1.0824e-02,  6.6162e-03, -3.8710e-02,\n",
            "         -4.0977e-03, -1.6185e-02,  1.8786e-02,  2.2009e-02,  3.6565e-02,\n",
            "          1.2087e-01,  4.4089e-02,  3.5619e-02,  1.2939e-02,  3.1526e-02,\n",
            "         -9.8897e-02,  8.0675e-03, -2.9062e-03, -1.0530e-02, -8.8518e-03,\n",
            "          1.3163e-02, -3.0792e-03, -1.0108e-02, -3.9941e-02,  4.9710e-02,\n",
            "          2.6190e-02, -2.0526e-02,  1.6922e-02,  1.7994e-02, -2.7041e-02,\n",
            "          1.5931e-02,  1.4562e-02, -2.2061e-04, -3.8321e-03,  7.6826e-03,\n",
            "         -1.2330e-01,  8.6292e-03,  7.7596e-03,  4.6777e-02, -2.5263e-02,\n",
            "          1.5133e-02, -5.9091e-03,  1.8921e-02, -2.0278e-03,  3.5503e-02,\n",
            "          3.4132e-02],\n",
            "        [-1.9360e-02,  1.7811e-02, -1.1745e-02,  3.6370e-02,  1.1759e-01,\n",
            "         -4.3759e-02, -2.2549e-02,  4.1565e-02,  1.8698e-02, -9.4289e-03,\n",
            "          4.9295e-03, -5.0869e-03, -9.2541e-03,  2.4448e-02,  1.0758e-01,\n",
            "          4.5120e-02,  1.5157e-03, -1.7458e-02, -1.3021e-02, -3.8376e-02,\n",
            "          9.8431e-02,  1.6597e-02, -5.6282e-03, -8.7660e-02, -4.2824e-02,\n",
            "         -4.5131e-02,  2.4498e-03, -6.2010e-02,  1.9955e-02, -9.7726e-03,\n",
            "          8.3215e-02, -3.0531e-02, -1.9025e-02,  8.5073e-03,  3.1003e-02,\n",
            "         -6.0488e-02,  1.1978e-03, -5.5878e-02, -5.3159e-03,  1.9561e-02,\n",
            "          2.5565e-02, -1.1256e-02, -2.9542e-02,  3.4300e-03, -3.0182e-02,\n",
            "         -1.5105e-02, -1.6379e-04,  8.4048e-02,  2.1219e-02,  1.2308e-03,\n",
            "          8.0582e-02, -9.9771e-03, -4.3971e-02, -4.0409e-02,  8.1564e-02,\n",
            "          9.0714e-02, -2.0808e-03, -9.4419e-02, -7.2754e-03,  3.1203e-02,\n",
            "          7.3682e-02, -2.0528e-02, -1.3336e-02,  9.1632e-02,  1.8879e-03,\n",
            "          5.3967e-02,  2.7596e-02, -5.7075e-02, -1.8257e-02, -9.9197e-03,\n",
            "          4.8631e-02,  3.0414e-02,  3.4509e-02,  9.0214e-02,  6.2073e-02,\n",
            "          2.9273e-02, -1.2527e-01,  8.9644e-02,  1.4352e-02, -4.6464e-02,\n",
            "         -4.6857e-02, -3.7153e-02, -6.9208e-02,  8.1356e-02,  5.7902e-04,\n",
            "         -3.8189e-02,  7.4387e-02, -3.2677e-02,  1.2426e-02,  2.3107e-02,\n",
            "         -2.3297e-02,  1.7825e-02, -4.0611e-02, -1.2716e-02, -1.7205e-02,\n",
            "         -5.4217e-02, -6.5792e-02, -2.5369e-02, -6.2728e-02, -2.7003e-02,\n",
            "         -5.1998e-02, -1.9115e-02,  1.7837e-02, -1.5425e-02,  7.5540e-03,\n",
            "         -1.8683e-03, -3.8751e-02, -2.2375e-02,  4.0616e-02, -2.3755e-03,\n",
            "          7.3360e-03,  8.8917e-03, -8.6820e-02,  3.7655e-02,  7.7250e-02,\n",
            "         -3.6961e-02, -7.9701e-02, -3.4203e-02, -5.5956e-02, -6.1847e-02,\n",
            "         -1.5447e-02, -1.5313e-02, -1.8841e-02, -4.2735e-02,  9.9130e-03,\n",
            "          1.2898e-02,  3.1194e-02,  2.8646e-02, -5.1213e-02,  4.5327e-03,\n",
            "          2.6672e-02,  2.9449e-02,  2.8169e-02, -2.4319e-02,  8.7147e-04,\n",
            "          4.4189e-03, -3.4376e-02,  2.0659e-02,  5.9139e-02,  4.5808e-02,\n",
            "         -2.4392e-02, -7.6782e-02,  9.0638e-02, -4.9748e-02, -1.1756e-02,\n",
            "         -1.0459e-02,  3.1819e-02, -2.7021e-02,  1.3089e-02,  3.6884e-03,\n",
            "         -1.3308e-03,  5.2019e-02,  2.5134e-02,  3.4550e-03, -3.1019e-02,\n",
            "          3.6817e-02,  1.5959e-02, -3.7168e-02,  7.7816e-02, -6.9531e-02,\n",
            "         -4.8823e-02, -1.9934e-02, -1.9128e-02, -6.8792e-02, -3.6743e-02,\n",
            "          3.8359e-02, -6.9826e-02, -9.3162e-03, -5.6722e-02, -1.2526e-02,\n",
            "          2.8075e-02, -3.7572e-02, -1.7696e-02, -2.6985e-02, -1.0872e-02,\n",
            "         -1.1825e-02,  8.7746e-03,  6.8547e-02,  1.9170e-02, -1.6248e-02,\n",
            "         -4.4379e-02, -1.8821e-02, -8.3198e-03, -5.8212e-02, -9.5046e-03,\n",
            "         -9.0005e-03, -6.4634e-03,  1.8173e-02, -3.7660e-02,  3.2157e-02,\n",
            "         -7.0503e-03, -2.2675e-02,  6.4538e-02, -1.0678e-01,  6.2866e-03,\n",
            "          8.6163e-03,  7.3812e-02, -1.6389e-02,  7.2045e-03, -3.9775e-02,\n",
            "          4.8123e-02, -1.9853e-02, -3.5984e-02,  2.6683e-02, -9.3181e-02,\n",
            "         -3.8935e-02, -8.3947e-03, -5.2140e-02, -1.1289e-02, -4.0936e-02,\n",
            "         -6.6548e-02,  6.2051e-02, -2.1598e-02, -6.2530e-02, -8.6498e-02,\n",
            "          3.1883e-02,  8.8042e-03,  3.6615e-02,  3.3441e-02, -1.6087e-03,\n",
            "          2.9790e-02, -2.5203e-03,  1.2763e-02,  8.3736e-02, -3.0756e-02,\n",
            "          3.7454e-02,  3.5165e-02, -1.3869e-04,  4.8382e-02, -3.1583e-03,\n",
            "         -2.2963e-02, -2.8301e-03,  7.7545e-02,  1.9027e-03,  6.3042e-02,\n",
            "         -7.8946e-02,  2.5038e-02, -2.3280e-02, -2.2821e-02,  5.0007e-02,\n",
            "         -8.2355e-02, -2.9089e-02,  6.0588e-02, -2.0091e-03, -1.1807e-02,\n",
            "         -2.0221e-02, -2.3742e-02,  3.2247e-02, -2.2670e-03, -1.1042e-02,\n",
            "         -1.7289e-02,  5.7618e-02, -1.2195e-02, -4.8931e-02, -1.3943e-02,\n",
            "          1.8461e-02],\n",
            "        [-1.3469e-02,  4.9055e-02, -3.7059e-02, -3.2529e-03, -2.6852e-03,\n",
            "         -5.5847e-02, -2.2995e-02, -4.1216e-02, -6.4381e-04,  4.4728e-02,\n",
            "         -7.3506e-03,  1.1029e-03,  1.5553e-02, -3.6144e-02, -2.1786e-02,\n",
            "         -1.8865e-02, -1.6139e-02,  1.1590e-02, -1.9239e-02, -2.0915e-02,\n",
            "          1.4466e-03, -3.8180e-03, -6.9898e-03, -3.3856e-02, -4.0822e-02,\n",
            "          5.2634e-02,  1.9024e-02, -5.2888e-02, -5.2119e-02, -8.6217e-04,\n",
            "         -6.6672e-02,  3.6204e-02, -1.9516e-02, -1.8366e-02,  1.8773e-05,\n",
            "         -3.9579e-02, -2.2227e-03,  5.4842e-02,  1.8138e-02, -4.7030e-02,\n",
            "         -6.7283e-02, -1.8619e-02, -4.5044e-02,  3.4400e-02, -2.3921e-02,\n",
            "         -4.4691e-02, -3.0578e-02,  1.1724e-02,  4.8497e-02, -2.5615e-02,\n",
            "         -1.1009e-02, -3.7946e-02, -4.0426e-02,  5.5665e-02,  8.9578e-03,\n",
            "          2.4928e-02, -5.4011e-02,  6.9936e-02,  2.1910e-02,  1.9170e-02,\n",
            "          2.3382e-02,  1.9441e-02, -2.3382e-02, -3.3302e-02, -1.2225e-02,\n",
            "          3.3920e-02,  1.0475e-02,  2.9643e-04, -4.6011e-03,  6.2527e-02,\n",
            "          1.3586e-02,  7.1017e-03,  1.3130e-02, -1.7387e-02, -5.8055e-02,\n",
            "         -5.8878e-02, -3.8281e-03,  1.9860e-02, -5.3798e-03,  1.6392e-02,\n",
            "         -5.3975e-02,  2.6865e-02,  1.4565e-03, -1.0667e-02,  7.8378e-03,\n",
            "         -1.0916e-02,  3.4641e-02,  1.7279e-02,  2.2325e-02, -5.0121e-03,\n",
            "         -9.5885e-03, -2.7596e-02,  2.0930e-02, -4.2944e-02, -5.4885e-02,\n",
            "          3.7554e-02,  3.0432e-02, -9.8291e-03, -1.4742e-02,  2.0096e-02,\n",
            "         -4.4446e-02, -4.3151e-02, -1.5240e-02, -4.8669e-02, -1.5322e-02,\n",
            "         -3.7030e-02,  2.5570e-02, -4.0215e-02, -6.2975e-04,  9.1696e-04,\n",
            "         -1.8644e-02,  2.5876e-02, -7.3657e-03,  2.2839e-03, -2.1906e-03,\n",
            "         -3.7122e-02,  1.7277e-02,  9.8785e-02, -3.3108e-03, -3.6245e-02,\n",
            "          2.0444e-02,  1.6178e-02,  7.1656e-02,  1.3788e-02, -3.2645e-02,\n",
            "         -2.0720e-02,  8.6564e-03,  4.4442e-02,  1.9694e-03, -7.7829e-03,\n",
            "         -7.4566e-02, -2.2313e-03,  7.0001e-03, -4.0241e-02, -2.3567e-03,\n",
            "          2.2089e-02,  2.0560e-02,  2.2479e-02,  3.1729e-02,  2.7739e-02,\n",
            "          2.8686e-02, -3.0169e-02,  1.6598e-02,  2.2835e-02,  2.0633e-02,\n",
            "         -8.3987e-03,  3.3757e-02,  4.7074e-02, -2.8589e-02,  1.8146e-02,\n",
            "         -2.8792e-03, -1.8280e-02,  5.0778e-02,  2.7765e-02,  2.4845e-02,\n",
            "          1.2816e-02, -3.5706e-03, -3.4204e-02,  5.7670e-03, -8.1190e-02,\n",
            "          4.6752e-02,  1.0227e-02,  1.9600e-02, -4.8482e-02, -8.2291e-03,\n",
            "          5.3433e-02, -4.3241e-02, -1.9539e-02, -6.3178e-02, -3.7383e-02,\n",
            "          3.2474e-02, -2.4669e-02,  3.4065e-02, -2.5067e-03,  3.1161e-02,\n",
            "         -4.0473e-02,  1.0245e-01,  3.3878e-02,  3.1162e-02, -8.1756e-02,\n",
            "         -2.4384e-02,  8.1834e-03, -4.2224e-02,  2.7212e-02,  2.8492e-02,\n",
            "         -2.3250e-02, -9.9050e-03,  9.8212e-03,  6.5669e-02,  1.1369e-02,\n",
            "          9.3543e-03, -3.8569e-02,  2.8028e-02, -2.3977e-02,  1.1411e-02,\n",
            "         -5.0309e-02,  4.4590e-02,  1.8095e-03,  7.8303e-02,  9.7701e-02,\n",
            "         -5.8601e-02, -1.9045e-02, -5.8535e-02, -6.2839e-02, -6.8585e-02,\n",
            "         -1.6401e-02,  3.9257e-02, -2.5242e-02,  4.3769e-02,  5.9935e-03,\n",
            "          2.3952e-02,  6.9342e-03,  6.6831e-02,  1.0969e-02, -2.9970e-02,\n",
            "         -2.8002e-02, -1.8098e-02,  3.4470e-02,  6.4278e-02,  2.1529e-02,\n",
            "          3.6731e-02,  2.2941e-02,  4.2895e-02,  3.0620e-02, -9.8250e-03,\n",
            "         -5.3400e-02, -3.3380e-02, -2.2729e-02, -3.6701e-02,  3.1967e-02,\n",
            "          2.6139e-02,  1.8370e-02,  3.0518e-02,  9.0168e-03,  8.1202e-03,\n",
            "         -4.3600e-02, -1.8390e-02,  2.3712e-02, -3.2051e-02,  3.0209e-02,\n",
            "          5.4163e-02, -3.7494e-03,  5.1583e-02, -9.0504e-03, -2.2491e-03,\n",
            "         -1.3941e-02, -4.5437e-02,  8.4746e-02,  7.5599e-02, -8.0215e-03,\n",
            "         -2.1497e-02,  3.1263e-02,  8.1941e-03,  1.7045e-02,  1.3506e-02,\n",
            "          1.0957e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.5201,  0.4479,  4.6120, -4.0109, -3.9217], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4981,  0.1824,  3.9958, -4.1218, -4.0662], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,   379,   257,  ...,  2568,   357,  9328],\n",
            "        [50256,    13,   383,  ...,    13,   362,    77],\n",
            "        [50256,   796,   513,  ...,     9,    83,   532],\n",
            "        ...,\n",
            "        [50256,  2372,   284,  ..., 21983,   373,  8197],\n",
            "        [50256, 16502,   257,  ...,   290,   882,   285],\n",
            "        [50256, 29567,   220,  ...,  2998,    24,   220]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0110,  0.0714, -0.0194,  ..., -0.0021,  0.0354,  0.0342],\n",
            "        [-0.0193,  0.0179, -0.0121,  ..., -0.0490, -0.0142,  0.0187],\n",
            "        [-0.0132,  0.0494, -0.0373,  ...,  0.0168,  0.0130,  0.1098],\n",
            "        [-0.0116,  0.0597, -0.0591,  ..., -0.0163, -0.0320,  0.0273],\n",
            "        [ 0.0592, -0.0232, -0.0310,  ..., -0.0020,  0.0223,  0.0353]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1005e-02,  7.1392e-02, -1.9365e-02, -3.3902e-02, -9.1513e-03,\n",
            "         -4.9990e-03, -1.1704e-02,  3.5561e-03, -7.0292e-02, -1.2954e-02,\n",
            "          4.8993e-03,  1.4867e-02, -3.3173e-02, -2.8712e-02, -4.2320e-02,\n",
            "         -3.2487e-02, -3.8276e-03,  2.0650e-02,  1.5401e-02,  2.1738e-02,\n",
            "         -2.9466e-02,  4.3863e-02, -1.2259e-02, -3.2290e-02, -1.3976e-02,\n",
            "          2.6590e-02, -2.9595e-02,  3.6032e-02,  7.4305e-03, -1.0152e-02,\n",
            "         -7.3755e-02,  2.3101e-02,  1.4263e-02,  1.7525e-02, -3.3684e-02,\n",
            "         -8.1981e-03,  6.3449e-02,  3.2143e-02,  3.2609e-02,  2.2706e-03,\n",
            "         -4.0701e-02, -3.4489e-03,  3.0027e-02, -1.4407e-02,  1.1872e-02,\n",
            "          8.8053e-03,  2.7886e-02, -7.5663e-02, -3.9077e-02, -6.9802e-02,\n",
            "         -2.3436e-02,  1.4337e-02, -2.2106e-02,  4.7429e-02,  2.8288e-02,\n",
            "          2.8363e-03, -2.3211e-02, -1.6387e-02,  4.4779e-02,  2.1030e-02,\n",
            "         -3.8677e-03,  5.6157e-04, -4.3995e-03, -1.6581e-02, -3.4893e-02,\n",
            "          2.0493e-02,  5.0445e-02, -1.7428e-02,  2.3734e-02,  4.3434e-02,\n",
            "          2.4047e-02, -1.7408e-02, -1.3759e-02,  2.9398e-02, -4.4597e-02,\n",
            "         -3.7050e-02, -1.7701e-02,  7.1338e-02,  6.3611e-02,  2.5062e-02,\n",
            "         -1.3410e-02, -2.6645e-02,  3.3410e-02, -3.4536e-02, -1.1056e-02,\n",
            "          9.9538e-02,  5.5695e-03,  3.2965e-02, -1.4240e-02,  3.5094e-03,\n",
            "          3.1632e-03, -5.5571e-02,  2.2145e-02,  6.7174e-04, -2.1775e-02,\n",
            "         -1.4309e-02, -7.3875e-02, -4.4272e-02,  1.5676e-02, -1.5218e-02,\n",
            "         -4.6011e-02,  4.1555e-03,  9.5249e-03,  1.6570e-02, -1.2712e-02,\n",
            "         -7.6438e-03,  5.2408e-02, -2.5642e-02, -5.0381e-02,  4.6417e-02,\n",
            "         -3.5219e-02, -1.7780e-02, -6.1495e-03, -6.3931e-02, -9.7454e-03,\n",
            "         -9.5138e-03, -3.7477e-03,  3.6093e-02,  4.0455e-02,  6.8422e-02,\n",
            "          1.8489e-02, -6.8929e-03, -3.3697e-02,  1.1089e-01,  3.8323e-02,\n",
            "         -7.3331e-03, -3.0975e-02, -9.9638e-03,  2.3833e-02, -7.2998e-03,\n",
            "          6.2884e-02,  1.0007e-02, -4.2667e-03,  2.4039e-02, -3.9937e-02,\n",
            "          3.6838e-04, -4.7476e-02,  4.1834e-02, -1.0681e-02,  8.8595e-03,\n",
            "         -1.8027e-02, -1.3737e-02,  3.7581e-02, -4.4320e-02, -2.4846e-02,\n",
            "          3.6847e-02, -2.4169e-02, -3.3025e-02,  3.9394e-02,  5.9893e-04,\n",
            "         -1.3506e-02,  5.9545e-02,  4.1782e-03,  7.1287e-03,  8.1505e-03,\n",
            "         -1.8696e-02,  1.3729e-02, -9.8386e-03, -6.2947e-03,  2.7342e-02,\n",
            "         -7.2950e-03, -3.1197e-02, -2.6365e-02, -5.6722e-02, -1.0655e-02,\n",
            "          6.0576e-02, -8.7038e-03, -5.0577e-02, -2.1160e-02, -3.7283e-02,\n",
            "         -2.6071e-03, -1.0142e-02, -4.0471e-03, -2.4936e-02, -3.5608e-02,\n",
            "         -2.2610e-02,  4.1660e-03, -1.3628e-02, -1.5819e-02, -8.7070e-02,\n",
            "          3.0442e-02,  4.4728e-02, -1.2770e-02, -7.9674e-03, -1.9000e-02,\n",
            "         -3.1534e-03,  1.0288e-02,  3.8115e-02,  4.8398e-02,  3.9355e-02,\n",
            "          7.6303e-03, -4.1615e-02, -5.9996e-03,  2.9885e-02, -1.5866e-03,\n",
            "         -2.5431e-02, -1.5352e-01, -2.6218e-02, -5.1231e-02, -3.5220e-02,\n",
            "          8.6017e-03, -1.3039e-02, -2.5296e-02,  4.8372e-02,  4.7559e-02,\n",
            "          3.3061e-02,  1.0158e-02,  2.2664e-02,  2.5919e-02, -3.0685e-02,\n",
            "         -1.4761e-02, -3.4041e-02, -1.1210e-02,  6.7856e-03, -3.9004e-02,\n",
            "         -4.0840e-03, -1.6206e-02,  1.8976e-02,  2.1983e-02,  3.6363e-02,\n",
            "          1.2083e-01,  4.3625e-02,  3.5639e-02,  1.2615e-02,  3.1624e-02,\n",
            "         -9.8493e-02,  8.3474e-03, -3.1367e-03, -1.0083e-02, -8.9130e-03,\n",
            "          1.3157e-02, -3.1264e-03, -9.8344e-03, -4.0042e-02,  4.9471e-02,\n",
            "          2.6380e-02, -2.0539e-02,  1.6895e-02,  1.7909e-02, -2.6784e-02,\n",
            "          1.6183e-02,  1.4308e-02, -2.2562e-05, -3.2793e-03,  7.5966e-03,\n",
            "         -1.2357e-01,  8.6833e-03,  7.6679e-03,  4.7091e-02, -2.5368e-02,\n",
            "          1.4708e-02, -5.9130e-03,  1.9060e-02, -2.0979e-03,  3.5445e-02,\n",
            "          3.4220e-02],\n",
            "        [-1.9331e-02,  1.7904e-02, -1.2108e-02,  3.6657e-02,  1.1743e-01,\n",
            "         -4.3630e-02, -2.2843e-02,  4.1710e-02,  1.8764e-02, -9.3989e-03,\n",
            "          4.8468e-03, -5.0651e-03, -9.7924e-03,  2.4145e-02,  1.0776e-01,\n",
            "          4.5224e-02,  1.4130e-03, -1.7421e-02, -1.3103e-02, -3.8400e-02,\n",
            "          9.8432e-02,  1.6335e-02, -5.3716e-03, -8.7768e-02, -4.2797e-02,\n",
            "         -4.5083e-02,  2.5107e-03, -6.1984e-02,  1.9779e-02, -9.9254e-03,\n",
            "          8.3497e-02, -3.0111e-02, -1.8644e-02,  8.3916e-03,  3.1009e-02,\n",
            "         -6.0195e-02,  1.0512e-03, -5.5805e-02, -5.1409e-03,  1.9388e-02,\n",
            "          2.5617e-02, -1.1533e-02, -2.9304e-02,  3.0852e-03, -3.0193e-02,\n",
            "         -1.5559e-02, -6.4840e-05,  8.3950e-02,  2.1158e-02,  1.2772e-03,\n",
            "          8.0618e-02, -9.7859e-03, -4.4232e-02, -4.0625e-02,  8.1726e-02,\n",
            "          9.1011e-02, -2.0017e-03, -9.4557e-02, -7.2283e-03,  3.1404e-02,\n",
            "          7.3521e-02, -2.0410e-02, -1.3258e-02,  9.1526e-02,  1.9100e-03,\n",
            "          5.3936e-02,  2.7784e-02, -5.7058e-02, -1.7917e-02, -9.4301e-03,\n",
            "          4.8643e-02,  3.0556e-02,  3.4239e-02,  9.0413e-02,  6.2072e-02,\n",
            "          2.9416e-02, -1.2535e-01,  8.9662e-02,  1.4257e-02, -4.6526e-02,\n",
            "         -4.6821e-02, -3.7256e-02, -6.9075e-02,  8.1515e-02,  6.4565e-04,\n",
            "         -3.8243e-02,  7.4410e-02, -3.2406e-02,  1.2603e-02,  2.3298e-02,\n",
            "         -2.3235e-02,  1.7641e-02, -4.0529e-02, -1.2819e-02, -1.7453e-02,\n",
            "         -5.4115e-02, -6.6052e-02, -2.5195e-02, -6.2732e-02, -2.7222e-02,\n",
            "         -5.2167e-02, -1.9005e-02,  1.7733e-02, -1.5480e-02,  7.7633e-03,\n",
            "         -1.5685e-03, -3.9052e-02, -2.2441e-02,  4.0687e-02, -2.7005e-03,\n",
            "          7.5099e-03,  8.9677e-03, -8.6743e-02,  3.7464e-02,  7.7381e-02,\n",
            "         -3.7155e-02, -7.9577e-02, -3.4146e-02, -5.5810e-02, -6.1912e-02,\n",
            "         -1.5433e-02, -1.5467e-02, -1.8762e-02, -4.3134e-02,  9.8823e-03,\n",
            "          1.2966e-02,  3.1580e-02,  2.8684e-02, -5.1159e-02,  4.5288e-03,\n",
            "          2.6770e-02,  2.9633e-02,  2.8226e-02, -2.4525e-02,  6.0781e-04,\n",
            "          4.4940e-03, -3.4247e-02,  2.0468e-02,  5.9173e-02,  4.6061e-02,\n",
            "         -2.4154e-02, -7.6722e-02,  9.0696e-02, -5.0029e-02, -1.1831e-02,\n",
            "         -1.0338e-02,  3.1810e-02, -2.7097e-02,  1.3191e-02,  3.5865e-03,\n",
            "         -1.4593e-03,  5.1762e-02,  2.5399e-02,  3.5226e-03, -3.0794e-02,\n",
            "          3.6631e-02,  1.6042e-02, -3.7071e-02,  7.7693e-02, -6.9514e-02,\n",
            "         -4.8863e-02, -1.9817e-02, -1.8981e-02, -6.8694e-02, -3.6552e-02,\n",
            "          3.8176e-02, -7.0020e-02, -9.5919e-03, -5.6772e-02, -1.2282e-02,\n",
            "          2.7815e-02, -3.7405e-02, -1.7506e-02, -2.6890e-02, -1.0852e-02,\n",
            "         -1.1765e-02,  8.8783e-03,  6.8705e-02,  1.9386e-02, -1.6041e-02,\n",
            "         -4.4328e-02, -1.9020e-02, -8.4600e-03, -5.8180e-02, -9.6997e-03,\n",
            "         -8.8222e-03, -6.4474e-03,  1.8289e-02, -3.7531e-02,  3.2260e-02,\n",
            "         -7.3805e-03, -2.2588e-02,  6.4870e-02, -1.0663e-01,  6.0661e-03,\n",
            "          8.8246e-03,  7.4140e-02, -1.6654e-02,  7.2720e-03, -3.9656e-02,\n",
            "          4.7892e-02, -2.0184e-02, -3.5879e-02,  2.6525e-02, -9.3163e-02,\n",
            "         -3.9191e-02, -8.0335e-03, -5.2575e-02, -1.1195e-02, -4.1225e-02,\n",
            "         -6.6505e-02,  6.1828e-02, -2.1686e-02, -6.2569e-02, -8.6740e-02,\n",
            "          3.1829e-02,  9.0553e-03,  3.6756e-02,  3.3463e-02, -1.3769e-03,\n",
            "          2.9759e-02, -2.5986e-03,  1.2628e-02,  8.3541e-02, -3.0432e-02,\n",
            "          3.7395e-02,  3.4976e-02, -1.8006e-04,  4.8445e-02, -3.1970e-03,\n",
            "         -2.2759e-02, -3.0051e-03,  7.7743e-02,  1.6167e-03,  6.2773e-02,\n",
            "         -7.9032e-02,  2.5210e-02, -2.3107e-02, -2.2653e-02,  5.0154e-02,\n",
            "         -8.2249e-02, -2.9354e-02,  6.0302e-02, -2.1260e-03, -1.2029e-02,\n",
            "         -2.0371e-02, -2.3731e-02,  3.2531e-02, -2.3134e-03, -1.1172e-02,\n",
            "         -1.7460e-02,  5.7291e-02, -1.2283e-02, -4.9040e-02, -1.4166e-02,\n",
            "          1.8699e-02],\n",
            "        [-1.3248e-02,  4.9400e-02, -3.7315e-02, -2.8866e-03, -3.0272e-03,\n",
            "         -5.5434e-02, -2.3195e-02, -4.1086e-02, -6.6181e-04,  4.4840e-02,\n",
            "         -7.3993e-03,  1.0587e-03,  1.5074e-02, -3.6537e-02, -2.1612e-02,\n",
            "         -1.8587e-02, -1.6217e-02,  1.1473e-02, -1.9538e-02, -2.1050e-02,\n",
            "          1.1664e-03, -4.1003e-03, -6.6932e-03, -3.3628e-02, -4.0907e-02,\n",
            "          5.2917e-02,  1.9005e-02, -5.2720e-02, -5.2419e-02, -1.1141e-03,\n",
            "         -6.6544e-02,  3.6728e-02, -1.9315e-02, -1.8391e-02,  3.2646e-04,\n",
            "         -3.9359e-02, -2.3905e-03,  5.4728e-02,  1.7998e-02, -4.7144e-02,\n",
            "         -6.7199e-02, -1.8566e-02, -4.4889e-02,  3.4201e-02, -2.3967e-02,\n",
            "         -4.5081e-02, -3.0433e-02,  1.1658e-02,  4.8367e-02, -2.5694e-02,\n",
            "         -1.1034e-02, -3.7872e-02, -4.0428e-02,  5.5610e-02,  8.9280e-03,\n",
            "          2.4888e-02, -5.4054e-02,  6.9806e-02,  2.1767e-02,  1.9296e-02,\n",
            "          2.3201e-02,  1.9695e-02, -2.3186e-02, -3.3605e-02, -1.2190e-02,\n",
            "          3.3840e-02,  1.0324e-02,  8.3188e-04, -4.2965e-03,  6.2731e-02,\n",
            "          1.3477e-02,  7.4025e-03,  1.2751e-02, -1.7485e-02, -5.8212e-02,\n",
            "         -5.9018e-02, -3.6531e-03,  1.9515e-02, -5.6894e-03,  1.6392e-02,\n",
            "         -5.3885e-02,  2.6693e-02,  1.4505e-03, -1.0560e-02,  7.7950e-03,\n",
            "         -1.1042e-02,  3.4616e-02,  1.7386e-02,  2.2437e-02, -4.7370e-03,\n",
            "         -9.4968e-03, -2.7732e-02,  2.0766e-02, -4.3053e-02, -5.5034e-02,\n",
            "          3.7751e-02,  3.0139e-02, -9.3445e-03, -1.4628e-02,  1.9657e-02,\n",
            "         -4.4689e-02, -4.3116e-02, -1.5354e-02, -4.8623e-02, -1.5040e-02,\n",
            "         -3.6502e-02,  2.5128e-02, -4.0215e-02, -2.6176e-04,  3.9895e-04,\n",
            "         -1.8642e-02,  2.5884e-02, -7.0211e-03,  2.0126e-03, -2.1696e-03,\n",
            "         -3.7605e-02,  1.7381e-02,  9.8778e-02, -3.1874e-03, -3.6172e-02,\n",
            "          2.0305e-02,  1.6205e-02,  7.1845e-02,  1.3441e-02, -3.2602e-02,\n",
            "         -2.1001e-02,  9.0890e-03,  4.4492e-02,  2.0969e-03, -7.8057e-03,\n",
            "         -7.4718e-02, -1.8706e-03,  6.9101e-03, -4.0278e-02, -2.4880e-03,\n",
            "          2.2332e-02,  2.0864e-02,  2.2294e-02,  3.1653e-02,  2.7666e-02,\n",
            "          2.8765e-02, -2.9881e-02,  1.6585e-02,  2.2606e-02,  2.0609e-02,\n",
            "         -8.2510e-03,  3.3785e-02,  4.6931e-02, -2.8655e-02,  1.8290e-02,\n",
            "         -3.0288e-03, -1.8471e-02,  5.1153e-02,  2.7572e-02,  2.5068e-02,\n",
            "          1.2573e-02, -3.4782e-03, -3.4180e-02,  5.7015e-03, -8.1442e-02,\n",
            "          4.6952e-02,  1.0212e-02,  1.9869e-02, -4.8429e-02, -8.1802e-03,\n",
            "          5.3312e-02, -4.3676e-02, -1.9786e-02, -6.3358e-02, -3.7128e-02,\n",
            "          3.2254e-02, -2.4682e-02,  3.4028e-02, -2.3584e-03,  3.1516e-02,\n",
            "         -4.0266e-02,  1.0249e-01,  3.4070e-02,  3.1409e-02, -8.1673e-02,\n",
            "         -2.4050e-02,  8.0638e-03, -4.2173e-02,  2.7317e-02,  2.8324e-02,\n",
            "         -2.2784e-02, -9.4867e-03,  9.9498e-03,  6.5818e-02,  1.1424e-02,\n",
            "          8.8061e-03, -3.8285e-02,  2.8418e-02, -2.3835e-02,  1.1533e-02,\n",
            "         -5.0308e-02,  4.4770e-02,  1.8548e-03,  7.8057e-02,  9.7695e-02,\n",
            "         -5.8826e-02, -1.9300e-02, -5.8399e-02, -6.2698e-02, -6.8284e-02,\n",
            "         -1.6819e-02,  3.9721e-02, -2.5430e-02,  4.3861e-02,  5.6120e-03,\n",
            "          2.3951e-02,  6.8439e-03,  6.6697e-02,  1.1037e-02, -2.9992e-02,\n",
            "         -2.8503e-02, -1.8059e-02,  3.4579e-02,  6.4370e-02,  2.1928e-02,\n",
            "          3.6618e-02,  2.2870e-02,  4.2602e-02,  3.0563e-02, -9.3195e-03,\n",
            "         -5.3360e-02, -3.3739e-02, -2.2586e-02, -3.6801e-02,  3.2234e-02,\n",
            "          2.6503e-02,  1.8146e-02,  3.0673e-02,  8.7906e-03,  7.9324e-03,\n",
            "         -4.3518e-02, -1.8308e-02,  2.3924e-02, -3.2005e-02,  3.0112e-02,\n",
            "          5.4208e-02, -3.8544e-03,  5.1553e-02, -9.1529e-03, -2.4236e-03,\n",
            "         -1.4128e-02, -4.5569e-02,  8.5100e-02,  7.5442e-02, -8.0464e-03,\n",
            "         -2.1572e-02,  3.0750e-02,  8.1412e-03,  1.6795e-02,  1.3040e-02,\n",
            "          1.0981e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.5070,  0.4879,  4.5660, -4.0988, -3.7882], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4710,  0.2938,  3.9674, -4.1963, -3.8819], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,  2152,   257,  ...,    77,  3467,   259],\n",
            "        [50256,   262,  1402,  ...,    11,  1111, 43376],\n",
            "        [50256,   547,  3432,  ...,     1,  3646,  2913],\n",
            "        ...,\n",
            "        [50256,   220,   220,  ...,   220,   220,   220],\n",
            "        [50256,    12, 25272,  ..., 43697,   615,    22],\n",
            "        [50256,  3424,   262,  ...,   290, 44491,   612]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0110,  0.0714, -0.0198,  ..., -0.0023,  0.0353,  0.0344],\n",
            "        [-0.0192,  0.0181, -0.0125,  ..., -0.0492, -0.0144,  0.0190],\n",
            "        [-0.0129,  0.0498, -0.0376,  ...,  0.0163,  0.0126,  0.1102],\n",
            "        [-0.0117,  0.0597, -0.0592,  ..., -0.0163, -0.0319,  0.0272],\n",
            "        [ 0.0593, -0.0234, -0.0307,  ..., -0.0020,  0.0227,  0.0353]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.0992e-02,  7.1363e-02, -1.9754e-02, -3.3836e-02, -8.8068e-03,\n",
            "         -5.0243e-03, -1.2010e-02,  3.5185e-03, -7.0632e-02, -1.2828e-02,\n",
            "          4.8657e-03,  1.4659e-02, -3.3372e-02, -2.8698e-02, -4.2150e-02,\n",
            "         -3.2339e-02, -3.8871e-03,  2.0864e-02,  1.5581e-02,  2.1785e-02,\n",
            "         -2.9689e-02,  4.3734e-02, -1.2299e-02, -3.1996e-02, -1.4170e-02,\n",
            "          2.6258e-02, -2.9489e-02,  3.6248e-02,  7.5941e-03, -1.0303e-02,\n",
            "         -7.3702e-02,  2.3129e-02,  1.3916e-02,  1.8066e-02, -3.3870e-02,\n",
            "         -7.9103e-03,  6.3647e-02,  3.2302e-02,  3.2624e-02,  2.6101e-03,\n",
            "         -4.0355e-02, -3.6921e-03,  3.0101e-02, -1.4385e-02,  1.1925e-02,\n",
            "          8.5937e-03,  2.7527e-02, -7.5714e-02, -3.9373e-02, -6.9989e-02,\n",
            "         -2.3297e-02,  1.4179e-02, -2.2236e-02,  4.7369e-02,  2.8567e-02,\n",
            "          2.7954e-03, -2.3058e-02, -1.6286e-02,  4.5075e-02,  2.1166e-02,\n",
            "         -3.9763e-03,  7.7605e-04, -4.0686e-03, -1.6263e-02, -3.4792e-02,\n",
            "          2.0493e-02,  5.0625e-02, -1.7563e-02,  2.3880e-02,  4.3753e-02,\n",
            "          2.4120e-02, -1.7199e-02, -1.3866e-02,  2.9373e-02, -4.4443e-02,\n",
            "         -3.7117e-02, -1.7753e-02,  7.1358e-02,  6.4070e-02,  2.5216e-02,\n",
            "         -1.3587e-02, -2.6510e-02,  3.3191e-02, -3.4183e-02, -1.1255e-02,\n",
            "          9.9596e-02,  5.3851e-03,  3.2878e-02, -1.4169e-02,  3.6990e-03,\n",
            "          3.0994e-03, -5.5441e-02,  2.2322e-02,  7.3625e-04, -2.1900e-02,\n",
            "         -1.4822e-02, -7.3976e-02, -4.4392e-02,  1.5685e-02, -1.5142e-02,\n",
            "         -4.6436e-02,  3.9343e-03,  9.2992e-03,  1.6478e-02, -1.2485e-02,\n",
            "         -7.4890e-03,  5.2274e-02, -2.5437e-02, -5.0378e-02,  4.6111e-02,\n",
            "         -3.5140e-02, -1.7938e-02, -6.5952e-03, -6.4021e-02, -1.0009e-02,\n",
            "         -9.1731e-03, -3.4061e-03,  3.6281e-02,  4.0312e-02,  6.8396e-02,\n",
            "          1.8721e-02, -6.8110e-03, -3.3401e-02,  1.1083e-01,  3.8621e-02,\n",
            "         -7.4507e-03, -3.1051e-02, -1.0123e-02,  2.3921e-02, -7.1743e-03,\n",
            "          6.2961e-02,  9.9492e-03, -4.3657e-03,  2.4441e-02, -4.0066e-02,\n",
            "          3.3071e-04, -4.7727e-02,  4.1877e-02, -1.0560e-02,  9.3642e-03,\n",
            "         -1.8301e-02, -1.3823e-02,  3.7255e-02, -4.4060e-02, -2.5053e-02,\n",
            "          3.6898e-02, -2.4615e-02, -3.3067e-02,  3.9482e-02,  5.3335e-04,\n",
            "         -1.3874e-02,  5.9368e-02,  4.3712e-03,  6.9659e-03,  8.2138e-03,\n",
            "         -1.8684e-02,  1.4086e-02, -9.8641e-03, -6.1287e-03,  2.7090e-02,\n",
            "         -7.4654e-03, -3.1148e-02, -2.6522e-02, -5.6884e-02, -1.0403e-02,\n",
            "          6.1302e-02, -8.5537e-03, -5.0922e-02, -2.1319e-02, -3.7086e-02,\n",
            "         -2.7887e-03, -1.0376e-02, -4.0113e-03, -2.5282e-02, -3.5922e-02,\n",
            "         -2.2613e-02,  4.0990e-03, -1.3906e-02, -1.5917e-02, -8.7066e-02,\n",
            "          3.0510e-02,  4.4593e-02, -1.3056e-02, -8.1226e-03, -1.9485e-02,\n",
            "         -2.8238e-03,  1.0425e-02,  3.8113e-02,  4.8410e-02,  3.9395e-02,\n",
            "          7.6258e-03, -4.1485e-02, -5.8421e-03,  2.9799e-02, -1.8925e-03,\n",
            "         -2.5186e-02, -1.5374e-01, -2.6356e-02, -5.1119e-02, -3.5192e-02,\n",
            "          8.5821e-03, -1.3306e-02, -2.5061e-02,  4.8372e-02,  4.7645e-02,\n",
            "          3.3311e-02,  1.0383e-02,  2.2799e-02,  2.5943e-02, -3.0290e-02,\n",
            "         -1.5016e-02, -3.4213e-02, -1.1612e-02,  6.9119e-03, -3.9230e-02,\n",
            "         -4.0340e-03, -1.6113e-02,  1.9254e-02,  2.2066e-02,  3.6235e-02,\n",
            "          1.2071e-01,  4.3261e-02,  3.5609e-02,  1.2176e-02,  3.1786e-02,\n",
            "         -9.8119e-02,  8.5543e-03, -3.3790e-03, -9.5048e-03, -8.9049e-03,\n",
            "          1.3102e-02, -3.2844e-03, -9.5059e-03, -4.0257e-02,  4.9160e-02,\n",
            "          2.6587e-02, -2.0446e-02,  1.6930e-02,  1.7964e-02, -2.6689e-02,\n",
            "          1.6575e-02,  1.3940e-02,  1.4419e-04, -2.8336e-03,  7.5089e-03,\n",
            "         -1.2392e-01,  8.5472e-03,  7.6530e-03,  4.7312e-02, -2.5551e-02,\n",
            "          1.4363e-02, -6.0069e-03,  1.9062e-02, -2.2897e-03,  3.5284e-02,\n",
            "          3.4440e-02],\n",
            "        [-1.9198e-02,  1.8070e-02, -1.2460e-02,  3.6901e-02,  1.1715e-01,\n",
            "         -4.3482e-02, -2.3161e-02,  4.1932e-02,  1.8823e-02, -9.3660e-03,\n",
            "          4.6617e-03, -4.9875e-03, -1.0306e-02,  2.3795e-02,  1.0790e-01,\n",
            "          4.5483e-02,  1.4799e-03, -1.7332e-02, -1.3343e-02, -3.8383e-02,\n",
            "          9.8549e-02,  1.6071e-02, -5.0957e-03, -8.7987e-02, -4.2749e-02,\n",
            "         -4.4999e-02,  2.4192e-03, -6.2005e-02,  1.9611e-02, -1.0187e-02,\n",
            "          8.3670e-02, -2.9721e-02, -1.8170e-02,  8.2577e-03,  3.1118e-02,\n",
            "         -5.9940e-02,  8.7336e-04, -5.5771e-02, -4.9874e-03,  1.9182e-02,\n",
            "          2.5487e-02, -1.1736e-02, -2.8949e-02,  2.8073e-03, -3.0240e-02,\n",
            "         -1.5981e-02,  4.6450e-05,  8.3822e-02,  2.1154e-02,  1.2760e-03,\n",
            "          8.0647e-02, -9.5200e-03, -4.4529e-02, -4.0940e-02,  8.1891e-02,\n",
            "          9.1270e-02, -2.0003e-03, -9.4742e-02, -7.2566e-03,  3.1648e-02,\n",
            "          7.3443e-02, -2.0296e-02, -1.3161e-02,  9.1478e-02,  1.8740e-03,\n",
            "          5.3892e-02,  2.8022e-02, -5.7215e-02, -1.7554e-02, -8.9536e-03,\n",
            "          4.8745e-02,  3.0671e-02,  3.3956e-02,  9.0674e-02,  6.2013e-02,\n",
            "          2.9598e-02, -1.2562e-01,  8.9647e-02,  1.4112e-02, -4.6749e-02,\n",
            "         -4.6782e-02, -3.7359e-02, -6.8883e-02,  8.1776e-02,  7.2722e-04,\n",
            "         -3.8260e-02,  7.4485e-02, -3.2100e-02,  1.2815e-02,  2.3613e-02,\n",
            "         -2.3173e-02,  1.7402e-02, -4.0520e-02, -1.2969e-02, -1.7633e-02,\n",
            "         -5.4001e-02, -6.6380e-02, -2.5131e-02, -6.2687e-02, -2.7432e-02,\n",
            "         -5.2395e-02, -1.8840e-02,  1.7745e-02, -1.5478e-02,  7.9264e-03,\n",
            "         -1.3012e-03, -3.9346e-02, -2.2490e-02,  4.0744e-02, -3.0033e-03,\n",
            "          7.7027e-03,  9.1388e-03, -8.6671e-02,  3.7211e-02,  7.7639e-02,\n",
            "         -3.7410e-02, -7.9467e-02, -3.4042e-02, -5.5539e-02, -6.1923e-02,\n",
            "         -1.5499e-02, -1.5677e-02, -1.8745e-02, -4.3435e-02,  9.7631e-03,\n",
            "          1.3049e-02,  3.2006e-02,  2.8854e-02, -5.1166e-02,  4.4930e-03,\n",
            "          2.6913e-02,  2.9870e-02,  2.8401e-02, -2.4868e-02,  4.1195e-04,\n",
            "          4.5236e-03, -3.4111e-02,  2.0262e-02,  5.9194e-02,  4.6201e-02,\n",
            "         -2.3878e-02, -7.6682e-02,  9.0848e-02, -5.0429e-02, -1.1932e-02,\n",
            "         -1.0170e-02,  3.1807e-02, -2.7168e-02,  1.3318e-02,  3.4575e-03,\n",
            "         -1.5926e-03,  5.1565e-02,  2.5589e-02,  3.6469e-03, -3.0433e-02,\n",
            "          3.6399e-02,  1.6069e-02, -3.7003e-02,  7.7554e-02, -6.9540e-02,\n",
            "         -4.8883e-02, -1.9743e-02, -1.8815e-02, -6.8695e-02, -3.6455e-02,\n",
            "          3.8058e-02, -7.0288e-02, -9.8834e-03, -5.6904e-02, -1.1946e-02,\n",
            "          2.7540e-02, -3.7122e-02, -1.7425e-02, -2.6746e-02, -1.0823e-02,\n",
            "         -1.1691e-02,  9.0337e-03,  6.8981e-02,  1.9747e-02, -1.5930e-02,\n",
            "         -4.4282e-02, -1.9231e-02, -8.6078e-03, -5.8149e-02, -9.8816e-03,\n",
            "         -8.6374e-03, -6.4621e-03,  1.8478e-02, -3.7351e-02,  3.2519e-02,\n",
            "         -7.7552e-03, -2.2554e-02,  6.5270e-02, -1.0647e-01,  5.9240e-03,\n",
            "          9.0241e-03,  7.4405e-02, -1.6800e-02,  7.2374e-03, -3.9585e-02,\n",
            "          4.7536e-02, -2.0393e-02, -3.5822e-02,  2.6327e-02, -9.3158e-02,\n",
            "         -3.9416e-02, -7.6072e-03, -5.3055e-02, -1.1129e-02, -4.1598e-02,\n",
            "         -6.6318e-02,  6.1672e-02, -2.1687e-02, -6.2612e-02, -8.6985e-02,\n",
            "          3.1730e-02,  9.3639e-03,  3.7051e-02,  3.3645e-02, -1.2057e-03,\n",
            "          2.9805e-02, -2.5916e-03,  1.2467e-02,  8.3427e-02, -3.0014e-02,\n",
            "          3.7217e-02,  3.4744e-02, -1.4647e-04,  4.8419e-02, -3.1373e-03,\n",
            "         -2.2739e-02, -3.2547e-03,  7.7853e-02,  1.3148e-03,  6.2674e-02,\n",
            "         -7.9195e-02,  2.5431e-02, -2.2943e-02, -2.2398e-02,  5.0135e-02,\n",
            "         -8.2286e-02, -2.9661e-02,  5.9993e-02, -2.3163e-03, -1.2177e-02,\n",
            "         -2.0642e-02, -2.3832e-02,  3.2877e-02, -2.4021e-03, -1.1280e-02,\n",
            "         -1.7588e-02,  5.7071e-02, -1.2442e-02, -4.9180e-02, -1.4442e-02,\n",
            "          1.9026e-02],\n",
            "        [-1.2943e-02,  4.9777e-02, -3.7639e-02, -2.6190e-03, -3.2780e-03,\n",
            "         -5.4987e-02, -2.3485e-02, -4.0927e-02, -6.7193e-04,  4.4962e-02,\n",
            "         -7.3703e-03,  9.0337e-04,  1.4688e-02, -3.6913e-02, -2.1596e-02,\n",
            "         -1.8248e-02, -1.6201e-02,  1.1584e-02, -1.9634e-02, -2.1082e-02,\n",
            "          1.0448e-03, -4.3815e-03, -6.3275e-03, -3.3518e-02, -4.1010e-02,\n",
            "          5.2871e-02,  1.8890e-02, -5.2622e-02, -5.2553e-02, -1.4411e-03,\n",
            "         -6.6494e-02,  3.7363e-02, -1.9204e-02, -1.8405e-02,  6.0523e-04,\n",
            "         -3.9135e-02, -2.5998e-03,  5.4662e-02,  1.7889e-02, -4.7086e-02,\n",
            "         -6.7266e-02, -1.8584e-02, -4.4666e-02,  3.4187e-02, -2.4127e-02,\n",
            "         -4.5394e-02, -3.0367e-02,  1.1347e-02,  4.8250e-02, -2.5882e-02,\n",
            "         -1.0946e-02, -3.7770e-02, -4.0573e-02,  5.5468e-02,  8.8886e-03,\n",
            "          2.4778e-02, -5.4187e-02,  6.9669e-02,  2.1710e-02,  1.9352e-02,\n",
            "          2.2897e-02,  1.9991e-02, -2.3013e-02, -3.3895e-02, -1.2232e-02,\n",
            "          3.3858e-02,  1.0391e-02,  1.1582e-03, -4.0189e-03,  6.2931e-02,\n",
            "          1.3410e-02,  7.6425e-03,  1.2370e-02, -1.7533e-02, -5.8402e-02,\n",
            "         -5.9140e-02, -3.4823e-03,  1.9336e-02, -5.8692e-03,  1.6547e-02,\n",
            "         -5.3751e-02,  2.6544e-02,  1.5261e-03, -1.0476e-02,  7.6533e-03,\n",
            "         -1.1019e-02,  3.4582e-02,  1.7481e-02,  2.2560e-02, -4.5879e-03,\n",
            "         -9.3537e-03, -2.7846e-02,  2.0671e-02, -4.3262e-02, -5.5196e-02,\n",
            "          3.7793e-02,  2.9752e-02, -9.0269e-03, -1.4419e-02,  1.9363e-02,\n",
            "         -4.4999e-02, -4.3077e-02, -1.5495e-02, -4.8482e-02, -1.4849e-02,\n",
            "         -3.6020e-02,  2.4774e-02, -4.0161e-02,  1.2138e-04,  2.4805e-05,\n",
            "         -1.8587e-02,  2.5990e-02, -6.7883e-03,  1.6861e-03, -2.0557e-03,\n",
            "         -3.7751e-02,  1.7604e-02,  9.8693e-02, -3.1418e-03, -3.6019e-02,\n",
            "          2.0230e-02,  1.6263e-02,  7.1924e-02,  1.3260e-02, -3.2412e-02,\n",
            "         -2.1302e-02,  9.4217e-03,  4.4656e-02,  2.1473e-03, -8.0344e-03,\n",
            "         -7.4644e-02, -1.6952e-03,  6.7721e-03, -4.0367e-02, -2.4987e-03,\n",
            "          2.2530e-02,  2.0939e-02,  2.2053e-02,  3.1618e-02,  2.7687e-02,\n",
            "          2.8849e-02, -2.9587e-02,  1.6468e-02,  2.2414e-02,  2.0325e-02,\n",
            "         -8.0661e-03,  3.3754e-02,  4.6947e-02, -2.8653e-02,  1.8251e-02,\n",
            "         -3.2327e-03, -1.8672e-02,  5.1530e-02,  2.7395e-02,  2.5345e-02,\n",
            "          1.2314e-02, -3.4985e-03, -3.4155e-02,  5.6657e-03, -8.1854e-02,\n",
            "          4.7092e-02,  1.0206e-02,  2.0055e-02, -4.8433e-02, -7.9901e-03,\n",
            "          5.3243e-02, -4.4057e-02, -2.0140e-02, -6.3587e-02, -3.6864e-02,\n",
            "          3.1988e-02, -2.4637e-02,  3.3909e-02, -2.3870e-03,  3.1846e-02,\n",
            "         -3.9964e-02,  1.0273e-01,  3.4282e-02,  3.1629e-02, -8.1838e-02,\n",
            "         -2.3751e-02,  7.9775e-03, -4.2270e-02,  2.7450e-02,  2.8169e-02,\n",
            "         -2.2372e-02, -9.0882e-03,  1.0264e-02,  6.6042e-02,  1.1681e-02,\n",
            "          8.3498e-03, -3.8076e-02,  2.8746e-02, -2.3563e-02,  1.1690e-02,\n",
            "         -5.0309e-02,  4.4806e-02,  2.0031e-03,  7.7889e-02,  9.7563e-02,\n",
            "         -5.9194e-02, -1.9441e-02, -5.8313e-02, -6.2570e-02, -6.7980e-02,\n",
            "         -1.7084e-02,  4.0039e-02, -2.5546e-02,  4.3912e-02,  5.3633e-03,\n",
            "          2.3976e-02,  6.6141e-03,  6.6593e-02,  1.1055e-02, -2.9955e-02,\n",
            "         -2.8955e-02, -1.7898e-02,  3.4672e-02,  6.4566e-02,  2.2371e-02,\n",
            "          3.6591e-02,  2.3019e-02,  4.2317e-02,  3.0470e-02, -8.8686e-03,\n",
            "         -5.3354e-02, -3.3986e-02, -2.2453e-02, -3.6803e-02,  3.2587e-02,\n",
            "          2.6617e-02,  1.7812e-02,  3.0861e-02,  8.5207e-03,  7.7382e-03,\n",
            "         -4.3445e-02, -1.8173e-02,  2.4128e-02, -3.1857e-02,  2.9818e-02,\n",
            "          5.4509e-02, -3.9714e-03,  5.1571e-02, -9.3046e-03, -2.4324e-03,\n",
            "         -1.4434e-02, -4.5854e-02,  8.5433e-02,  7.5240e-02, -8.1647e-03,\n",
            "         -2.1543e-02,  3.0277e-02,  8.0001e-03,  1.6340e-02,  1.2585e-02,\n",
            "          1.1016e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4894,  0.5829,  4.5600, -4.2055, -3.6847], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4466,  0.4500,  3.9601, -4.3132, -3.7369], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256, 17202,    37,  ...,   220,   220,   220],\n",
            "        [50256, 32383,  2318,  ..., 15580,   654,    58],\n",
            "        [50256,    11, 16143,  ..., 13816,   484,   477],\n",
            "        ...,\n",
            "        [50256,    44,    16,  ...,    15,    13, 22914],\n",
            "        [50256,  1635,  2488,  ...,   198,   220,   220],\n",
            "        [50256,   718,    25,  ...,  5668,  7675,    11]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0111,  0.0714, -0.0203,  ..., -0.0026,  0.0351,  0.0348],\n",
            "        [-0.0191,  0.0183, -0.0129,  ..., -0.0494, -0.0148,  0.0194],\n",
            "        [-0.0127,  0.0501, -0.0380,  ...,  0.0160,  0.0121,  0.1105],\n",
            "        [-0.0118,  0.0598, -0.0595,  ..., -0.0163, -0.0318,  0.0272],\n",
            "        [ 0.0593, -0.0236, -0.0307,  ..., -0.0020,  0.0230,  0.0353]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1083e-02,  7.1409e-02, -2.0255e-02, -3.3705e-02, -8.4503e-03,\n",
            "         -5.1481e-03, -1.2377e-02,  3.5446e-03, -7.1044e-02, -1.2675e-02,\n",
            "          4.7284e-03,  1.4428e-02, -3.3596e-02, -2.8725e-02, -4.1955e-02,\n",
            "         -3.2142e-02, -3.8893e-03,  2.1125e-02,  1.5679e-02,  2.1824e-02,\n",
            "         -2.9931e-02,  4.3554e-02, -1.2241e-02, -3.1754e-02, -1.4416e-02,\n",
            "          2.5884e-02, -2.9335e-02,  3.6416e-02,  7.8014e-03, -1.0571e-02,\n",
            "         -7.3669e-02,  2.3146e-02,  1.3538e-02,  1.8759e-02, -3.4092e-02,\n",
            "         -7.5692e-03,  6.3869e-02,  3.2526e-02,  3.2682e-02,  2.9581e-03,\n",
            "         -3.9932e-02, -3.9392e-03,  3.0245e-02, -1.4429e-02,  1.1952e-02,\n",
            "          8.3003e-03,  2.7170e-02, -7.5811e-02, -3.9708e-02, -7.0197e-02,\n",
            "         -2.3028e-02,  1.4036e-02, -2.2413e-02,  4.7330e-02,  2.8952e-02,\n",
            "          2.8640e-03, -2.2877e-02, -1.6154e-02,  4.5342e-02,  2.1347e-02,\n",
            "         -4.1335e-03,  1.0365e-03, -3.6261e-03, -1.5913e-02, -3.4692e-02,\n",
            "          2.0503e-02,  5.0893e-02, -1.7806e-02,  2.4164e-02,  4.4198e-02,\n",
            "          2.4103e-02, -1.6878e-02, -1.3951e-02,  2.9387e-02, -4.4262e-02,\n",
            "         -3.7132e-02, -1.7872e-02,  7.1458e-02,  6.4559e-02,  2.5280e-02,\n",
            "         -1.3992e-02, -2.6419e-02,  3.2948e-02, -3.3776e-02, -1.1482e-02,\n",
            "          9.9567e-02,  5.1923e-03,  3.2838e-02, -1.4125e-02,  3.9443e-03,\n",
            "          2.9949e-03, -5.5339e-02,  2.2421e-02,  7.5631e-04, -2.2136e-02,\n",
            "         -1.5501e-02, -7.4144e-02, -4.4527e-02,  1.5634e-02, -1.5112e-02,\n",
            "         -4.6923e-02,  3.6941e-03,  9.0884e-03,  1.6439e-02, -1.2271e-02,\n",
            "         -7.3509e-03,  5.2081e-02, -2.5246e-02, -5.0410e-02,  4.5708e-02,\n",
            "         -3.4991e-02, -1.8070e-02, -7.1067e-03, -6.4174e-02, -1.0184e-02,\n",
            "         -8.8574e-03, -2.9909e-03,  3.6697e-02,  4.0248e-02,  6.8268e-02,\n",
            "          1.9014e-02, -6.7402e-03, -3.3150e-02,  1.1068e-01,  3.8973e-02,\n",
            "         -7.5340e-03, -3.1019e-02, -1.0173e-02,  2.4093e-02, -7.0299e-03,\n",
            "          6.3059e-02,  9.9823e-03, -4.5054e-03,  2.4837e-02, -4.0308e-02,\n",
            "          2.5639e-04, -4.8005e-02,  4.1915e-02, -1.0378e-02,  1.0012e-02,\n",
            "         -1.8527e-02, -1.3931e-02,  3.7001e-02, -4.3769e-02, -2.5373e-02,\n",
            "          3.7065e-02, -2.5162e-02, -3.3143e-02,  3.9651e-02,  4.6957e-04,\n",
            "         -1.4303e-02,  5.9175e-02,  4.6075e-03,  6.8495e-03,  8.2738e-03,\n",
            "         -1.8688e-02,  1.4429e-02, -9.8978e-03, -5.9230e-03,  2.6810e-02,\n",
            "         -7.6794e-03, -3.1027e-02, -2.6796e-02, -5.7039e-02, -1.0139e-02,\n",
            "          6.2027e-02, -8.3964e-03, -5.1433e-02, -2.1577e-02, -3.6818e-02,\n",
            "         -3.0220e-03, -1.0584e-02, -3.9725e-03, -2.5725e-02, -3.6224e-02,\n",
            "         -2.2617e-02,  4.0642e-03, -1.4156e-02, -1.5932e-02, -8.7008e-02,\n",
            "          3.0613e-02,  4.4388e-02, -1.3499e-02, -8.2518e-03, -2.0094e-02,\n",
            "         -2.4357e-03,  1.0591e-02,  3.8112e-02,  4.8431e-02,  3.9533e-02,\n",
            "          7.5314e-03, -4.1409e-02, -5.5432e-03,  2.9764e-02, -2.2920e-03,\n",
            "         -2.4808e-02, -1.5380e-01, -2.6444e-02, -5.0926e-02, -3.5251e-02,\n",
            "          8.4398e-03, -1.3606e-02, -2.4753e-02,  4.8292e-02,  4.7652e-02,\n",
            "          3.3534e-02,  1.0720e-02,  2.2862e-02,  2.5975e-02, -2.9887e-02,\n",
            "         -1.5279e-02, -3.4436e-02, -1.2106e-02,  7.1074e-03, -3.9506e-02,\n",
            "         -3.9271e-03, -1.6023e-02,  1.9590e-02,  2.2150e-02,  3.6134e-02,\n",
            "          1.2051e-01,  4.2910e-02,  3.5531e-02,  1.1642e-02,  3.1955e-02,\n",
            "         -9.7696e-02,  8.7789e-03, -3.5554e-03, -8.8692e-03, -8.8015e-03,\n",
            "          1.3060e-02, -3.4878e-03, -9.1322e-03, -4.0526e-02,  4.8783e-02,\n",
            "          2.6795e-02, -2.0343e-02,  1.6963e-02,  1.8112e-02, -2.6576e-02,\n",
            "          1.6888e-02,  1.3472e-02,  3.2492e-04, -2.3287e-03,  7.4050e-03,\n",
            "         -1.2429e-01,  8.3243e-03,  7.6262e-03,  4.7546e-02, -2.5741e-02,\n",
            "          1.3977e-02, -6.0333e-03,  1.9029e-02, -2.5787e-03,  3.5078e-02,\n",
            "          3.4784e-02],\n",
            "        [-1.9097e-02,  1.8272e-02, -1.2941e-02,  3.7263e-02,  1.1689e-01,\n",
            "         -4.3282e-02, -2.3550e-02,  4.2167e-02,  1.8796e-02, -9.4309e-03,\n",
            "          4.3491e-03, -4.9247e-03, -1.0843e-02,  2.3334e-02,  1.0802e-01,\n",
            "          4.5840e-02,  1.5038e-03, -1.7206e-02, -1.3529e-02, -3.8441e-02,\n",
            "          9.8574e-02,  1.5640e-02, -4.8036e-03, -8.8144e-02, -4.2738e-02,\n",
            "         -4.4956e-02,  2.2789e-03, -6.1874e-02,  1.9441e-02, -1.0527e-02,\n",
            "          8.3796e-02, -2.9163e-02, -1.7649e-02,  8.1698e-03,  3.1281e-02,\n",
            "         -5.9546e-02,  6.8237e-04, -5.5688e-02, -4.8446e-03,  1.9036e-02,\n",
            "          2.5426e-02, -1.1961e-02, -2.8508e-02,  2.5033e-03, -3.0187e-02,\n",
            "         -1.6541e-02,  2.1233e-04,  8.3651e-02,  2.1008e-02,  1.2449e-03,\n",
            "          8.0689e-02, -9.2599e-03, -4.4881e-02, -4.1336e-02,  8.1995e-02,\n",
            "          9.1559e-02, -2.0450e-03, -9.4906e-02, -7.2992e-03,  3.1969e-02,\n",
            "          7.3261e-02, -2.0177e-02, -1.2989e-02,  9.1452e-02,  1.8880e-03,\n",
            "          5.3832e-02,  2.8282e-02, -5.7284e-02, -1.7068e-02, -8.3564e-03,\n",
            "          4.8823e-02,  3.0831e-02,  3.3579e-02,  9.0894e-02,  6.1900e-02,\n",
            "          2.9682e-02, -1.2586e-01,  8.9564e-02,  1.3913e-02, -4.6941e-02,\n",
            "         -4.6773e-02, -3.7530e-02, -6.8740e-02,  8.2138e-02,  6.7628e-04,\n",
            "         -3.8350e-02,  7.4538e-02, -3.1796e-02,  1.3084e-02,  2.3969e-02,\n",
            "         -2.3039e-02,  1.7257e-02, -4.0502e-02, -1.3097e-02, -1.7828e-02,\n",
            "         -5.3910e-02, -6.6757e-02, -2.4994e-02, -6.2642e-02, -2.7705e-02,\n",
            "         -5.2614e-02, -1.8712e-02,  1.7647e-02, -1.5455e-02,  8.2196e-03,\n",
            "         -9.4740e-04, -3.9767e-02, -2.2528e-02,  4.0840e-02, -3.4370e-03,\n",
            "          7.9382e-03,  9.2489e-03, -8.6561e-02,  3.6861e-02,  7.7990e-02,\n",
            "         -3.7655e-02, -7.9258e-02, -3.3980e-02, -5.5243e-02, -6.1968e-02,\n",
            "         -1.5557e-02, -1.5866e-02, -1.8644e-02, -4.3836e-02,  9.7452e-03,\n",
            "          1.3023e-02,  3.2542e-02,  2.9088e-02, -5.1179e-02,  4.4854e-03,\n",
            "          2.7050e-02,  3.0210e-02,  2.8597e-02, -2.5168e-02,  1.2921e-04,\n",
            "          4.5795e-03, -3.3957e-02,  2.0069e-02,  5.9215e-02,  4.6342e-02,\n",
            "         -2.3660e-02, -7.6573e-02,  9.0981e-02, -5.0808e-02, -1.2101e-02,\n",
            "         -9.9770e-03,  3.1730e-02, -2.7353e-02,  1.3394e-02,  3.3149e-03,\n",
            "         -1.7908e-03,  5.1273e-02,  2.5990e-02,  3.6633e-03, -3.0013e-02,\n",
            "          3.6117e-02,  1.6185e-02, -3.6892e-02,  7.7380e-02, -6.9642e-02,\n",
            "         -4.8823e-02, -1.9610e-02, -1.8672e-02, -6.8683e-02, -3.6234e-02,\n",
            "          3.7927e-02, -7.0559e-02, -1.0169e-02, -5.7030e-02, -1.1537e-02,\n",
            "          2.7189e-02, -3.6886e-02, -1.7345e-02, -2.6681e-02, -1.0755e-02,\n",
            "         -1.1618e-02,  9.2054e-03,  6.9228e-02,  2.0119e-02, -1.5843e-02,\n",
            "         -4.4144e-02, -1.9429e-02, -8.8636e-03, -5.8154e-02, -1.0052e-02,\n",
            "         -8.3261e-03, -6.3907e-03,  1.8693e-02, -3.7172e-02,  3.2802e-02,\n",
            "         -8.2757e-03, -2.2560e-02,  6.5702e-02, -1.0624e-01,  5.7303e-03,\n",
            "          9.2854e-03,  7.4741e-02, -1.6951e-02,  7.1984e-03, -3.9613e-02,\n",
            "          4.7198e-02, -2.0657e-02, -3.5638e-02,  2.6235e-02, -9.3021e-02,\n",
            "         -3.9718e-02, -7.0747e-03, -5.3490e-02, -1.1015e-02, -4.1923e-02,\n",
            "         -6.6201e-02,  6.1418e-02, -2.1754e-02, -6.2545e-02, -8.7176e-02,\n",
            "          3.1597e-02,  9.7402e-03,  3.7426e-02,  3.3807e-02, -1.0228e-03,\n",
            "          2.9801e-02, -2.5930e-03,  1.2317e-02,  8.3237e-02, -2.9542e-02,\n",
            "          3.7145e-02,  3.4417e-02, -9.7536e-05,  4.8395e-02, -3.0467e-03,\n",
            "         -2.2621e-02, -3.5404e-03,  7.7936e-02,  8.7414e-04,  6.2631e-02,\n",
            "         -7.9403e-02,  2.5647e-02, -2.2705e-02, -2.1981e-02,  5.0102e-02,\n",
            "         -8.2233e-02, -3.0144e-02,  5.9729e-02, -2.5253e-03, -1.2362e-02,\n",
            "         -2.0916e-02, -2.3913e-02,  3.3322e-02, -2.5569e-03, -1.1395e-02,\n",
            "         -1.7757e-02,  5.6630e-02, -1.2695e-02, -4.9370e-02, -1.4839e-02,\n",
            "          1.9432e-02],\n",
            "        [-1.2708e-02,  5.0131e-02, -3.8016e-02, -2.3668e-03, -3.5901e-03,\n",
            "         -5.4482e-02, -2.3830e-02, -4.0636e-02, -5.7259e-04,  4.5113e-02,\n",
            "         -7.3418e-03,  7.3435e-04,  1.4296e-02, -3.7363e-02, -2.1676e-02,\n",
            "         -1.7939e-02, -1.6176e-02,  1.1754e-02, -1.9748e-02, -2.1067e-02,\n",
            "          1.0122e-03, -4.7906e-03, -5.8411e-03, -3.3421e-02, -4.1106e-02,\n",
            "          5.2734e-02,  1.8675e-02, -5.2457e-02, -5.2689e-02, -1.6192e-03,\n",
            "         -6.6308e-02,  3.7924e-02, -1.9084e-02, -1.8544e-02,  7.2288e-04,\n",
            "         -3.8868e-02, -2.8949e-03,  5.4722e-02,  1.7928e-02, -4.6991e-02,\n",
            "         -6.7234e-02, -1.8667e-02, -4.4482e-02,  3.4085e-02, -2.4202e-02,\n",
            "         -4.5786e-02, -3.0238e-02,  1.1001e-02,  4.8035e-02, -2.5908e-02,\n",
            "         -1.0924e-02, -3.7562e-02, -4.0700e-02,  5.5343e-02,  8.9271e-03,\n",
            "          2.4620e-02, -5.4263e-02,  6.9410e-02,  2.1856e-02,  1.9449e-02,\n",
            "          2.2509e-02,  2.0160e-02, -2.2785e-02, -3.4214e-02, -1.2263e-02,\n",
            "          3.3715e-02,  1.0420e-02,  1.6121e-03, -3.6424e-03,  6.3243e-02,\n",
            "          1.3329e-02,  7.8595e-03,  1.1938e-02, -1.7485e-02, -5.8700e-02,\n",
            "         -5.9252e-02, -3.3043e-03,  1.9110e-02, -6.1700e-03,  1.6730e-02,\n",
            "         -5.3572e-02,  2.6392e-02,  1.7100e-03, -1.0571e-02,  7.5708e-03,\n",
            "         -1.0944e-02,  3.4493e-02,  1.7747e-02,  2.2789e-02, -4.3995e-03,\n",
            "         -9.3074e-03, -2.7976e-02,  2.0767e-02, -4.3472e-02, -5.5485e-02,\n",
            "          3.7891e-02,  2.9360e-02, -8.7152e-03, -1.4095e-02,  1.9103e-02,\n",
            "         -4.5278e-02, -4.2959e-02, -1.5761e-02, -4.8475e-02, -1.4536e-02,\n",
            "         -3.5470e-02,  2.4345e-02, -4.0061e-02,  3.7112e-04, -3.4481e-04,\n",
            "         -1.8439e-02,  2.6140e-02, -6.6202e-03,  1.1864e-03, -1.9356e-03,\n",
            "         -3.7895e-02,  1.7932e-02,  9.8570e-02, -3.1118e-03, -3.5838e-02,\n",
            "          2.0153e-02,  1.6185e-02,  7.2073e-02,  1.2970e-02, -3.2170e-02,\n",
            "         -2.1553e-02,  9.8209e-03,  4.4852e-02,  2.2016e-03, -8.3003e-03,\n",
            "         -7.4478e-02, -1.5886e-03,  6.5532e-03, -4.0488e-02, -2.4693e-03,\n",
            "          2.2799e-02,  2.0985e-02,  2.1793e-02,  3.1457e-02,  2.7696e-02,\n",
            "          2.9002e-02, -2.9226e-02,  1.6243e-02,  2.2261e-02,  2.0079e-02,\n",
            "         -7.8207e-03,  3.3660e-02,  4.6913e-02, -2.8571e-02,  1.8124e-02,\n",
            "         -3.5117e-03, -1.8896e-02,  5.1969e-02,  2.7193e-02,  2.5624e-02,\n",
            "          1.1935e-02, -3.3482e-03, -3.3930e-02,  5.4592e-03, -8.2191e-02,\n",
            "          4.7258e-02,  1.0384e-02,  2.0362e-02, -4.8203e-02, -7.7852e-03,\n",
            "          5.3146e-02, -4.4363e-02, -2.0474e-02, -6.3721e-02, -3.6764e-02,\n",
            "          3.1618e-02, -2.4550e-02,  3.3777e-02, -2.3419e-03,  3.2131e-02,\n",
            "         -3.9559e-02,  1.0302e-01,  3.4562e-02,  3.1822e-02, -8.1977e-02,\n",
            "         -2.3559e-02,  7.8023e-03, -4.2378e-02,  2.7451e-02,  2.8007e-02,\n",
            "         -2.1918e-02, -8.6364e-03,  1.0591e-02,  6.6181e-02,  1.2005e-02,\n",
            "          7.9468e-03, -3.7927e-02,  2.8992e-02, -2.3164e-02,  1.1848e-02,\n",
            "         -5.0219e-02,  4.4790e-02,  2.1057e-03,  7.7868e-02,  9.7479e-02,\n",
            "         -5.9594e-02, -1.9612e-02, -5.8107e-02, -6.2465e-02, -6.7789e-02,\n",
            "         -1.7384e-02,  4.0400e-02, -2.5667e-02,  4.3774e-02,  5.0897e-03,\n",
            "          2.4063e-02,  6.3754e-03,  6.6459e-02,  1.0994e-02, -2.9768e-02,\n",
            "         -2.9319e-02, -1.7654e-02,  3.4892e-02,  6.4798e-02,  2.2877e-02,\n",
            "          3.6348e-02,  2.3177e-02,  4.2067e-02,  3.0203e-02, -8.4204e-03,\n",
            "         -5.3398e-02, -3.4349e-02, -2.2417e-02, -3.6762e-02,  3.2771e-02,\n",
            "          2.6743e-02,  1.7367e-02,  3.1131e-02,  8.3072e-03,  7.5232e-03,\n",
            "         -4.3304e-02, -1.7890e-02,  2.4413e-02, -3.1654e-02,  2.9422e-02,\n",
            "          5.4883e-02, -4.1365e-03,  5.1438e-02, -9.5483e-03, -2.5733e-03,\n",
            "         -1.4659e-02, -4.6174e-02,  8.5740e-02,  7.4954e-02, -8.3757e-03,\n",
            "         -2.1416e-02,  2.9760e-02,  7.7609e-03,  1.5996e-02,  1.2072e-02,\n",
            "          1.1052e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.5078,  0.7336,  4.5945, -4.3965, -3.6047], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.3914,  0.6879,  4.0205, -4.6045, -3.6572], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "Step: 990, Loss: 5.5023\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,    13,   198,  ...,    46,    62,  7036],\n",
            "        [50256,   383,  1917,  ...,  1679,   290,   285],\n",
            "        [50256,  4778,    11,  ...,   393,   611,   511],\n",
            "        ...,\n",
            "        [50256,  1255,   286,  ...,  6946,   351,  1854],\n",
            "        [50256,   765,   340,  ...,   329,  2563, 20211],\n",
            "        [50256, 10563, 31478,  ..., 35505,  1782,    28]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0113,  0.0715, -0.0208,  ..., -0.0030,  0.0348,  0.0351],\n",
            "        [-0.0191,  0.0185, -0.0134,  ..., -0.0496, -0.0152,  0.0199],\n",
            "        [-0.0125,  0.0504, -0.0384,  ...,  0.0157,  0.0115,  0.1108],\n",
            "        [-0.0120,  0.0603, -0.0600,  ..., -0.0164, -0.0319,  0.0275],\n",
            "        [ 0.0592, -0.0238, -0.0306,  ..., -0.0020,  0.0231,  0.0353]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1256e-02,  7.1494e-02, -2.0788e-02, -3.3478e-02, -8.1613e-03,\n",
            "         -5.2056e-03, -1.2808e-02,  3.5966e-03, -7.1459e-02, -1.2557e-02,\n",
            "          4.6236e-03,  1.4205e-02, -3.3838e-02, -2.8759e-02, -4.1752e-02,\n",
            "         -3.1853e-02, -3.8301e-03,  2.1498e-02,  1.5755e-02,  2.1997e-02,\n",
            "         -3.0175e-02,  4.3317e-02, -1.2141e-02, -3.1465e-02, -1.4583e-02,\n",
            "          2.5462e-02, -2.9306e-02,  3.6608e-02,  8.0235e-03, -1.0833e-02,\n",
            "         -7.3536e-02,  2.3259e-02,  1.3146e-02,  1.9435e-02, -3.4328e-02,\n",
            "         -7.1729e-03,  6.4039e-02,  3.2749e-02,  3.2789e-02,  3.4340e-03,\n",
            "         -3.9483e-02, -4.1840e-03,  3.0426e-02, -1.4509e-02,  1.1931e-02,\n",
            "          7.9927e-03,  2.6900e-02, -7.5962e-02, -4.0164e-02, -7.0347e-02,\n",
            "         -2.2806e-02,  1.3883e-02, -2.2626e-02,  4.7220e-02,  2.9373e-02,\n",
            "          2.8144e-03, -2.2770e-02, -1.6023e-02,  4.5637e-02,  2.1537e-02,\n",
            "         -4.3864e-03,  1.2578e-03, -3.1152e-03, -1.5599e-02, -3.4715e-02,\n",
            "          2.0532e-02,  5.1184e-02, -1.8087e-02,  2.4515e-02,  4.4687e-02,\n",
            "          2.4086e-02, -1.6530e-02, -1.4110e-02,  2.9383e-02, -4.4137e-02,\n",
            "         -3.7096e-02, -1.7905e-02,  7.1491e-02,  6.5058e-02,  2.5365e-02,\n",
            "         -1.4328e-02, -2.6338e-02,  3.2757e-02, -3.3335e-02, -1.1690e-02,\n",
            "          9.9492e-02,  4.9675e-03,  3.2864e-02, -1.4104e-02,  4.2089e-03,\n",
            "          2.9249e-03, -5.5277e-02,  2.2614e-02,  7.1988e-04, -2.2416e-02,\n",
            "         -1.6233e-02, -7.4237e-02, -4.4718e-02,  1.5674e-02, -1.5094e-02,\n",
            "         -4.7474e-02,  3.4693e-03,  8.9242e-03,  1.6365e-02, -1.1999e-02,\n",
            "         -7.2010e-03,  5.1837e-02, -2.5001e-02, -5.0506e-02,  4.5241e-02,\n",
            "         -3.4754e-02, -1.8101e-02, -7.6906e-03, -6.4393e-02, -1.0330e-02,\n",
            "         -8.5318e-03, -2.4759e-03,  3.7058e-02,  4.0214e-02,  6.8232e-02,\n",
            "          1.9258e-02, -6.6504e-03, -3.2940e-02,  1.1053e-01,  3.9428e-02,\n",
            "         -7.6586e-03, -3.0962e-02, -1.0123e-02,  2.4275e-02, -6.9091e-03,\n",
            "          6.3197e-02,  9.9488e-03, -4.6679e-03,  2.5246e-02, -4.0511e-02,\n",
            "          2.0298e-04, -4.8285e-02,  4.1846e-02, -1.0211e-02,  1.0708e-02,\n",
            "         -1.8750e-02, -1.4020e-02,  3.6684e-02, -4.3468e-02, -2.5811e-02,\n",
            "          3.7247e-02, -2.5786e-02, -3.3226e-02,  3.9915e-02,  4.0002e-04,\n",
            "         -1.4757e-02,  5.8911e-02,  4.9498e-03,  6.6981e-03,  8.3517e-03,\n",
            "         -1.8792e-02,  1.4714e-02, -9.8289e-03, -5.7793e-03,  2.6531e-02,\n",
            "         -7.8634e-03, -3.0905e-02, -2.6996e-02, -5.7210e-02, -9.8518e-03,\n",
            "          6.2680e-02, -8.2596e-03, -5.1916e-02, -2.1851e-02, -3.6579e-02,\n",
            "         -3.3167e-03, -1.0759e-02, -4.0109e-03, -2.6161e-02, -3.6580e-02,\n",
            "         -2.2631e-02,  4.0950e-03, -1.4441e-02, -1.5955e-02, -8.6958e-02,\n",
            "          3.0772e-02,  4.4093e-02, -1.4028e-02, -8.3244e-03, -2.0725e-02,\n",
            "         -1.9661e-03,  1.0761e-02,  3.8211e-02,  4.8614e-02,  3.9723e-02,\n",
            "          7.3614e-03, -4.1282e-02, -5.2873e-03,  2.9767e-02, -2.6504e-03,\n",
            "         -2.4369e-02, -1.5389e-01, -2.6528e-02, -5.0645e-02, -3.5372e-02,\n",
            "          8.2368e-03, -1.3932e-02, -2.4392e-02,  4.8236e-02,  4.7629e-02,\n",
            "          3.3795e-02,  1.1047e-02,  2.2938e-02,  2.5921e-02, -2.9503e-02,\n",
            "         -1.5527e-02, -3.4759e-02, -1.2617e-02,  7.2944e-03, -3.9677e-02,\n",
            "         -3.8680e-03, -1.5868e-02,  1.9927e-02,  2.2202e-02,  3.6093e-02,\n",
            "          1.2022e-01,  4.2693e-02,  3.5393e-02,  1.1016e-02,  3.2095e-02,\n",
            "         -9.7248e-02,  8.9446e-03, -3.8018e-03, -8.1597e-03, -8.6520e-03,\n",
            "          1.3045e-02, -3.7466e-03, -8.7579e-03, -4.0823e-02,  4.8320e-02,\n",
            "          2.7007e-02, -2.0171e-02,  1.7123e-02,  1.8316e-02, -2.6525e-02,\n",
            "          1.7307e-02,  1.2961e-02,  4.6607e-04, -1.8694e-03,  7.3139e-03,\n",
            "         -1.2470e-01,  7.9936e-03,  7.5748e-03,  4.7677e-02, -2.5974e-02,\n",
            "          1.3720e-02, -6.1010e-03,  1.8850e-02, -2.9610e-03,  3.4788e-02,\n",
            "          3.5135e-02],\n",
            "        [-1.9056e-02,  1.8470e-02, -1.3418e-02,  3.7602e-02,  1.1680e-01,\n",
            "         -4.3126e-02, -2.3923e-02,  4.2326e-02,  1.8773e-02, -9.5438e-03,\n",
            "          4.1075e-03, -4.8883e-03, -1.1396e-02,  2.2925e-02,  1.0811e-01,\n",
            "          4.6125e-02,  1.4308e-03, -1.7215e-02, -1.3592e-02, -3.8557e-02,\n",
            "          9.8635e-02,  1.5185e-02, -4.6203e-03, -8.8332e-02, -4.2816e-02,\n",
            "         -4.4990e-02,  2.1808e-03, -6.1840e-02,  1.9332e-02, -1.0827e-02,\n",
            "          8.3843e-02, -2.8516e-02, -1.7202e-02,  8.1439e-03,  3.1441e-02,\n",
            "         -5.9160e-02,  5.3204e-04, -5.5622e-02, -4.7851e-03,  1.8884e-02,\n",
            "          2.5326e-02, -1.2227e-02, -2.8160e-02,  2.3046e-03, -3.0123e-02,\n",
            "         -1.7040e-02,  3.6956e-04,  8.3479e-02,  2.0834e-02,  1.1625e-03,\n",
            "          8.0792e-02, -9.0974e-03, -4.5243e-02, -4.1715e-02,  8.2083e-02,\n",
            "          9.1879e-02, -2.0653e-03, -9.5058e-02, -7.3444e-03,  3.2353e-02,\n",
            "          7.3069e-02, -2.0052e-02, -1.2937e-02,  9.1465e-02,  1.9376e-03,\n",
            "          5.3887e-02,  2.8530e-02, -5.7356e-02, -1.6648e-02, -7.7970e-03,\n",
            "          4.8912e-02,  3.0903e-02,  3.3246e-02,  9.1106e-02,  6.1855e-02,\n",
            "          2.9771e-02, -1.2618e-01,  8.9621e-02,  1.3826e-02, -4.7021e-02,\n",
            "         -4.6831e-02, -3.7718e-02, -6.8677e-02,  8.2526e-02,  4.9657e-04,\n",
            "         -3.8368e-02,  7.4663e-02, -3.1607e-02,  1.3384e-02,  2.4209e-02,\n",
            "         -2.2858e-02,  1.7198e-02, -4.0562e-02, -1.3197e-02, -1.7943e-02,\n",
            "         -5.3892e-02, -6.7172e-02, -2.4927e-02, -6.2665e-02, -2.7941e-02,\n",
            "         -5.2832e-02, -1.8640e-02,  1.7464e-02, -1.5401e-02,  8.4492e-03,\n",
            "         -6.6499e-04, -4.0054e-02, -2.2675e-02,  4.0969e-02, -3.7357e-03,\n",
            "          8.1173e-03,  9.2996e-03, -8.6517e-02,  3.6568e-02,  7.8379e-02,\n",
            "         -3.7780e-02, -7.9094e-02, -3.3902e-02, -5.4996e-02, -6.2025e-02,\n",
            "         -1.5527e-02, -1.5947e-02, -1.8517e-02, -4.4126e-02,  9.8260e-03,\n",
            "          1.3011e-02,  3.2947e-02,  2.9282e-02, -5.1251e-02,  4.3965e-03,\n",
            "          2.7180e-02,  3.0475e-02,  2.8670e-02, -2.5336e-02, -1.4822e-04,\n",
            "          4.6613e-03, -3.4020e-02,  1.9950e-02,  5.9407e-02,  4.6527e-02,\n",
            "         -2.3530e-02, -7.6528e-02,  9.1221e-02, -5.1192e-02, -1.2293e-02,\n",
            "         -9.9177e-03,  3.1688e-02, -2.7511e-02,  1.3447e-02,  3.1449e-03,\n",
            "         -2.0364e-03,  5.1002e-02,  2.6438e-02,  3.7510e-03, -2.9625e-02,\n",
            "          3.5890e-02,  1.6377e-02, -3.6848e-02,  7.7307e-02, -6.9824e-02,\n",
            "         -4.8823e-02, -1.9502e-02, -1.8664e-02, -6.8720e-02, -3.5906e-02,\n",
            "          3.7880e-02, -7.0807e-02, -1.0530e-02, -5.7159e-02, -1.1252e-02,\n",
            "          2.6928e-02, -3.6751e-02, -1.7208e-02, -2.6724e-02, -1.0710e-02,\n",
            "         -1.1485e-02,  9.4291e-03,  6.9549e-02,  2.0499e-02, -1.5815e-02,\n",
            "         -4.4002e-02, -1.9486e-02, -9.0944e-03, -5.8177e-02, -1.0172e-02,\n",
            "         -8.0439e-03, -6.3499e-03,  1.8926e-02, -3.6991e-02,  3.3079e-02,\n",
            "         -8.7500e-03, -2.2621e-02,  6.6142e-02, -1.0605e-01,  5.4807e-03,\n",
            "          9.5242e-03,  7.5008e-02, -1.7108e-02,  7.1256e-03, -3.9719e-02,\n",
            "          4.6997e-02, -2.0880e-02, -3.5506e-02,  2.6239e-02, -9.2901e-02,\n",
            "         -3.9938e-02, -6.6684e-03, -5.3768e-02, -1.0810e-02, -4.2155e-02,\n",
            "         -6.6192e-02,  6.1133e-02, -2.1853e-02, -6.2494e-02, -8.7383e-02,\n",
            "          3.1517e-02,  1.0074e-02,  3.7704e-02,  3.3980e-02, -8.1239e-04,\n",
            "          2.9916e-02, -2.6296e-03,  1.2261e-02,  8.3118e-02, -2.9079e-02,\n",
            "          3.7106e-02,  3.4188e-02,  2.8726e-05,  4.8409e-02, -2.9804e-03,\n",
            "         -2.2444e-02, -3.7692e-03,  7.8054e-02,  5.1159e-04,  6.2605e-02,\n",
            "         -7.9637e-02,  2.5804e-02, -2.2593e-02, -2.1593e-02,  5.0105e-02,\n",
            "         -8.2154e-02, -3.0566e-02,  5.9514e-02, -2.6307e-03, -1.2596e-02,\n",
            "         -2.1242e-02, -2.3970e-02,  3.3739e-02, -2.6539e-03, -1.1459e-02,\n",
            "         -1.7958e-02,  5.6126e-02, -1.2922e-02, -4.9573e-02, -1.5233e-02,\n",
            "          1.9900e-02],\n",
            "        [-1.2513e-02,  5.0416e-02, -3.8383e-02, -2.1470e-03, -3.8024e-03,\n",
            "         -5.3932e-02, -2.4208e-02, -4.0528e-02, -5.4011e-04,  4.5218e-02,\n",
            "         -7.2193e-03,  5.2846e-04,  1.3771e-02, -3.7745e-02, -2.1638e-02,\n",
            "         -1.7678e-02, -1.6347e-02,  1.1796e-02, -1.9896e-02, -2.1033e-02,\n",
            "          8.4175e-04, -5.1384e-03, -5.4303e-03, -3.3089e-02, -4.1184e-02,\n",
            "          5.2721e-02,  1.8479e-02, -5.2296e-02, -5.2886e-02, -1.6797e-03,\n",
            "         -6.6191e-02,  3.8613e-02, -1.8943e-02, -1.8700e-02,  7.7463e-04,\n",
            "         -3.8531e-02, -3.1630e-03,  5.4623e-02,  1.7914e-02, -4.6907e-02,\n",
            "         -6.7060e-02, -1.8708e-02, -4.4321e-02,  3.3883e-02, -2.4201e-02,\n",
            "         -4.6267e-02, -3.0082e-02,  1.0754e-02,  4.7664e-02, -2.5882e-02,\n",
            "         -1.0977e-02, -3.7372e-02, -4.0760e-02,  5.5297e-02,  9.0240e-03,\n",
            "          2.4454e-02, -5.4222e-02,  6.9160e-02,  2.2089e-02,  1.9551e-02,\n",
            "          2.2100e-02,  2.0286e-02, -2.2514e-02, -3.4501e-02, -1.2361e-02,\n",
            "          3.3636e-02,  1.0400e-02,  2.2820e-03, -3.2538e-03,  6.3591e-02,\n",
            "          1.3211e-02,  8.0890e-03,  1.1463e-02, -1.7617e-02, -5.8867e-02,\n",
            "         -5.9337e-02, -3.0684e-03,  1.8936e-02, -6.3839e-03,  1.7079e-02,\n",
            "         -5.3369e-02,  2.6263e-02,  1.7616e-03, -1.0685e-02,  7.4724e-03,\n",
            "         -1.0832e-02,  3.4418e-02,  1.7947e-02,  2.3015e-02, -4.0933e-03,\n",
            "         -9.2944e-03, -2.8106e-02,  2.0861e-02, -4.3616e-02, -5.5715e-02,\n",
            "          3.8015e-02,  2.9003e-02, -8.2356e-03, -1.3783e-02,  1.8656e-02,\n",
            "         -4.5573e-02, -4.2917e-02, -1.6061e-02, -4.8444e-02, -1.4176e-02,\n",
            "         -3.4867e-02,  2.4010e-02, -4.0014e-02,  6.7926e-04, -7.2680e-04,\n",
            "         -1.8274e-02,  2.6156e-02, -6.4048e-03,  6.9324e-04, -1.9820e-03,\n",
            "         -3.8155e-02,  1.8221e-02,  9.8476e-02, -3.0382e-03, -3.5669e-02,\n",
            "          2.0014e-02,  1.6167e-02,  7.2309e-02,  1.2602e-02, -3.1830e-02,\n",
            "         -2.1804e-02,  1.0211e-02,  4.4931e-02,  2.3719e-03, -8.5803e-03,\n",
            "         -7.4424e-02, -1.5007e-03,  6.1419e-03, -4.0282e-02, -2.4945e-03,\n",
            "          2.3031e-02,  2.0924e-02,  2.1619e-02,  3.1367e-02,  2.7811e-02,\n",
            "          2.9026e-02, -2.8934e-02,  1.6070e-02,  2.2220e-02,  1.9890e-02,\n",
            "         -7.5993e-03,  3.3466e-02,  4.6838e-02, -2.8399e-02,  1.8128e-02,\n",
            "         -3.8855e-03, -1.9200e-02,  5.2527e-02,  2.7013e-02,  2.5850e-02,\n",
            "          1.1551e-02, -3.0916e-03, -3.3787e-02,  5.3905e-03, -8.2568e-02,\n",
            "          4.7504e-02,  1.0586e-02,  2.0720e-02, -4.7961e-02, -7.4927e-03,\n",
            "          5.3057e-02, -4.4629e-02, -2.0800e-02, -6.3937e-02, -3.6765e-02,\n",
            "          3.1253e-02, -2.4522e-02,  3.3668e-02, -2.3360e-03,  3.2202e-02,\n",
            "         -3.9040e-02,  1.0326e-01,  3.4907e-02,  3.1935e-02, -8.2088e-02,\n",
            "         -2.3212e-02,  7.6248e-03, -4.2391e-02,  2.7445e-02,  2.7680e-02,\n",
            "         -2.1371e-02, -8.0740e-03,  1.0839e-02,  6.6320e-02,  1.2197e-02,\n",
            "          7.5030e-03, -3.7701e-02,  2.9238e-02, -2.2734e-02,  1.1992e-02,\n",
            "         -5.0066e-02,  4.4683e-02,  2.0919e-03,  7.7820e-02,  9.7392e-02,\n",
            "         -5.9888e-02, -1.9841e-02, -5.7808e-02, -6.2250e-02, -6.7518e-02,\n",
            "         -1.7685e-02,  4.0730e-02, -2.5703e-02,  4.3704e-02,  4.8190e-03,\n",
            "          2.4056e-02,  6.0288e-03,  6.6281e-02,  1.0972e-02, -2.9611e-02,\n",
            "         -2.9748e-02, -1.7535e-02,  3.5005e-02,  6.4773e-02,  2.3403e-02,\n",
            "          3.6109e-02,  2.3227e-02,  4.1771e-02,  2.9957e-02, -7.9454e-03,\n",
            "         -5.3370e-02, -3.4743e-02, -2.2394e-02, -3.6785e-02,  3.3018e-02,\n",
            "          2.7174e-02,  1.6979e-02,  3.1338e-02,  8.0637e-03,  7.1787e-03,\n",
            "         -4.3304e-02, -1.7666e-02,  2.4661e-02, -3.1545e-02,  2.9081e-02,\n",
            "          5.5170e-02, -4.2651e-03,  5.1239e-02, -9.6994e-03, -2.8609e-03,\n",
            "         -1.4808e-02, -4.6491e-02,  8.6035e-02,  7.4639e-02, -8.5247e-03,\n",
            "         -2.1243e-02,  2.9129e-02,  7.6641e-03,  1.5696e-02,  1.1508e-02,\n",
            "          1.1085e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.5618,  0.9748,  4.6907, -4.6071, -3.5930], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.3529,  1.0363,  4.1573, -4.9824, -3.6647], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,   616,  4100,  ...,   624,    54,   392],\n",
            "        [50256, 11092,   329,  ..., 11580,  3887,   640],\n",
            "        [50256,   338,  2581,  ..., 15136,    23,    12],\n",
            "        ...,\n",
            "        [50256,    11,   345,  ...,  1532,   345,   447],\n",
            "        [50256,    63,   198,  ...,  4382,  1912,   319],\n",
            "        [50256,  2986,   314,  ...,   554,  5599,   383]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0115,  0.0716, -0.0214,  ..., -0.0035,  0.0345,  0.0356],\n",
            "        [-0.0191,  0.0186, -0.0140,  ..., -0.0496, -0.0156,  0.0203],\n",
            "        [-0.0124,  0.0507, -0.0389,  ...,  0.0154,  0.0110,  0.1112],\n",
            "        [-0.0122,  0.0606, -0.0605,  ..., -0.0165, -0.0321,  0.0278],\n",
            "        [ 0.0591, -0.0239, -0.0306,  ..., -0.0021,  0.0231,  0.0355]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1489e-02,  7.1599e-02, -2.1397e-02, -3.3052e-02, -7.9144e-03,\n",
            "         -5.2259e-03, -1.3344e-02,  3.6576e-03, -7.1864e-02, -1.2439e-02,\n",
            "          4.5058e-03,  1.4188e-02, -3.4111e-02, -2.8760e-02, -4.1669e-02,\n",
            "         -3.1509e-02, -3.7324e-03,  2.1835e-02,  1.5898e-02,  2.2200e-02,\n",
            "         -3.0501e-02,  4.3116e-02, -1.2032e-02, -3.1216e-02, -1.4700e-02,\n",
            "          2.5091e-02, -2.9297e-02,  3.6909e-02,  8.2798e-03, -1.1148e-02,\n",
            "         -7.3372e-02,  2.3362e-02,  1.2626e-02,  2.0163e-02, -3.4607e-02,\n",
            "         -6.7566e-03,  6.4171e-02,  3.3007e-02,  3.2968e-02,  3.8485e-03,\n",
            "         -3.8951e-02, -4.4073e-03,  3.0662e-02, -1.4625e-02,  1.1846e-02,\n",
            "          7.6953e-03,  2.6657e-02, -7.6140e-02, -4.0672e-02, -7.0486e-02,\n",
            "         -2.2556e-02,  1.3718e-02, -2.2841e-02,  4.7096e-02,  2.9788e-02,\n",
            "          2.7683e-03, -2.2651e-02, -1.5865e-02,  4.5951e-02,  2.1597e-02,\n",
            "         -4.7311e-03,  1.5399e-03, -2.5254e-03, -1.5353e-02, -3.4750e-02,\n",
            "          2.0525e-02,  5.1531e-02, -1.8409e-02,  2.4961e-02,  4.5221e-02,\n",
            "          2.4064e-02, -1.6085e-02, -1.4223e-02,  2.9409e-02, -4.4094e-02,\n",
            "         -3.7013e-02, -1.8023e-02,  7.1505e-02,  6.5527e-02,  2.5419e-02,\n",
            "         -1.4701e-02, -2.6370e-02,  3.2556e-02, -3.2890e-02, -1.1792e-02,\n",
            "          9.9299e-02,  4.7629e-03,  3.2913e-02, -1.4195e-02,  4.3938e-03,\n",
            "          2.7871e-03, -5.5227e-02,  2.2886e-02,  6.5399e-04, -2.2759e-02,\n",
            "         -1.7096e-02, -7.4302e-02, -4.5035e-02,  1.5669e-02, -1.5044e-02,\n",
            "         -4.7981e-02,  3.2336e-03,  8.8766e-03,  1.6325e-02, -1.1716e-02,\n",
            "         -7.0607e-03,  5.1533e-02, -2.4773e-02, -5.0624e-02,  4.4734e-02,\n",
            "         -3.4560e-02, -1.8123e-02, -8.4123e-03, -6.4631e-02, -1.0274e-02,\n",
            "         -8.2480e-03, -1.8963e-03,  3.7487e-02,  4.0228e-02,  6.8148e-02,\n",
            "          1.9555e-02, -6.5401e-03, -3.2798e-02,  1.1029e-01,  3.9865e-02,\n",
            "         -7.8426e-03, -3.0864e-02, -1.0020e-02,  2.4593e-02, -6.7762e-03,\n",
            "          6.3289e-02,  9.9836e-03, -4.8578e-03,  2.5680e-02, -4.0785e-02,\n",
            "          1.2861e-04, -4.8652e-02,  4.1780e-02, -1.0059e-02,  1.1424e-02,\n",
            "         -1.8956e-02, -1.4020e-02,  3.6352e-02, -4.3212e-02, -2.6334e-02,\n",
            "          3.7442e-02, -2.6565e-02, -3.3333e-02,  4.0152e-02,  2.9589e-04,\n",
            "         -1.5211e-02,  5.8603e-02,  5.3261e-03,  6.5403e-03,  8.4017e-03,\n",
            "         -1.8900e-02,  1.5033e-02, -9.7154e-03, -5.6321e-03,  2.6230e-02,\n",
            "         -8.0368e-03, -3.0755e-02, -2.7189e-02, -5.7336e-02, -9.6143e-03,\n",
            "          6.3404e-02, -8.1481e-03, -5.2495e-02, -2.2037e-02, -3.6426e-02,\n",
            "         -3.5672e-03, -1.0915e-02, -4.0550e-03, -2.6623e-02, -3.6902e-02,\n",
            "         -2.2632e-02,  4.1258e-03, -1.4728e-02, -1.5993e-02, -8.6799e-02,\n",
            "          3.1049e-02,  4.3810e-02, -1.4669e-02, -8.3638e-03, -2.1363e-02,\n",
            "         -1.4454e-03,  1.0937e-02,  3.8317e-02,  4.8781e-02,  3.9969e-02,\n",
            "          7.0961e-03, -4.1173e-02, -5.0082e-03,  2.9765e-02, -3.0747e-03,\n",
            "         -2.3792e-02, -1.5388e-01, -2.6629e-02, -5.0344e-02, -3.5444e-02,\n",
            "          8.0478e-03, -1.4381e-02, -2.4001e-02,  4.8169e-02,  4.7539e-02,\n",
            "          3.4048e-02,  1.1425e-02,  2.3004e-02,  2.5804e-02, -2.9098e-02,\n",
            "         -1.5767e-02, -3.5054e-02, -1.3215e-02,  7.5425e-03, -3.9891e-02,\n",
            "         -3.7535e-03, -1.5732e-02,  2.0314e-02,  2.2303e-02,  3.6029e-02,\n",
            "          1.1981e-01,  4.2419e-02,  3.5293e-02,  1.0312e-02,  3.2287e-02,\n",
            "         -9.6738e-02,  9.1324e-03, -4.2176e-03, -7.2886e-03, -8.5002e-03,\n",
            "          1.3008e-02, -4.0295e-03, -8.3825e-03, -4.1088e-02,  4.7802e-02,\n",
            "          2.7336e-02, -2.0014e-02,  1.7358e-02,  1.8614e-02, -2.6427e-02,\n",
            "          1.7686e-02,  1.2319e-02,  6.5587e-04, -1.3304e-03,  7.2548e-03,\n",
            "         -1.2511e-01,  7.6633e-03,  7.6087e-03,  4.7783e-02, -2.6153e-02,\n",
            "          1.3400e-02, -6.1567e-03,  1.8646e-02, -3.4709e-03,  3.4461e-02,\n",
            "          3.5584e-02],\n",
            "        [-1.9095e-02,  1.8588e-02, -1.3961e-02,  3.8038e-02,  1.1677e-01,\n",
            "         -4.3000e-02, -2.4316e-02,  4.2444e-02,  1.8768e-02, -9.6592e-03,\n",
            "          3.8651e-03, -4.8383e-03, -1.1862e-02,  2.2506e-02,  1.0817e-01,\n",
            "          4.6367e-02,  1.3683e-03, -1.7231e-02, -1.3641e-02, -3.8714e-02,\n",
            "          9.8666e-02,  1.4692e-02, -4.5878e-03, -8.8347e-02, -4.2792e-02,\n",
            "         -4.5019e-02,  2.0558e-03, -6.1628e-02,  1.9216e-02, -1.1111e-02,\n",
            "          8.3871e-02, -2.8178e-02, -1.6933e-02,  8.1478e-03,  3.1606e-02,\n",
            "         -5.8718e-02,  4.1100e-04, -5.5528e-02, -4.7627e-03,  1.8992e-02,\n",
            "          2.5326e-02, -1.2564e-02, -2.7918e-02,  2.0745e-03, -3.0050e-02,\n",
            "         -1.7600e-02,  4.2812e-04,  8.3449e-02,  2.0537e-02,  1.1173e-03,\n",
            "          8.0888e-02, -8.9820e-03, -4.5553e-02, -4.2101e-02,  8.2103e-02,\n",
            "          9.2049e-02, -1.9937e-03, -9.5206e-02, -7.3892e-03,  3.2787e-02,\n",
            "          7.2981e-02, -2.0054e-02, -1.2793e-02,  9.1555e-02,  2.0827e-03,\n",
            "          5.3906e-02,  2.8733e-02, -5.7408e-02, -1.6252e-02, -7.2435e-03,\n",
            "          4.8958e-02,  3.0969e-02,  3.2948e-02,  9.1269e-02,  6.1829e-02,\n",
            "          2.9766e-02, -1.2642e-01,  8.9567e-02,  1.3722e-02, -4.7000e-02,\n",
            "         -4.6884e-02, -3.7843e-02, -6.8714e-02,  8.3031e-02,  2.3524e-04,\n",
            "         -3.8499e-02,  7.4706e-02, -3.1516e-02,  1.3750e-02,  2.4354e-02,\n",
            "         -2.2618e-02,  1.7290e-02, -4.0539e-02, -1.3166e-02, -1.8024e-02,\n",
            "         -5.3964e-02, -6.7420e-02, -2.4815e-02, -6.2742e-02, -2.8133e-02,\n",
            "         -5.2969e-02, -1.8593e-02,  1.7111e-02, -1.5430e-02,  8.7176e-03,\n",
            "         -4.2343e-04, -4.0339e-02, -2.2839e-02,  4.1087e-02, -4.1207e-03,\n",
            "          8.2572e-03,  9.2535e-03, -8.6569e-02,  3.6376e-02,  7.8701e-02,\n",
            "         -3.7813e-02, -7.8823e-02, -3.3789e-02, -5.4780e-02, -6.2194e-02,\n",
            "         -1.5439e-02, -1.5997e-02, -1.8208e-02, -4.4470e-02,  9.9331e-03,\n",
            "          1.2936e-02,  3.3317e-02,  2.9330e-02, -5.1427e-02,  4.3845e-03,\n",
            "          2.7230e-02,  3.0679e-02,  2.8748e-02, -2.5328e-02, -3.5414e-04,\n",
            "          4.6908e-03, -3.4017e-02,  1.9858e-02,  5.9497e-02,  4.6741e-02,\n",
            "         -2.3455e-02, -7.6527e-02,  9.1263e-02, -5.1381e-02, -1.2519e-02,\n",
            "         -9.9273e-03,  3.1508e-02, -2.7711e-02,  1.3385e-02,  3.0261e-03,\n",
            "         -2.2713e-03,  5.0643e-02,  2.6966e-02,  3.7124e-03, -2.9355e-02,\n",
            "          3.5705e-02,  1.6686e-02, -3.6760e-02,  7.7240e-02, -6.9952e-02,\n",
            "         -4.8903e-02, -1.9384e-02, -1.8705e-02, -6.8681e-02, -3.5434e-02,\n",
            "          3.7882e-02, -7.0969e-02, -1.0796e-02, -5.7202e-02, -1.0934e-02,\n",
            "          2.6644e-02, -3.6694e-02, -1.6999e-02, -2.6866e-02, -1.0660e-02,\n",
            "         -1.1443e-02,  9.5689e-03,  6.9710e-02,  2.0701e-02, -1.5669e-02,\n",
            "         -4.3874e-02, -1.9583e-02, -9.3200e-03, -5.8232e-02, -1.0259e-02,\n",
            "         -7.7157e-03, -6.3133e-03,  1.9086e-02, -3.6886e-02,  3.3279e-02,\n",
            "         -9.2135e-03, -2.2456e-02,  6.6416e-02, -1.0590e-01,  5.2104e-03,\n",
            "          9.8226e-03,  7.5360e-02, -1.7326e-02,  7.0518e-03, -3.9903e-02,\n",
            "          4.6917e-02, -2.1100e-02, -3.5322e-02,  2.6337e-02, -9.2731e-02,\n",
            "         -4.0152e-02, -6.3447e-03, -5.3928e-02, -1.0681e-02, -4.2127e-02,\n",
            "         -6.6248e-02,  6.0886e-02, -2.2026e-02, -6.2414e-02, -8.7398e-02,\n",
            "          3.1468e-02,  1.0414e-02,  3.8044e-02,  3.4015e-02, -6.9855e-04,\n",
            "          2.9855e-02, -2.7490e-03,  1.2270e-02,  8.2890e-02, -2.8696e-02,\n",
            "          3.7245e-02,  3.4038e-02,  1.4830e-04,  4.8470e-02, -2.9663e-03,\n",
            "         -2.2241e-02, -3.9684e-03,  7.8202e-02,  2.1567e-04,  6.2546e-02,\n",
            "         -7.9817e-02,  2.5979e-02, -2.2522e-02, -2.1162e-02,  5.0100e-02,\n",
            "         -8.2043e-02, -3.1034e-02,  5.9366e-02, -2.6834e-03, -1.2800e-02,\n",
            "         -2.1403e-02, -2.3967e-02,  3.4073e-02, -2.7055e-03, -1.1480e-02,\n",
            "         -1.8186e-02,  5.5602e-02, -1.3198e-02, -4.9613e-02, -1.5639e-02,\n",
            "          2.0323e-02],\n",
            "        [-1.2369e-02,  5.0688e-02, -3.8933e-02, -1.8716e-03, -3.9312e-03,\n",
            "         -5.3304e-02, -2.4625e-02, -4.0391e-02, -5.1307e-04,  4.5307e-02,\n",
            "         -7.0180e-03,  4.3056e-04,  1.3159e-02, -3.8092e-02, -2.1616e-02,\n",
            "         -1.7382e-02, -1.6474e-02,  1.1924e-02, -1.9956e-02, -2.1110e-02,\n",
            "          7.4181e-04, -5.5172e-03, -4.9705e-03, -3.2758e-02, -4.1229e-02,\n",
            "          5.2530e-02,  1.8197e-02, -5.2119e-02, -5.3157e-02, -1.7562e-03,\n",
            "         -6.6035e-02,  3.9276e-02, -1.8885e-02, -1.8846e-02,  7.5671e-04,\n",
            "         -3.8128e-02, -3.5010e-03,  5.4645e-02,  1.7888e-02, -4.6755e-02,\n",
            "         -6.6867e-02, -1.8864e-02, -4.4140e-02,  3.3634e-02, -2.4173e-02,\n",
            "         -4.6843e-02, -2.9853e-02,  1.0494e-02,  4.7248e-02, -2.5926e-02,\n",
            "         -1.0972e-02, -3.7179e-02, -4.0924e-02,  5.5076e-02,  9.1216e-03,\n",
            "          2.4291e-02, -5.4177e-02,  6.8862e-02,  2.2300e-02,  1.9829e-02,\n",
            "          2.1657e-02,  2.0422e-02, -2.2236e-02, -3.4802e-02, -1.2370e-02,\n",
            "          3.3542e-02,  1.0422e-02,  2.9250e-03, -2.8627e-03,  6.4032e-02,\n",
            "          1.3034e-02,  8.2979e-03,  1.0993e-02, -1.7715e-02, -5.9003e-02,\n",
            "         -5.9340e-02, -2.7746e-03,  1.8751e-02, -6.5883e-03,  1.7484e-02,\n",
            "         -5.3150e-02,  2.6052e-02,  1.8296e-03, -1.0662e-02,  7.2834e-03,\n",
            "         -1.0685e-02,  3.4350e-02,  1.8104e-02,  2.3227e-02, -3.8782e-03,\n",
            "         -9.2934e-03, -2.8231e-02,  2.1039e-02, -4.3734e-02, -5.5986e-02,\n",
            "          3.8030e-02,  2.8593e-02, -7.8467e-03, -1.3562e-02,  1.8293e-02,\n",
            "         -4.5837e-02, -4.2849e-02, -1.6414e-02, -4.8324e-02, -1.3762e-02,\n",
            "         -3.4289e-02,  2.3622e-02, -3.9881e-02,  9.7422e-04, -1.1073e-03,\n",
            "         -1.8110e-02,  2.6174e-02, -6.3069e-03,  1.8784e-04, -1.9448e-03,\n",
            "         -3.8301e-02,  1.8593e-02,  9.8404e-02, -2.9653e-03, -3.5586e-02,\n",
            "          1.9887e-02,  1.6201e-02,  7.2496e-02,  1.2206e-02, -3.1436e-02,\n",
            "         -2.2036e-02,  1.0597e-02,  4.4966e-02,  2.4751e-03, -8.9043e-03,\n",
            "         -7.4316e-02, -1.3616e-03,  5.7406e-03, -4.0071e-02, -2.4847e-03,\n",
            "          2.3283e-02,  2.0857e-02,  2.1401e-02,  3.1309e-02,  2.7980e-02,\n",
            "          2.9085e-02, -2.8581e-02,  1.5912e-02,  2.2162e-02,  1.9520e-02,\n",
            "         -7.4283e-03,  3.3200e-02,  4.6890e-02, -2.8262e-02,  1.8096e-02,\n",
            "         -4.3668e-03, -1.9622e-02,  5.3179e-02,  2.6776e-02,  2.6018e-02,\n",
            "          1.1183e-02, -2.8440e-03, -3.3601e-02,  5.3667e-03, -8.3028e-02,\n",
            "          4.7765e-02,  1.0826e-02,  2.1099e-02, -4.7716e-02, -7.0278e-03,\n",
            "          5.2977e-02, -4.4715e-02, -2.1159e-02, -6.4114e-02, -3.6729e-02,\n",
            "          3.0894e-02, -2.4518e-02,  3.3482e-02, -2.3812e-03,  3.2264e-02,\n",
            "         -3.8489e-02,  1.0350e-01,  3.5228e-02,  3.2133e-02, -8.2214e-02,\n",
            "         -2.2927e-02,  7.4388e-03, -4.2566e-02,  2.7386e-02,  2.7400e-02,\n",
            "         -2.0789e-02, -7.5203e-03,  1.1219e-02,  6.6408e-02,  1.2475e-02,\n",
            "          6.9696e-03, -3.7464e-02,  2.9513e-02, -2.2336e-02,  1.2063e-02,\n",
            "         -4.9828e-02,  4.4669e-02,  2.1038e-03,  7.7819e-02,  9.7267e-02,\n",
            "         -6.0182e-02, -2.0090e-02, -5.7481e-02, -6.2025e-02, -6.7273e-02,\n",
            "         -1.7918e-02,  4.1024e-02, -2.5754e-02,  4.3543e-02,  4.5838e-03,\n",
            "          2.4082e-02,  5.7036e-03,  6.6011e-02,  1.0974e-02, -2.9461e-02,\n",
            "         -3.0099e-02, -1.7284e-02,  3.5173e-02,  6.4804e-02,  2.3882e-02,\n",
            "          3.5875e-02,  2.3258e-02,  4.1541e-02,  2.9638e-02, -7.5012e-03,\n",
            "         -5.3285e-02, -3.5051e-02, -2.2434e-02, -3.6734e-02,  3.3243e-02,\n",
            "          2.7551e-02,  1.6563e-02,  3.1553e-02,  7.7630e-03,  6.8046e-03,\n",
            "         -4.3344e-02, -1.7448e-02,  2.4850e-02, -3.1335e-02,  2.8727e-02,\n",
            "          5.5630e-02, -4.4488e-03,  5.1119e-02, -9.8570e-03, -3.1136e-03,\n",
            "         -1.4900e-02, -4.6755e-02,  8.6365e-02,  7.4264e-02, -8.6374e-03,\n",
            "         -2.1114e-02,  2.8424e-02,  7.4131e-03,  1.5386e-02,  1.0981e-02,\n",
            "          1.1120e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.6268,  1.2430,  4.7918, -4.8243, -3.6341], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.3353,  1.4355,  4.2870, -5.3789, -3.7507], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,   286,  6727,  ...,    11,  1936,   319],\n",
            "        [50256,  1391,    62,  ...,  3237, 31431,   284],\n",
            "        [50256,   503,   286,  ...,   526,   366,  5492],\n",
            "        ...,\n",
            "        [50256,   220,   220,  ...,   220,   220,   220],\n",
            "        [50256, 12417,  1776,  ...,   198,   220,   220],\n",
            "        [50256,   262,  3321,  ...,  1107,  1263,  3356]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0118,  0.0715, -0.0219,  ..., -0.0041,  0.0342,  0.0360],\n",
            "        [-0.0191,  0.0186, -0.0144,  ..., -0.0498, -0.0160,  0.0207],\n",
            "        [-0.0123,  0.0510, -0.0393,  ...,  0.0151,  0.0104,  0.1116],\n",
            "        [-0.0123,  0.0609, -0.0608,  ..., -0.0166, -0.0323,  0.0281],\n",
            "        [ 0.0590, -0.0239, -0.0306,  ..., -0.0020,  0.0231,  0.0356]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1805e-02,  7.1537e-02, -2.1858e-02, -3.2683e-02, -7.6463e-03,\n",
            "         -5.2132e-03, -1.3820e-02,  3.7017e-03, -7.2218e-02, -1.2460e-02,\n",
            "          4.4921e-03,  1.4159e-02, -3.4216e-02, -2.8636e-02, -4.1713e-02,\n",
            "         -3.1131e-02, -3.5753e-03,  2.2231e-02,  1.6048e-02,  2.2456e-02,\n",
            "         -3.0710e-02,  4.3039e-02, -1.1934e-02, -3.1026e-02, -1.4697e-02,\n",
            "          2.4783e-02, -2.9451e-02,  3.7076e-02,  8.5207e-03, -1.1400e-02,\n",
            "         -7.3261e-02,  2.3488e-02,  1.2050e-02,  2.0754e-02, -3.4799e-02,\n",
            "         -6.3989e-03,  6.4177e-02,  3.3197e-02,  3.3127e-02,  4.2321e-03,\n",
            "         -3.8571e-02, -4.5204e-03,  3.0867e-02, -1.4628e-02,  1.1682e-02,\n",
            "          7.5939e-03,  2.6503e-02, -7.6336e-02, -4.1089e-02, -7.0582e-02,\n",
            "         -2.2312e-02,  1.3607e-02, -2.3022e-02,  4.6935e-02,  3.0048e-02,\n",
            "          2.7084e-03, -2.2719e-02, -1.5782e-02,  4.6168e-02,  2.1585e-02,\n",
            "         -5.1013e-03,  1.7925e-03, -2.0450e-03, -1.5194e-02, -3.4907e-02,\n",
            "          2.0591e-02,  5.1877e-02, -1.8826e-02,  2.5287e-02,  4.5599e-02,\n",
            "          2.4067e-02, -1.5678e-02, -1.4268e-02,  2.9379e-02, -4.4122e-02,\n",
            "         -3.6811e-02, -1.8181e-02,  7.1571e-02,  6.5973e-02,  2.5471e-02,\n",
            "         -1.4903e-02, -2.6484e-02,  3.2400e-02, -3.2535e-02, -1.1887e-02,\n",
            "          9.9098e-02,  4.6534e-03,  3.2937e-02, -1.4382e-02,  4.4871e-03,\n",
            "          2.7453e-03, -5.5191e-02,  2.3202e-02,  5.2314e-04, -2.2988e-02,\n",
            "         -1.7861e-02, -7.4180e-02, -4.5482e-02,  1.5599e-02, -1.4898e-02,\n",
            "         -4.8457e-02,  3.0854e-03,  8.9885e-03,  1.6329e-02, -1.1527e-02,\n",
            "         -7.0800e-03,  5.1333e-02, -2.4542e-02, -5.0795e-02,  4.4367e-02,\n",
            "         -3.4409e-02, -1.8037e-02, -9.1087e-03, -6.4800e-02, -1.0210e-02,\n",
            "         -7.9318e-03, -1.3099e-03,  3.7756e-02,  4.0249e-02,  6.8123e-02,\n",
            "          1.9701e-02, -6.3433e-03, -3.2833e-02,  1.1023e-01,  4.0277e-02,\n",
            "         -8.0867e-03, -3.0826e-02, -9.7868e-03,  2.4833e-02, -6.7466e-03,\n",
            "          6.3463e-02,  9.9692e-03, -4.9540e-03,  2.5954e-02, -4.0955e-02,\n",
            "          8.2007e-05, -4.9055e-02,  4.1653e-02, -9.8513e-03,  1.1986e-02,\n",
            "         -1.9096e-02, -1.3973e-02,  3.6044e-02, -4.3067e-02, -2.6936e-02,\n",
            "          3.7589e-02, -2.7285e-02, -3.3309e-02,  4.0355e-02,  7.7755e-05,\n",
            "         -1.5517e-02,  5.8304e-02,  5.6654e-03,  6.3196e-03,  8.4378e-03,\n",
            "         -1.9006e-02,  1.5148e-02, -9.5727e-03, -5.4927e-03,  2.5912e-02,\n",
            "         -8.1907e-03, -3.0634e-02, -2.7401e-02, -5.7532e-02, -9.5163e-03,\n",
            "          6.3990e-02, -7.9449e-03, -5.2949e-02, -2.2128e-02, -3.6346e-02,\n",
            "         -3.7032e-03, -1.0982e-02, -4.2498e-03, -2.7017e-02, -3.7219e-02,\n",
            "         -2.2664e-02,  4.2198e-03, -1.4900e-02, -1.5929e-02, -8.6724e-02,\n",
            "          3.1313e-02,  4.3627e-02, -1.5371e-02, -8.2757e-03, -2.1825e-02,\n",
            "         -1.0228e-03,  1.1044e-02,  3.8497e-02,  4.8952e-02,  4.0162e-02,\n",
            "          6.8494e-03, -4.1167e-02, -4.8316e-03,  2.9667e-02, -3.3787e-03,\n",
            "         -2.3234e-02, -1.5386e-01, -2.6600e-02, -5.0088e-02, -3.5548e-02,\n",
            "          7.8792e-03, -1.4670e-02, -2.3742e-02,  4.8120e-02,  4.7407e-02,\n",
            "          3.4336e-02,  1.1637e-02,  2.3100e-02,  2.5650e-02, -2.8865e-02,\n",
            "         -1.5911e-02, -3.5240e-02, -1.3706e-02,  7.7655e-03, -3.9959e-02,\n",
            "         -3.6429e-03, -1.5614e-02,  2.0619e-02,  2.2482e-02,  3.5996e-02,\n",
            "          1.1949e-01,  4.2305e-02,  3.5180e-02,  9.6757e-03,  3.2314e-02,\n",
            "         -9.6296e-02,  9.3688e-03, -4.6894e-03, -6.4217e-03, -8.3270e-03,\n",
            "          1.2859e-02, -4.2464e-03, -8.1350e-03, -4.1313e-02,  4.7366e-02,\n",
            "          2.7584e-02, -1.9880e-02,  1.7620e-02,  1.8858e-02, -2.6440e-02,\n",
            "          1.8114e-02,  1.1819e-02,  8.2555e-04, -9.0046e-04,  7.3461e-03,\n",
            "         -1.2541e-01,  7.2342e-03,  7.5919e-03,  4.7892e-02, -2.6260e-02,\n",
            "          1.3231e-02, -6.1212e-03,  1.8335e-02, -4.0525e-03,  3.4204e-02,\n",
            "          3.5951e-02],\n",
            "        [-1.9076e-02,  1.8577e-02, -1.4369e-02,  3.8506e-02,  1.1675e-01,\n",
            "         -4.2865e-02, -2.4702e-02,  4.2511e-02,  1.8761e-02, -9.8701e-03,\n",
            "          3.6668e-03, -4.9512e-03, -1.2309e-02,  2.2151e-02,  1.0814e-01,\n",
            "          4.6587e-02,  1.3009e-03, -1.7229e-02, -1.3457e-02, -3.8850e-02,\n",
            "          9.8703e-02,  1.4216e-02, -4.5916e-03, -8.8266e-02, -4.2746e-02,\n",
            "         -4.5028e-02,  1.8605e-03, -6.1426e-02,  1.9188e-02, -1.1320e-02,\n",
            "          8.3793e-02, -2.7827e-02, -1.6749e-02,  8.0811e-03,  3.1932e-02,\n",
            "         -5.8291e-02,  2.5343e-04, -5.5557e-02, -4.8475e-03,  1.9183e-02,\n",
            "          2.5269e-02, -1.2784e-02, -2.7668e-02,  2.0607e-03, -3.0063e-02,\n",
            "         -1.7932e-02,  4.2557e-04,  8.3395e-02,  2.0203e-02,  9.4010e-04,\n",
            "          8.1036e-02, -8.8820e-03, -4.5661e-02, -4.2544e-02,  8.2029e-02,\n",
            "          9.2097e-02, -2.0256e-03, -9.5388e-02, -7.4258e-03,  3.3043e-02,\n",
            "          7.2924e-02, -2.0015e-02, -1.2595e-02,  9.1679e-02,  2.1816e-03,\n",
            "          5.4024e-02,  2.8833e-02, -5.7348e-02, -1.5919e-02, -6.8641e-03,\n",
            "          4.9089e-02,  3.1015e-02,  3.2728e-02,  9.1338e-02,  6.1713e-02,\n",
            "          2.9786e-02, -1.2669e-01,  8.9513e-02,  1.3737e-02, -4.6931e-02,\n",
            "         -4.6884e-02, -3.7973e-02, -6.8748e-02,  8.3428e-02, -5.2648e-05,\n",
            "         -3.8548e-02,  7.4837e-02, -3.1509e-02,  1.4077e-02,  2.4498e-02,\n",
            "         -2.2314e-02,  1.7463e-02, -4.0539e-02, -1.3117e-02, -1.8017e-02,\n",
            "         -5.4038e-02, -6.7659e-02, -2.4700e-02, -6.2845e-02, -2.8225e-02,\n",
            "         -5.3135e-02, -1.8552e-02,  1.6799e-02, -1.5490e-02,  8.9212e-03,\n",
            "         -2.0735e-04, -4.0593e-02, -2.2940e-02,  4.1251e-02, -4.4971e-03,\n",
            "          8.2834e-03,  9.2447e-03, -8.6498e-02,  3.6235e-02,  7.8910e-02,\n",
            "         -3.7867e-02, -7.8565e-02, -3.3912e-02, -5.4649e-02, -6.2316e-02,\n",
            "         -1.5439e-02, -1.5900e-02, -1.7970e-02, -4.4702e-02,  1.0131e-02,\n",
            "          1.2765e-02,  3.3582e-02,  2.9465e-02, -5.1493e-02,  4.3555e-03,\n",
            "          2.7328e-02,  3.0832e-02,  2.8803e-02, -2.5299e-02, -4.5563e-04,\n",
            "          4.8529e-03, -3.4172e-02,  1.9798e-02,  5.9715e-02,  4.6886e-02,\n",
            "         -2.3458e-02, -7.6504e-02,  9.1246e-02, -5.1525e-02, -1.2804e-02,\n",
            "         -9.8764e-03,  3.1307e-02, -2.7994e-02,  1.3180e-02,  2.8220e-03,\n",
            "         -2.4529e-03,  5.0294e-02,  2.7551e-02,  3.4921e-03, -2.9053e-02,\n",
            "          3.5529e-02,  1.7015e-02, -3.6703e-02,  7.7226e-02, -7.0198e-02,\n",
            "         -4.8921e-02, -1.9301e-02, -1.8714e-02, -6.8630e-02, -3.5115e-02,\n",
            "          3.7864e-02, -7.1173e-02, -1.0958e-02, -5.7235e-02, -1.0767e-02,\n",
            "          2.6454e-02, -3.6682e-02, -1.6957e-02, -2.7061e-02, -1.0670e-02,\n",
            "         -1.1366e-02,  9.6608e-03,  6.9882e-02,  2.0913e-02, -1.5604e-02,\n",
            "         -4.3679e-02, -1.9589e-02, -9.6383e-03, -5.8206e-02, -1.0361e-02,\n",
            "         -7.3685e-03, -6.2676e-03,  1.9294e-02, -3.6798e-02,  3.3425e-02,\n",
            "         -9.6192e-03, -2.2354e-02,  6.6675e-02, -1.0587e-01,  5.0523e-03,\n",
            "          1.0111e-02,  7.5499e-02, -1.7427e-02,  6.9062e-03, -4.0214e-02,\n",
            "          4.6872e-02, -2.1270e-02, -3.5205e-02,  2.6631e-02, -9.2540e-02,\n",
            "         -4.0339e-02, -6.0286e-03, -5.3936e-02, -1.0566e-02, -4.2073e-02,\n",
            "         -6.6284e-02,  6.0709e-02, -2.2187e-02, -6.2303e-02, -8.7336e-02,\n",
            "          3.1325e-02,  1.0634e-02,  3.8279e-02,  3.4065e-02, -5.9999e-04,\n",
            "          2.9811e-02, -2.8432e-03,  1.2343e-02,  8.2690e-02, -2.8372e-02,\n",
            "          3.7394e-02,  3.3989e-02,  2.2964e-04,  4.8599e-02, -2.9017e-03,\n",
            "         -2.2072e-02, -4.1146e-03,  7.8321e-02,  1.1929e-05,  6.2515e-02,\n",
            "         -7.9984e-02,  2.6083e-02, -2.2424e-02, -2.0778e-02,  4.9961e-02,\n",
            "         -8.1895e-02, -3.1415e-02,  5.9251e-02, -2.7440e-03, -1.2829e-02,\n",
            "         -2.1600e-02, -2.3996e-02,  3.4438e-02, -2.7325e-03, -1.1451e-02,\n",
            "         -1.8309e-02,  5.5123e-02, -1.3476e-02, -4.9773e-02, -1.6041e-02,\n",
            "          2.0667e-02],\n",
            "        [-1.2332e-02,  5.1036e-02, -3.9350e-02, -1.7057e-03, -3.9260e-03,\n",
            "         -5.2622e-02, -2.4940e-02, -4.0350e-02, -4.4574e-04,  4.5370e-02,\n",
            "         -6.6050e-03,  1.3971e-04,  1.2628e-02, -3.8384e-02, -2.1711e-02,\n",
            "         -1.7103e-02, -1.6765e-02,  1.1911e-02, -2.0019e-02, -2.1185e-02,\n",
            "          6.6139e-04, -5.9263e-03, -4.4907e-03, -3.2480e-02, -4.1316e-02,\n",
            "          5.2426e-02,  1.7849e-02, -5.2094e-02, -5.3632e-02, -1.7249e-03,\n",
            "         -6.5947e-02,  4.0233e-02, -1.8724e-02, -1.9054e-02,  7.0835e-04,\n",
            "         -3.7744e-02, -3.9213e-03,  5.4704e-02,  1.7806e-02, -4.6796e-02,\n",
            "         -6.6687e-02, -1.8971e-02, -4.4087e-02,  3.3433e-02, -2.4086e-02,\n",
            "         -4.7349e-02, -2.9444e-02,  1.0178e-02,  4.6870e-02, -2.5945e-02,\n",
            "         -1.1061e-02, -3.7013e-02, -4.1137e-02,  5.4846e-02,  9.1620e-03,\n",
            "          2.4182e-02, -5.4226e-02,  6.8565e-02,  2.2515e-02,  2.0180e-02,\n",
            "          2.1171e-02,  2.0501e-02, -2.2089e-02, -3.5245e-02, -1.2413e-02,\n",
            "          3.3589e-02,  1.0369e-02,  3.6792e-03, -2.5751e-03,  6.4403e-02,\n",
            "          1.2759e-02,  8.3291e-03,  1.0471e-02, -1.7889e-02, -5.9137e-02,\n",
            "         -5.9362e-02, -2.4343e-03,  1.8733e-02, -6.8166e-03,  1.7966e-02,\n",
            "         -5.2837e-02,  2.5845e-02,  1.9052e-03, -1.0644e-02,  7.0798e-03,\n",
            "         -1.0424e-02,  3.4315e-02,  1.8227e-02,  2.3419e-02, -3.6800e-03,\n",
            "         -9.2864e-03, -2.8359e-02,  2.1218e-02, -4.3859e-02, -5.6149e-02,\n",
            "          3.8160e-02,  2.8183e-02, -7.3817e-03, -1.3312e-02,  1.7913e-02,\n",
            "         -4.6075e-02, -4.2803e-02, -1.6747e-02, -4.8119e-02, -1.3390e-02,\n",
            "         -3.3796e-02,  2.3330e-02, -3.9828e-02,  1.3015e-03, -1.3278e-03,\n",
            "         -1.8016e-02,  2.6166e-02, -6.0569e-03, -3.3136e-04, -1.8697e-03,\n",
            "         -3.8355e-02,  1.8930e-02,  9.8332e-02, -2.7984e-03, -3.5475e-02,\n",
            "          1.9590e-02,  1.6301e-02,  7.2683e-02,  1.1911e-02, -3.1038e-02,\n",
            "         -2.2326e-02,  1.0952e-02,  4.5029e-02,  2.5814e-03, -9.4282e-03,\n",
            "         -7.4230e-02, -1.2408e-03,  5.1997e-03, -3.9921e-02, -2.4481e-03,\n",
            "          2.3612e-02,  2.0667e-02,  2.1288e-02,  3.1326e-02,  2.8018e-02,\n",
            "          2.9119e-02, -2.8165e-02,  1.5881e-02,  2.2056e-02,  1.9222e-02,\n",
            "         -7.3358e-03,  3.3019e-02,  4.7034e-02, -2.8100e-02,  1.8056e-02,\n",
            "         -4.9414e-03, -2.0098e-02,  5.3909e-02,  2.6646e-02,  2.6189e-02,\n",
            "          1.0842e-02, -2.6643e-03, -3.3520e-02,  5.4077e-03, -8.3629e-02,\n",
            "          4.8121e-02,  1.1092e-02,  2.1447e-02, -4.7537e-02, -6.5938e-03,\n",
            "          5.2960e-02, -4.4817e-02, -2.1425e-02, -6.4275e-02, -3.6841e-02,\n",
            "          3.0646e-02, -2.4506e-02,  3.3279e-02, -2.4102e-03,  3.2296e-02,\n",
            "         -3.7856e-02,  1.0382e-01,  3.5670e-02,  3.2338e-02, -8.2528e-02,\n",
            "         -2.2672e-02,  7.4299e-03, -4.2635e-02,  2.7320e-02,  2.7279e-02,\n",
            "         -2.0390e-02, -6.9505e-03,  1.1577e-02,  6.6484e-02,  1.2772e-02,\n",
            "          6.4727e-03, -3.7430e-02,  2.9782e-02, -2.1895e-02,  1.2161e-02,\n",
            "         -4.9648e-02,  4.4604e-02,  2.1733e-03,  7.7900e-02,  9.7239e-02,\n",
            "         -6.0480e-02, -2.0316e-02, -5.7205e-02, -6.1696e-02, -6.7022e-02,\n",
            "         -1.8148e-02,  4.1112e-02, -2.5686e-02,  4.3502e-02,  4.1989e-03,\n",
            "          2.4066e-02,  5.3478e-03,  6.5866e-02,  1.0940e-02, -2.9442e-02,\n",
            "         -3.0610e-02, -1.7110e-02,  3.5210e-02,  6.4782e-02,  2.4437e-02,\n",
            "          3.5790e-02,  2.3452e-02,  4.1359e-02,  2.9516e-02, -7.0302e-03,\n",
            "         -5.3280e-02, -3.5390e-02, -2.2466e-02, -3.6854e-02,  3.3505e-02,\n",
            "          2.7990e-02,  1.6289e-02,  3.1615e-02,  7.5239e-03,  6.6194e-03,\n",
            "         -4.3557e-02, -1.7313e-02,  2.4984e-02, -3.1150e-02,  2.8346e-02,\n",
            "          5.6198e-02, -4.5822e-03,  5.0882e-02, -1.0075e-02, -3.5217e-03,\n",
            "         -1.4957e-02, -4.7057e-02,  8.6798e-02,  7.3802e-02, -8.6177e-03,\n",
            "         -2.0910e-02,  2.7557e-02,  7.2207e-03,  1.5078e-02,  1.0393e-02,\n",
            "          1.1156e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.6414,  1.4457,  4.9168, -5.0126, -3.6597], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.3361,  1.6925,  4.4237, -5.6771, -3.7811], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,   784,  1243,  ...,  2332,  6487,   373],\n",
            "        [50256,   220,   402,  ..., 46578,  9594,    13],\n",
            "        [50256, 13122,  2816,  ...,    13,   628,  1212],\n",
            "        ...,\n",
            "        [50256,  1641,  5410,  ...,   312, 16198, 34959],\n",
            "        [50256,    11,   198,  ...,   197,   197,   197],\n",
            "        [50256,  2634,   422,  ...,    64,   447,   247]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0121,  0.0713, -0.0221,  ..., -0.0047,  0.0340,  0.0362],\n",
            "        [-0.0191,  0.0185, -0.0147,  ..., -0.0500, -0.0164,  0.0210],\n",
            "        [-0.0122,  0.0513, -0.0396,  ...,  0.0147,  0.0099,  0.1119],\n",
            "        [-0.0123,  0.0611, -0.0612,  ..., -0.0169, -0.0325,  0.0285],\n",
            "        [ 0.0589, -0.0240, -0.0305,  ..., -0.0021,  0.0232,  0.0358]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.2142e-02,  7.1329e-02, -2.2115e-02, -3.2219e-02, -7.3504e-03,\n",
            "         -5.1712e-03, -1.4258e-02,  3.6463e-03, -7.2528e-02, -1.2672e-02,\n",
            "          4.6122e-03,  1.4175e-02, -3.4147e-02, -2.8365e-02, -4.2052e-02,\n",
            "         -3.0744e-02, -3.4445e-03,  2.2604e-02,  1.6286e-02,  2.2709e-02,\n",
            "         -3.0818e-02,  4.3094e-02, -1.1898e-02, -3.0833e-02, -1.4446e-02,\n",
            "          2.4660e-02, -2.9777e-02,  3.7143e-02,  8.6286e-03, -1.1514e-02,\n",
            "         -7.3272e-02,  2.3767e-02,  1.1371e-02,  2.1211e-02, -3.4940e-02,\n",
            "         -6.1637e-03,  6.4071e-02,  3.3229e-02,  3.3241e-02,  4.5248e-03,\n",
            "         -3.8311e-02, -4.5015e-03,  3.0992e-02, -1.4390e-02,  1.1368e-02,\n",
            "          7.7182e-03,  2.6411e-02, -7.6552e-02, -4.1495e-02, -7.0657e-02,\n",
            "         -2.2119e-02,  1.3506e-02, -2.3058e-02,  4.6741e-02,  3.0102e-02,\n",
            "          2.5792e-03, -2.2927e-02, -1.5754e-02,  4.6346e-02,  2.1372e-02,\n",
            "         -5.4239e-03,  2.0005e-03, -1.6299e-03, -1.5199e-02, -3.5164e-02,\n",
            "          2.0788e-02,  5.2159e-02, -1.9210e-02,  2.5479e-02,  4.5720e-02,\n",
            "          2.4114e-02, -1.5349e-02, -1.4193e-02,  2.9272e-02, -4.4249e-02,\n",
            "         -3.6488e-02, -1.8254e-02,  7.1675e-02,  6.6390e-02,  2.5626e-02,\n",
            "         -1.4931e-02, -2.6698e-02,  3.2279e-02, -3.2262e-02, -1.1876e-02,\n",
            "          9.8860e-02,  4.6823e-03,  3.2870e-02, -1.4697e-02,  4.4750e-03,\n",
            "          2.6731e-03, -5.5159e-02,  2.3581e-02,  3.3667e-04, -2.3084e-02,\n",
            "         -1.8607e-02, -7.3801e-02, -4.6083e-02,  1.5484e-02, -1.4649e-02,\n",
            "         -4.8945e-02,  2.9030e-03,  9.2744e-03,  1.6474e-02, -1.1481e-02,\n",
            "         -7.3376e-03,  5.1207e-02, -2.4342e-02, -5.0973e-02,  4.4168e-02,\n",
            "         -3.4445e-02, -1.7916e-02, -9.7254e-03, -6.4827e-02, -1.0095e-02,\n",
            "         -7.6482e-03, -6.5862e-04,  3.7946e-02,  4.0428e-02,  6.8023e-02,\n",
            "          1.9690e-02, -6.0037e-03, -3.3035e-02,  1.1034e-01,  4.0668e-02,\n",
            "         -8.4206e-03, -3.0904e-02, -9.4180e-03,  2.5073e-02, -6.8892e-03,\n",
            "          6.3646e-02,  9.9507e-03, -5.0382e-03,  2.6126e-02, -4.1060e-02,\n",
            "          2.0088e-04, -4.9595e-02,  4.1632e-02, -9.4542e-03,  1.2328e-02,\n",
            "         -1.9226e-02, -1.3885e-02,  3.5751e-02, -4.3014e-02, -2.7603e-02,\n",
            "          3.7737e-02, -2.8013e-02, -3.3164e-02,  4.0455e-02, -1.5841e-04,\n",
            "         -1.5636e-02,  5.7948e-02,  6.0733e-03,  5.9748e-03,  8.4701e-03,\n",
            "         -1.9018e-02,  1.5139e-02, -9.4750e-03, -5.2525e-03,  2.5504e-02,\n",
            "         -8.2062e-03, -3.0512e-02, -2.7668e-02, -5.7800e-02, -9.6243e-03,\n",
            "          6.4496e-02, -7.6880e-03, -5.3304e-02, -2.2085e-02, -3.6520e-02,\n",
            "         -3.6346e-03, -1.1075e-02, -4.5880e-03, -2.7485e-02, -3.7611e-02,\n",
            "         -2.2654e-02,  4.3303e-03, -1.5003e-02, -1.5873e-02, -8.6674e-02,\n",
            "          3.1601e-02,  4.3611e-02, -1.6109e-02, -7.9673e-03, -2.2144e-02,\n",
            "         -7.6548e-04,  1.1094e-02,  3.8676e-02,  4.9104e-02,  4.0269e-02,\n",
            "          6.6665e-03, -4.1229e-02, -4.7152e-03,  2.9469e-02, -3.6171e-03,\n",
            "         -2.2706e-02, -1.5385e-01, -2.6450e-02, -4.9941e-02, -3.5615e-02,\n",
            "          7.7424e-03, -1.4795e-02, -2.3637e-02,  4.8152e-02,  4.7237e-02,\n",
            "          3.4731e-02,  1.1581e-02,  2.3285e-02,  2.5493e-02, -2.8727e-02,\n",
            "         -1.6036e-02, -3.5266e-02, -1.4104e-02,  8.0383e-03, -3.9903e-02,\n",
            "         -3.7366e-03, -1.5611e-02,  2.0773e-02,  2.2689e-02,  3.5983e-02,\n",
            "          1.1928e-01,  4.2297e-02,  3.5104e-02,  9.1867e-03,  3.2185e-02,\n",
            "         -9.5894e-02,  9.7273e-03, -5.2601e-03, -5.4500e-03, -8.1317e-03,\n",
            "          1.2679e-02, -4.3455e-03, -8.0246e-03, -4.1321e-02,  4.6941e-02,\n",
            "          2.7681e-02, -1.9853e-02,  1.7893e-02,  1.9056e-02, -2.6548e-02,\n",
            "          1.8494e-02,  1.1441e-02,  1.0039e-03, -4.6084e-04,  7.5691e-03,\n",
            "         -1.2554e-01,  6.7763e-03,  7.5451e-03,  4.7982e-02, -2.6191e-02,\n",
            "          1.3186e-02, -6.0705e-03,  1.7942e-02, -4.6713e-03,  3.3975e-02,\n",
            "          3.6169e-02],\n",
            "        [-1.9074e-02,  1.8478e-02, -1.4724e-02,  3.8918e-02,  1.1681e-01,\n",
            "         -4.2690e-02, -2.5107e-02,  4.2521e-02,  1.8834e-02, -1.0177e-02,\n",
            "          3.6748e-03, -5.0469e-03, -1.2622e-02,  2.2020e-02,  1.0790e-01,\n",
            "          4.6785e-02,  1.2417e-03, -1.7220e-02, -1.3031e-02, -3.8920e-02,\n",
            "          9.8861e-02,  1.3765e-02, -4.6357e-03, -8.8182e-02, -4.2584e-02,\n",
            "         -4.5039e-02,  1.5159e-03, -6.1261e-02,  1.9155e-02, -1.1359e-02,\n",
            "          8.3663e-02, -2.7352e-02, -1.6700e-02,  7.9391e-03,  3.2182e-02,\n",
            "         -5.7856e-02,  4.4198e-05, -5.5559e-02, -4.9261e-03,  1.9421e-02,\n",
            "          2.5125e-02, -1.2997e-02, -2.7560e-02,  2.2294e-03, -3.0079e-02,\n",
            "         -1.8044e-02,  4.3098e-04,  8.3210e-02,  1.9840e-02,  8.1231e-04,\n",
            "          8.1100e-02, -8.8394e-03, -4.5753e-02, -4.3010e-02,  8.1834e-02,\n",
            "          9.1990e-02, -2.0388e-03, -9.5632e-02, -7.4229e-03,  3.3204e-02,\n",
            "          7.2690e-02, -1.9976e-02, -1.2499e-02,  9.1659e-02,  2.3107e-03,\n",
            "          5.4201e-02,  2.8926e-02, -5.7399e-02, -1.5702e-02, -6.6146e-03,\n",
            "          4.9172e-02,  3.0874e-02,  3.2560e-02,  9.1358e-02,  6.1542e-02,\n",
            "          2.9941e-02, -1.2690e-01,  8.9512e-02,  1.3793e-02, -4.6761e-02,\n",
            "         -4.6809e-02, -3.8145e-02, -6.8710e-02,  8.3769e-02, -3.5887e-04,\n",
            "         -3.8518e-02,  7.5022e-02, -3.1563e-02,  1.4321e-02,  2.4446e-02,\n",
            "         -2.2024e-02,  1.7721e-02, -4.0365e-02, -1.3099e-02, -1.7800e-02,\n",
            "         -5.4172e-02, -6.7744e-02, -2.4867e-02, -6.2990e-02, -2.8143e-02,\n",
            "         -5.3199e-02, -1.8469e-02,  1.6590e-02, -1.5535e-02,  9.0051e-03,\n",
            "         -2.6872e-04, -4.0722e-02, -2.3000e-02,  4.1272e-02, -4.7140e-03,\n",
            "          8.2236e-03,  9.3126e-03, -8.6537e-02,  3.6138e-02,  7.9109e-02,\n",
            "         -3.7682e-02, -7.8204e-02, -3.4113e-02, -5.4549e-02, -6.2453e-02,\n",
            "         -1.5466e-02, -1.5614e-02, -1.7830e-02, -4.4794e-02,  1.0422e-02,\n",
            "          1.2537e-02,  3.3699e-02,  2.9697e-02, -5.1538e-02,  4.1404e-03,\n",
            "          2.7508e-02,  3.0840e-02,  2.8915e-02, -2.5251e-02, -3.7952e-04,\n",
            "          5.0790e-03, -3.4565e-02,  1.9778e-02,  6.0104e-02,  4.6944e-02,\n",
            "         -2.3519e-02, -7.6388e-02,  9.1139e-02, -5.1700e-02, -1.3170e-02,\n",
            "         -9.9755e-03,  3.1065e-02, -2.8093e-02,  1.2907e-02,  2.4576e-03,\n",
            "         -2.5947e-03,  4.9881e-02,  2.8175e-02,  3.2066e-03, -2.8779e-02,\n",
            "          3.5382e-02,  1.7340e-02, -3.6506e-02,  7.7213e-02, -7.0471e-02,\n",
            "         -4.8933e-02, -1.9121e-02, -1.8745e-02, -6.8535e-02, -3.4831e-02,\n",
            "          3.7953e-02, -7.1174e-02, -1.1029e-02, -5.7091e-02, -1.0845e-02,\n",
            "          2.6407e-02, -3.6642e-02, -1.7086e-02, -2.7350e-02, -1.0758e-02,\n",
            "         -1.1245e-02,  9.7669e-03,  7.0040e-02,  2.1159e-02, -1.5589e-02,\n",
            "         -4.3585e-02, -1.9481e-02, -1.0048e-02, -5.8171e-02, -1.0372e-02,\n",
            "         -7.0678e-03, -6.2669e-03,  1.9533e-02, -3.6798e-02,  3.3552e-02,\n",
            "         -9.8920e-03, -2.2405e-02,  6.6733e-02, -1.0588e-01,  4.9373e-03,\n",
            "          1.0431e-02,  7.5424e-02, -1.7411e-02,  6.8291e-03, -4.0526e-02,\n",
            "          4.6965e-02, -2.1393e-02, -3.5199e-02,  2.7012e-02, -9.2454e-02,\n",
            "         -4.0314e-02, -6.0043e-03, -5.3719e-02, -1.0593e-02, -4.1915e-02,\n",
            "         -6.6292e-02,  6.0524e-02, -2.2293e-02, -6.2182e-02, -8.7116e-02,\n",
            "          3.1194e-02,  1.0863e-02,  3.8447e-02,  3.4221e-02, -6.6076e-04,\n",
            "          2.9787e-02, -2.8728e-03,  1.2507e-02,  8.2438e-02, -2.8233e-02,\n",
            "          3.7547e-02,  3.4042e-02,  6.0905e-05,  4.8909e-02, -2.9541e-03,\n",
            "         -2.2106e-02, -4.2303e-03,  7.8345e-02, -2.5018e-05,  6.2436e-02,\n",
            "         -8.0185e-02,  2.6160e-02, -2.2365e-02, -2.0377e-02,  4.9679e-02,\n",
            "         -8.1529e-02, -3.1666e-02,  5.9134e-02, -2.8225e-03, -1.2840e-02,\n",
            "         -2.1698e-02, -2.4032e-02,  3.4773e-02, -2.7641e-03, -1.1376e-02,\n",
            "         -1.8321e-02,  5.4623e-02, -1.3871e-02, -4.9974e-02, -1.6416e-02,\n",
            "          2.1018e-02],\n",
            "        [-1.2212e-02,  5.1350e-02, -3.9598e-02, -1.5994e-03, -3.8667e-03,\n",
            "         -5.2009e-02, -2.5278e-02, -4.0352e-02, -1.8792e-04,  4.5397e-02,\n",
            "         -5.8913e-03, -1.8626e-04,  1.2069e-02, -3.8361e-02, -2.1851e-02,\n",
            "         -1.6816e-02, -1.6978e-02,  1.1841e-02, -2.0007e-02, -2.1119e-02,\n",
            "          5.8150e-04, -6.1407e-03, -4.0420e-03, -3.2226e-02, -4.1266e-02,\n",
            "          5.2384e-02,  1.7436e-02, -5.2193e-02, -5.4218e-02, -1.5035e-03,\n",
            "         -6.5755e-02,  4.1220e-02, -1.8687e-02, -1.9326e-02,  5.6233e-04,\n",
            "         -3.7378e-02, -4.3713e-03,  5.4741e-02,  1.7715e-02, -4.6770e-02,\n",
            "         -6.6526e-02, -1.9058e-02, -4.4105e-02,  3.3369e-02, -2.4043e-02,\n",
            "         -4.7610e-02, -2.9050e-02,  9.7914e-03,  4.6574e-02, -2.5915e-02,\n",
            "         -1.1254e-02, -3.6862e-02, -4.1273e-02,  5.4621e-02,  9.2262e-03,\n",
            "          2.3948e-02, -5.4276e-02,  6.8155e-02,  2.2671e-02,  2.0389e-02,\n",
            "          2.0619e-02,  2.0517e-02, -2.2089e-02, -3.5791e-02, -1.2554e-02,\n",
            "          3.3751e-02,  1.0398e-02,  4.1505e-03, -2.4949e-03,  6.4603e-02,\n",
            "          1.2519e-02,  8.3082e-03,  1.0046e-02, -1.8142e-02, -5.9305e-02,\n",
            "         -5.9103e-02, -2.0017e-03,  1.8896e-02, -6.8649e-03,  1.8566e-02,\n",
            "         -5.2570e-02,  2.5617e-02,  2.0648e-03, -1.0649e-02,  6.9275e-03,\n",
            "         -1.0039e-02,  3.4429e-02,  1.8282e-02,  2.3456e-02, -3.7228e-03,\n",
            "         -9.2832e-03, -2.8507e-02,  2.1481e-02, -4.4193e-02, -5.6190e-02,\n",
            "          3.8328e-02,  2.7976e-02, -7.1252e-03, -1.3161e-02,  1.7625e-02,\n",
            "         -4.6325e-02, -4.2643e-02, -1.6874e-02, -4.7913e-02, -1.3094e-02,\n",
            "         -3.3603e-02,  2.3226e-02, -3.9725e-02,  1.5147e-03, -1.4515e-03,\n",
            "         -1.7944e-02,  2.6395e-02, -5.9991e-03, -7.9897e-04, -1.8775e-03,\n",
            "         -3.8355e-02,  1.9250e-02,  9.8237e-02, -2.6711e-03, -3.5351e-02,\n",
            "          1.9145e-02,  1.6618e-02,  7.2715e-02,  1.1820e-02, -3.0682e-02,\n",
            "         -2.2630e-02,  1.1191e-02,  4.5198e-02,  2.7126e-03, -1.0101e-02,\n",
            "         -7.3994e-02, -1.3036e-03,  4.6123e-03, -3.9818e-02, -2.2888e-03,\n",
            "          2.4012e-02,  2.0193e-02,  2.1121e-02,  3.1483e-02,  2.8025e-02,\n",
            "          2.9209e-02, -2.7670e-02,  1.5879e-02,  2.1892e-02,  1.8896e-02,\n",
            "         -7.3423e-03,  3.2845e-02,  4.7270e-02, -2.7798e-02,  1.7793e-02,\n",
            "         -5.4731e-03, -2.0539e-02,  5.4554e-02,  2.6589e-02,  2.6363e-02,\n",
            "          1.0535e-02, -2.6670e-03, -3.3319e-02,  5.4845e-03, -8.4204e-02,\n",
            "          4.8398e-02,  1.1392e-02,  2.1878e-02, -4.7369e-02, -6.3091e-03,\n",
            "          5.3032e-02, -4.4706e-02, -2.1675e-02, -6.4388e-02, -3.7099e-02,\n",
            "          3.0569e-02, -2.4428e-02,  3.2933e-02, -2.4688e-03,  3.2282e-02,\n",
            "         -3.7312e-02,  1.0413e-01,  3.6151e-02,  3.2582e-02, -8.2858e-02,\n",
            "         -2.2451e-02,  7.4569e-03, -4.2755e-02,  2.7318e-02,  2.7149e-02,\n",
            "         -2.0121e-02, -6.4774e-03,  1.1985e-02,  6.6572e-02,  1.3003e-02,\n",
            "          6.0706e-03, -3.7499e-02,  2.9991e-02, -2.1502e-02,  1.2302e-02,\n",
            "         -4.9478e-02,  4.4406e-02,  2.3740e-03,  7.8033e-02,  9.7317e-02,\n",
            "         -6.0656e-02, -2.0495e-02, -5.7057e-02, -6.1390e-02, -6.7021e-02,\n",
            "         -1.8239e-02,  4.1058e-02, -2.5464e-02,  4.3467e-02,  3.8130e-03,\n",
            "          2.4104e-02,  4.8793e-03,  6.5745e-02,  1.0836e-02, -2.9563e-02,\n",
            "         -3.1105e-02, -1.6939e-02,  3.5080e-02,  6.4900e-02,  2.4898e-02,\n",
            "          3.5743e-02,  2.3760e-02,  4.1239e-02,  2.9386e-02, -6.8277e-03,\n",
            "         -5.3363e-02, -3.5560e-02, -2.2648e-02, -3.6910e-02,  3.3744e-02,\n",
            "          2.8270e-02,  1.6041e-02,  3.1622e-02,  7.4218e-03,  6.2115e-03,\n",
            "         -4.3742e-02, -1.7165e-02,  2.5125e-02, -3.1197e-02,  2.7952e-02,\n",
            "          5.6929e-02, -4.5242e-03,  5.0594e-02, -1.0285e-02, -3.8877e-03,\n",
            "         -1.4993e-02, -4.7455e-02,  8.7119e-02,  7.3451e-02, -8.6306e-03,\n",
            "         -2.0645e-02,  2.6835e-02,  6.8189e-03,  1.4654e-02,  9.9180e-03,\n",
            "          1.1192e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.5621,  1.5953,  5.0619, -5.1024, -3.7238], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.3714,  1.7705,  4.5566, -5.7770, -3.7994], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256, 13291,  2625,  ...,    11, 24418, 16922],\n",
            "        [50256,   357,    66,  ...,  3926,   198,   198],\n",
            "        [50256,  2290,  1058,  ...,   513,   317, 16175],\n",
            "        ...,\n",
            "        [50256,     8,   198,  ...,   716,  1262,   262],\n",
            "        [50256,  5397,   415,  ...,   286,   465,  9709],\n",
            "        [50256,  3538,  9694,  ...,   198,   464,  8305]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0122,  0.0710, -0.0220,  ..., -0.0051,  0.0338,  0.0361],\n",
            "        [-0.0191,  0.0184, -0.0150,  ..., -0.0501, -0.0167,  0.0213],\n",
            "        [-0.0122,  0.0516, -0.0399,  ...,  0.0143,  0.0094,  0.1123],\n",
            "        [-0.0123,  0.0612, -0.0613,  ..., -0.0172, -0.0326,  0.0288],\n",
            "        [ 0.0587, -0.0240, -0.0305,  ..., -0.0022,  0.0232,  0.0361]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.2229e-02,  7.1002e-02, -2.2020e-02, -3.1777e-02, -6.9254e-03,\n",
            "         -5.2692e-03, -1.4393e-02,  3.3872e-03, -7.2899e-02, -1.3076e-02,\n",
            "          4.9052e-03,  1.4026e-02, -3.3888e-02, -2.8002e-02, -4.2621e-02,\n",
            "         -3.0581e-02, -3.6386e-03,  2.2683e-02,  1.6608e-02,  2.2915e-02,\n",
            "         -3.0859e-02,  4.3207e-02, -1.2067e-02, -3.0524e-02, -1.4140e-02,\n",
            "          2.4851e-02, -3.0060e-02,  3.7101e-02,  8.7200e-03, -1.1425e-02,\n",
            "         -7.3446e-02,  2.4048e-02,  1.0758e-02,  2.1493e-02, -3.5099e-02,\n",
            "         -6.0678e-03,  6.4073e-02,  3.3090e-02,  3.3139e-02,  4.6858e-03,\n",
            "         -3.8010e-02, -4.3012e-03,  3.0970e-02, -1.4030e-02,  1.1182e-02,\n",
            "          7.9982e-03,  2.6477e-02, -7.6615e-02, -4.1922e-02, -7.0698e-02,\n",
            "         -2.1981e-02,  1.3351e-02, -2.2850e-02,  4.6632e-02,  2.9904e-02,\n",
            "          2.5206e-03, -2.3260e-02, -1.5677e-02,  4.6506e-02,  2.0963e-02,\n",
            "         -5.5285e-03,  1.9453e-03, -1.3020e-03, -1.5193e-02, -3.5273e-02,\n",
            "          2.1072e-02,  5.2121e-02, -1.9236e-02,  2.5446e-02,  4.5546e-02,\n",
            "          2.3995e-02, -1.5162e-02, -1.3934e-02,  2.8975e-02, -4.4350e-02,\n",
            "         -3.6303e-02, -1.8188e-02,  7.1996e-02,  6.6770e-02,  2.5969e-02,\n",
            "         -1.4953e-02, -2.6891e-02,  3.2091e-02, -3.2217e-02, -1.1885e-02,\n",
            "          9.8770e-02,  4.8724e-03,  3.2640e-02, -1.5069e-02,  4.4475e-03,\n",
            "          2.4412e-03, -5.5016e-02,  2.3847e-02,  2.3278e-04, -2.3006e-02,\n",
            "         -1.9168e-02, -7.3377e-02, -4.6464e-02,  1.5356e-02, -1.4463e-02,\n",
            "         -4.9269e-02,  2.6622e-03,  9.5271e-03,  1.6695e-02, -1.1517e-02,\n",
            "         -7.7669e-03,  5.1193e-02, -2.4171e-02, -5.1016e-02,  4.4072e-02,\n",
            "         -3.4740e-02, -1.7925e-02, -1.0020e-02, -6.4700e-02, -1.0060e-02,\n",
            "         -7.3873e-03, -1.8241e-04,  3.8071e-02,  4.0605e-02,  6.7779e-02,\n",
            "          1.9502e-02, -5.5296e-03, -3.3242e-02,  1.1061e-01,  4.1043e-02,\n",
            "         -8.8250e-03, -3.1166e-02, -9.1034e-03,  2.5339e-02, -7.0014e-03,\n",
            "          6.3749e-02,  9.9805e-03, -5.2934e-03,  2.6310e-02, -4.1124e-02,\n",
            "          5.4236e-04, -5.0276e-02,  4.1853e-02, -8.9378e-03,  1.2390e-02,\n",
            "         -1.9520e-02, -1.3839e-02,  3.5611e-02, -4.2883e-02, -2.8092e-02,\n",
            "          3.8011e-02, -2.8581e-02, -3.3169e-02,  4.0436e-02, -1.2724e-04,\n",
            "         -1.5561e-02,  5.7623e-02,  6.5207e-03,  5.4954e-03,  8.3744e-03,\n",
            "         -1.8878e-02,  1.5246e-02, -9.6009e-03, -4.8515e-03,  2.5058e-02,\n",
            "         -8.0290e-03, -3.0406e-02, -2.7907e-02, -5.8010e-02, -9.9283e-03,\n",
            "          6.4868e-02, -7.3669e-03, -5.3347e-02, -2.1896e-02, -3.7034e-02,\n",
            "         -3.3558e-03, -1.1394e-02, -4.9200e-03, -2.8075e-02, -3.8064e-02,\n",
            "         -2.2616e-02,  4.2561e-03, -1.5249e-02, -1.5987e-02, -8.6809e-02,\n",
            "          3.1820e-02,  4.3838e-02, -1.6536e-02, -7.6943e-03, -2.2255e-02,\n",
            "         -7.1127e-04,  1.1234e-02,  3.8742e-02,  4.9069e-02,  4.0179e-02,\n",
            "          6.6783e-03, -4.1345e-02, -4.7782e-03,  2.9284e-02, -3.7466e-03,\n",
            "         -2.2375e-02, -1.5394e-01, -2.6256e-02, -5.0031e-02, -3.5814e-02,\n",
            "          7.9091e-03, -1.4759e-02, -2.3634e-02,  4.8491e-02,  4.7140e-02,\n",
            "          3.5007e-02,  1.1267e-02,  2.3665e-02,  2.5471e-02, -2.8579e-02,\n",
            "         -1.6332e-02, -3.5217e-02, -1.4461e-02,  8.2698e-03, -3.9788e-02,\n",
            "         -4.0709e-03, -1.5874e-02,  2.0687e-02,  2.2625e-02,  3.5928e-02,\n",
            "          1.1920e-01,  4.2395e-02,  3.5206e-02,  8.9649e-03,  3.1938e-02,\n",
            "         -9.5496e-02,  1.0133e-02, -5.7411e-03, -4.6124e-03, -8.0188e-03,\n",
            "          1.2826e-02, -4.1486e-03, -8.0271e-03, -4.1029e-02,  4.6735e-02,\n",
            "          2.7701e-02, -1.9993e-02,  1.8062e-02,  1.9102e-02, -2.6575e-02,\n",
            "          1.8680e-02,  1.1289e-02,  1.0889e-03,  4.4842e-06,  7.7544e-03,\n",
            "         -1.2539e-01,  6.5448e-03,  7.4998e-03,  4.8101e-02, -2.5973e-02,\n",
            "          1.3309e-02, -6.0779e-03,  1.7607e-02, -5.0683e-03,  3.3800e-02,\n",
            "          3.6069e-02],\n",
            "        [-1.9086e-02,  1.8366e-02, -1.4998e-02,  3.9378e-02,  1.1694e-01,\n",
            "         -4.2582e-02, -2.5417e-02,  4.2489e-02,  1.8921e-02, -1.0413e-02,\n",
            "          3.6831e-03, -5.1853e-03, -1.2865e-02,  2.1887e-02,  1.0761e-01,\n",
            "          4.6960e-02,  1.1870e-03, -1.7240e-02, -1.2573e-02, -3.9088e-02,\n",
            "          9.8976e-02,  1.3390e-02, -4.7157e-03, -8.8146e-02, -4.2439e-02,\n",
            "         -4.5041e-02,  1.2065e-03, -6.1124e-02,  1.9115e-02, -1.1407e-02,\n",
            "          8.3466e-02, -2.6966e-02, -1.6728e-02,  7.8011e-03,  3.2453e-02,\n",
            "         -5.7489e-02, -1.5317e-04, -5.5591e-02, -5.1011e-03,  1.9712e-02,\n",
            "          2.5055e-02, -1.3169e-02, -2.7478e-02,  2.4226e-03, -3.0122e-02,\n",
            "         -1.8109e-02,  4.7733e-04,  8.3090e-02,  1.9519e-02,  6.4441e-04,\n",
            "          8.1285e-02, -8.8504e-03, -4.5781e-02, -4.3473e-02,  8.1634e-02,\n",
            "          9.1942e-02, -2.0933e-03, -9.5890e-02, -7.4361e-03,  3.3409e-02,\n",
            "          7.2565e-02, -1.9978e-02, -1.2427e-02,  9.1708e-02,  2.4622e-03,\n",
            "          5.4447e-02,  2.8976e-02, -5.7413e-02, -1.5521e-02, -6.4465e-03,\n",
            "          4.9203e-02,  3.0731e-02,  3.2490e-02,  9.1380e-02,  6.1428e-02,\n",
            "          3.0100e-02, -1.2714e-01,  8.9546e-02,  1.3898e-02, -4.6606e-02,\n",
            "         -4.6786e-02, -3.8354e-02, -6.8714e-02,  8.4096e-02, -7.3466e-04,\n",
            "         -3.8482e-02,  7.5210e-02, -3.1688e-02,  1.4538e-02,  2.4330e-02,\n",
            "         -2.1717e-02,  1.8005e-02, -4.0281e-02, -1.2995e-02, -1.7595e-02,\n",
            "         -5.4356e-02, -6.7839e-02, -2.5009e-02, -6.3163e-02, -2.8020e-02,\n",
            "         -5.3301e-02, -1.8448e-02,  1.6379e-02, -1.5489e-02,  9.0542e-03,\n",
            "         -4.3140e-04, -4.0816e-02, -2.3159e-02,  4.1369e-02, -4.9308e-03,\n",
            "          8.0738e-03,  9.3929e-03, -8.6610e-02,  3.6119e-02,  7.9264e-02,\n",
            "         -3.7466e-02, -7.7910e-02, -3.4241e-02, -5.4533e-02, -6.2653e-02,\n",
            "         -1.5532e-02, -1.5319e-02, -1.7683e-02, -4.4780e-02,  1.0717e-02,\n",
            "          1.2327e-02,  3.3771e-02,  2.9927e-02, -5.1604e-02,  3.9761e-03,\n",
            "          2.7611e-02,  3.0844e-02,  2.8982e-02, -2.5162e-02, -2.9889e-04,\n",
            "          5.3110e-03, -3.4987e-02,  1.9771e-02,  6.0477e-02,  4.6992e-02,\n",
            "         -2.3577e-02, -7.6342e-02,  9.1205e-02, -5.1835e-02, -1.3526e-02,\n",
            "         -1.0137e-02,  3.0815e-02, -2.8237e-02,  1.2560e-02,  2.1747e-03,\n",
            "         -2.7618e-03,  4.9492e-02,  2.8785e-02,  2.9159e-03, -2.8536e-02,\n",
            "          3.5311e-02,  1.7668e-02, -3.6384e-02,  7.7271e-02, -7.0776e-02,\n",
            "         -4.8938e-02, -1.9072e-02, -1.8810e-02, -6.8455e-02, -3.4577e-02,\n",
            "          3.8031e-02, -7.1143e-02, -1.1048e-02, -5.6974e-02, -1.0920e-02,\n",
            "          2.6442e-02, -3.6617e-02, -1.7206e-02, -2.7709e-02, -1.0882e-02,\n",
            "         -1.1102e-02,  9.8404e-03,  7.0232e-02,  2.1347e-02, -1.5584e-02,\n",
            "         -4.3524e-02, -1.9268e-02, -1.0452e-02, -5.8112e-02, -1.0373e-02,\n",
            "         -6.8788e-03, -6.2812e-03,  1.9792e-02, -3.6821e-02,  3.3665e-02,\n",
            "         -1.0214e-02, -2.2392e-02,  6.6813e-02, -1.0591e-01,  4.8168e-03,\n",
            "          1.0739e-02,  7.5400e-02, -1.7382e-02,  6.7515e-03, -4.0868e-02,\n",
            "          4.7071e-02, -2.1456e-02, -3.5216e-02,  2.7422e-02, -9.2417e-02,\n",
            "         -4.0351e-02, -6.0197e-03, -5.3437e-02, -1.0633e-02, -4.1767e-02,\n",
            "         -6.6312e-02,  6.0456e-02, -2.2399e-02, -6.2027e-02, -8.6921e-02,\n",
            "          3.1069e-02,  1.1008e-02,  3.8529e-02,  3.4359e-02, -7.6779e-04,\n",
            "          2.9793e-02, -2.9239e-03,  1.2721e-02,  8.2234e-02, -2.8216e-02,\n",
            "          3.7725e-02,  3.4177e-02, -2.7869e-05,  4.9242e-02, -3.0046e-03,\n",
            "         -2.2146e-02, -4.2306e-03,  7.8359e-02, -1.7125e-06,  6.2374e-02,\n",
            "         -8.0321e-02,  2.6176e-02, -2.2418e-02, -2.0005e-02,  4.9492e-02,\n",
            "         -8.1182e-02, -3.1914e-02,  5.9065e-02, -2.8927e-03, -1.2792e-02,\n",
            "         -2.1773e-02, -2.4055e-02,  3.5102e-02, -2.7274e-03, -1.1212e-02,\n",
            "         -1.8382e-02,  5.4210e-02, -1.4265e-02, -5.0094e-02, -1.6720e-02,\n",
            "          2.1288e-02],\n",
            "        [-1.2249e-02,  5.1614e-02, -3.9910e-02, -1.4597e-03, -3.9173e-03,\n",
            "         -5.1316e-02, -2.5566e-02, -4.0238e-02,  4.6659e-05,  4.5327e-02,\n",
            "         -5.2662e-03, -3.7825e-04,  1.1467e-02, -3.8354e-02, -2.2025e-02,\n",
            "         -1.6564e-02, -1.7268e-02,  1.1657e-02, -1.9913e-02, -2.1154e-02,\n",
            "          4.0125e-04, -6.4102e-03, -3.5912e-03, -3.1986e-02, -4.1114e-02,\n",
            "          5.2449e-02,  1.6962e-02, -5.2126e-02, -5.4873e-02, -1.2483e-03,\n",
            "         -6.5480e-02,  4.2278e-02, -1.8675e-02, -1.9694e-02,  4.2572e-04,\n",
            "         -3.6887e-02, -4.9322e-03,  5.4833e-02,  1.7681e-02, -4.6755e-02,\n",
            "         -6.6295e-02, -1.9148e-02, -4.4148e-02,  3.3181e-02, -2.3882e-02,\n",
            "         -4.7908e-02, -2.8517e-02,  9.4019e-03,  4.6188e-02, -2.5842e-02,\n",
            "         -1.1506e-02, -3.6585e-02, -4.1424e-02,  5.4382e-02,  9.2134e-03,\n",
            "          2.3722e-02, -5.4178e-02,  6.7649e-02,  2.2723e-02,  2.0717e-02,\n",
            "          1.9930e-02,  2.0555e-02, -2.2175e-02, -3.6441e-02, -1.2619e-02,\n",
            "          3.3851e-02,  1.0438e-02,  4.7075e-03, -2.3318e-03,  6.4864e-02,\n",
            "          1.2223e-02,  8.2856e-03,  9.5657e-03, -1.8363e-02, -5.9473e-02,\n",
            "         -5.8703e-02, -1.5843e-03,  1.8987e-02, -7.1159e-03,  1.9181e-02,\n",
            "         -5.2238e-02,  2.5272e-02,  2.2990e-03, -1.0628e-02,  6.6850e-03,\n",
            "         -9.8065e-03,  3.4594e-02,  1.8388e-02,  2.3478e-02, -3.8810e-03,\n",
            "         -9.0885e-03, -2.8664e-02,  2.1759e-02, -4.4526e-02, -5.6154e-02,\n",
            "          3.8556e-02,  2.7879e-02, -6.9329e-03, -1.3119e-02,  1.7371e-02,\n",
            "         -4.6375e-02, -4.2358e-02, -1.7038e-02, -4.7776e-02, -1.2828e-02,\n",
            "         -3.3383e-02,  2.3081e-02, -3.9689e-02,  1.5263e-03, -1.6884e-03,\n",
            "         -1.7880e-02,  2.6560e-02, -5.9515e-03, -1.3161e-03, -1.8278e-03,\n",
            "         -3.8365e-02,  1.9655e-02,  9.8133e-02, -2.4604e-03, -3.5293e-02,\n",
            "          1.8655e-02,  1.6971e-02,  7.2712e-02,  1.1634e-02, -3.0341e-02,\n",
            "         -2.3000e-02,  1.1555e-02,  4.5400e-02,  2.8818e-03, -1.0726e-02,\n",
            "         -7.3728e-02, -1.2627e-03,  4.1644e-03, -3.9815e-02, -2.1693e-03,\n",
            "          2.4411e-02,  1.9772e-02,  2.0923e-02,  3.1686e-02,  2.7934e-02,\n",
            "          2.9394e-02, -2.7028e-02,  1.5907e-02,  2.1527e-02,  1.8623e-02,\n",
            "         -7.5258e-03,  3.2633e-02,  4.7541e-02, -2.7638e-02,  1.7492e-02,\n",
            "         -5.9497e-03, -2.1004e-02,  5.5310e-02,  2.6462e-02,  2.6482e-02,\n",
            "          1.0154e-02, -2.5313e-03, -3.2934e-02,  5.4091e-03, -8.4608e-02,\n",
            "          4.8641e-02,  1.1745e-02,  2.2305e-02, -4.7042e-02, -5.9647e-03,\n",
            "          5.2974e-02, -4.4441e-02, -2.1849e-02, -6.4326e-02, -3.7372e-02,\n",
            "          3.0533e-02, -2.4319e-02,  3.2590e-02, -2.5201e-03,  3.2371e-02,\n",
            "         -3.6721e-02,  1.0432e-01,  3.6653e-02,  3.2949e-02, -8.2998e-02,\n",
            "         -2.2275e-02,  7.5242e-03, -4.2939e-02,  2.7268e-02,  2.7201e-02,\n",
            "         -1.9795e-02, -6.0477e-03,  1.2355e-02,  6.6579e-02,  1.3075e-02,\n",
            "          5.5845e-03, -3.7638e-02,  3.0189e-02, -2.1193e-02,  1.2414e-02,\n",
            "         -4.9247e-02,  4.4353e-02,  2.5553e-03,  7.8168e-02,  9.7323e-02,\n",
            "         -6.0676e-02, -2.0787e-02, -5.6878e-02, -6.1029e-02, -6.6999e-02,\n",
            "         -1.8383e-02,  4.1024e-02, -2.5216e-02,  4.3333e-02,  3.4066e-03,\n",
            "          2.4218e-02,  4.4115e-03,  6.5612e-02,  1.0747e-02, -2.9588e-02,\n",
            "         -3.1531e-02, -1.6730e-02,  3.4996e-02,  6.4947e-02,  2.5257e-02,\n",
            "          3.5656e-02,  2.3954e-02,  4.1214e-02,  2.9202e-02, -6.5680e-03,\n",
            "         -5.3403e-02, -3.5793e-02, -2.2849e-02, -3.7002e-02,  3.3848e-02,\n",
            "          2.8661e-02,  1.5769e-02,  3.1465e-02,  7.3027e-03,  5.8130e-03,\n",
            "         -4.3962e-02, -1.7050e-02,  2.5225e-02, -3.1184e-02,  2.7584e-02,\n",
            "          5.7693e-02, -4.4591e-03,  5.0285e-02, -1.0590e-02, -4.3881e-03,\n",
            "         -1.4933e-02, -4.7635e-02,  8.7518e-02,  7.3021e-02, -8.6646e-03,\n",
            "         -2.0403e-02,  2.5945e-02,  6.3357e-03,  1.4317e-02,  9.4335e-03,\n",
            "          1.1231e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4471,  1.6149,  5.1379, -5.0477, -3.8342], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4426,  1.6651,  4.6030, -5.6499, -3.8601], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,    13,   198,  ..., 37999,    11,   318],\n",
            "        [50256,   220,   220,  ...,   352, 11405,  5421],\n",
            "        [50256,   220,   220,  ...,    14,  8094,     8],\n",
            "        ...,\n",
            "        [50256,  2270,   362,  ..., 16792,    58,  1533],\n",
            "        [50256,  5173,   313,  ...,  1976,  5512,  1391],\n",
            "        [50256,   392,   340,  ...,  3203,  8799,    75]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0122,  0.0708, -0.0220,  ..., -0.0053,  0.0336,  0.0361],\n",
            "        [-0.0192,  0.0182, -0.0153,  ..., -0.0499, -0.0170,  0.0214],\n",
            "        [-0.0123,  0.0518, -0.0402,  ...,  0.0141,  0.0091,  0.1127],\n",
            "        [-0.0123,  0.0613, -0.0614,  ..., -0.0174, -0.0326,  0.0289],\n",
            "        [ 0.0586, -0.0240, -0.0305,  ..., -0.0022,  0.0232,  0.0363]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.2238e-02,  7.0797e-02, -2.2012e-02, -3.1408e-02, -6.6046e-03,\n",
            "         -5.3392e-03, -1.4564e-02,  3.2408e-03, -7.3252e-02, -1.3295e-02,\n",
            "          4.9993e-03,  1.3917e-02, -3.3755e-02, -2.7781e-02, -4.3072e-02,\n",
            "         -3.0394e-02, -3.7634e-03,  2.2749e-02,  1.6812e-02,  2.3108e-02,\n",
            "         -3.0938e-02,  4.3262e-02, -1.2190e-02, -3.0318e-02, -1.3879e-02,\n",
            "          2.4994e-02, -3.0218e-02,  3.7164e-02,  8.7808e-03, -1.1411e-02,\n",
            "         -7.3467e-02,  2.4162e-02,  1.0329e-02,  2.1843e-02, -3.5317e-02,\n",
            "         -5.9444e-03,  6.4165e-02,  3.3088e-02,  3.3068e-02,  4.8101e-03,\n",
            "         -3.7686e-02, -4.2147e-03,  3.1063e-02, -1.3842e-02,  1.1061e-02,\n",
            "          8.1482e-03,  2.6536e-02, -7.6689e-02, -4.2253e-02, -7.0716e-02,\n",
            "         -2.1934e-02,  1.3228e-02, -2.2728e-02,  4.6513e-02,  2.9852e-02,\n",
            "          2.4706e-03, -2.3459e-02, -1.5569e-02,  4.6720e-02,  2.0734e-02,\n",
            "         -5.6176e-03,  1.8371e-03, -9.7996e-04, -1.5121e-02, -3.5251e-02,\n",
            "          2.1222e-02,  5.2117e-02, -1.9276e-02,  2.5478e-02,  4.5505e-02,\n",
            "          2.3893e-02, -1.4932e-02, -1.3770e-02,  2.8799e-02, -4.4441e-02,\n",
            "         -3.6173e-02, -1.8177e-02,  7.2297e-02,  6.7100e-02,  2.6195e-02,\n",
            "         -1.5056e-02, -2.7019e-02,  3.1943e-02, -3.2278e-02, -1.1854e-02,\n",
            "          9.8693e-02,  4.8935e-03,  3.2521e-02, -1.5343e-02,  4.4724e-03,\n",
            "          2.2308e-03, -5.4915e-02,  2.4048e-02,  8.2664e-05, -2.3056e-02,\n",
            "         -1.9702e-02, -7.3094e-02, -4.6701e-02,  1.5306e-02, -1.4330e-02,\n",
            "         -4.9570e-02,  2.5585e-03,  9.6685e-03,  1.6847e-02, -1.1455e-02,\n",
            "         -8.0319e-03,  5.1179e-02, -2.4012e-02, -5.1093e-02,  4.3953e-02,\n",
            "         -3.4935e-02, -1.7932e-02, -1.0332e-02, -6.4642e-02, -1.0012e-02,\n",
            "         -7.2168e-03,  2.4693e-04,  3.8240e-02,  4.0766e-02,  6.7557e-02,\n",
            "          1.9404e-02, -5.2537e-03, -3.3344e-02,  1.1075e-01,  4.1349e-02,\n",
            "         -9.1452e-03, -3.1335e-02, -8.8276e-03,  2.5596e-02, -7.0316e-03,\n",
            "          6.3936e-02,  1.0030e-02, -5.4792e-03,  2.6511e-02, -4.1276e-02,\n",
            "          7.9408e-04, -5.0728e-02,  4.1993e-02, -8.5501e-03,  1.2484e-02,\n",
            "         -1.9785e-02, -1.3813e-02,  3.5420e-02, -4.2724e-02, -2.8501e-02,\n",
            "          3.8285e-02, -2.9093e-02, -3.3179e-02,  4.0469e-02, -9.9014e-05,\n",
            "         -1.5566e-02,  5.7428e-02,  6.8141e-03,  5.1095e-03,  8.3046e-03,\n",
            "         -1.8763e-02,  1.5413e-02, -9.6647e-03, -4.5959e-03,  2.4717e-02,\n",
            "         -7.8560e-03, -3.0314e-02, -2.8137e-02, -5.8078e-02, -1.0143e-02,\n",
            "          6.5228e-02, -7.0669e-03, -5.3500e-02, -2.1715e-02, -3.7343e-02,\n",
            "         -3.2267e-03, -1.1663e-02, -5.1035e-03, -2.8467e-02, -3.8448e-02,\n",
            "         -2.2503e-02,  4.1670e-03, -1.5451e-02, -1.6082e-02, -8.6868e-02,\n",
            "          3.1990e-02,  4.3941e-02, -1.6901e-02, -7.5502e-03, -2.2423e-02,\n",
            "         -6.2569e-04,  1.1382e-02,  3.8804e-02,  4.9032e-02,  4.0181e-02,\n",
            "          6.6666e-03, -4.1353e-02, -4.7401e-03,  2.9201e-02, -3.9427e-03,\n",
            "         -2.2068e-02, -1.5404e-01, -2.6162e-02, -5.0037e-02, -3.5891e-02,\n",
            "          8.0199e-03, -1.4787e-02, -2.3538e-02,  4.8661e-02,  4.7023e-02,\n",
            "          3.5210e-02,  1.1139e-02,  2.3933e-02,  2.5444e-02, -2.8390e-02,\n",
            "         -1.6613e-02, -3.5193e-02, -1.4901e-02,  8.4706e-03, -3.9677e-02,\n",
            "         -4.2613e-03, -1.6025e-02,  2.0703e-02,  2.2565e-02,  3.5900e-02,\n",
            "          1.1903e-01,  4.2413e-02,  3.5332e-02,  8.6615e-03,  3.1857e-02,\n",
            "         -9.5198e-02,  1.0403e-02, -6.1252e-03, -3.9728e-03, -7.9972e-03,\n",
            "          1.2911e-02, -4.0708e-03, -7.9534e-03, -4.0802e-02,  4.6493e-02,\n",
            "          2.7878e-02, -2.0050e-02,  1.8178e-02,  1.9164e-02, -2.6491e-02,\n",
            "          1.8735e-02,  1.1059e-02,  1.1011e-03,  4.3206e-04,  7.7893e-03,\n",
            "         -1.2527e-01,  6.3638e-03,  7.4503e-03,  4.8209e-02, -2.5896e-02,\n",
            "          1.3314e-02, -6.1043e-03,  1.7314e-02, -5.3370e-03,  3.3604e-02,\n",
            "          3.6056e-02],\n",
            "        [-1.9177e-02,  1.8221e-02, -1.5257e-02,  3.9769e-02,  1.1683e-01,\n",
            "         -4.2475e-02, -2.5557e-02,  4.2555e-02,  1.8990e-02, -1.0642e-02,\n",
            "          3.4924e-03, -5.2180e-03, -1.2929e-02,  2.1573e-02,  1.0729e-01,\n",
            "          4.6989e-02,  1.1985e-03, -1.7218e-02, -1.2358e-02, -3.9259e-02,\n",
            "          9.9028e-02,  1.2979e-02, -4.8099e-03, -8.8119e-02, -4.2293e-02,\n",
            "         -4.5058e-02,  9.4729e-04, -6.0890e-02,  1.9077e-02, -1.1467e-02,\n",
            "          8.3306e-02, -2.6761e-02, -1.6695e-02,  7.6255e-03,  3.2669e-02,\n",
            "         -5.7214e-02, -3.4620e-04, -5.5495e-02, -5.1307e-03,  1.9955e-02,\n",
            "          2.5051e-02, -1.3328e-02, -2.7420e-02,  2.4621e-03, -3.0015e-02,\n",
            "         -1.8220e-02,  6.1585e-04,  8.3058e-02,  1.9194e-02,  6.3585e-04,\n",
            "          8.1361e-02, -8.8124e-03, -4.5810e-02, -4.3675e-02,  8.1441e-02,\n",
            "          9.1861e-02, -2.0676e-03, -9.6083e-02, -7.4219e-03,  3.3699e-02,\n",
            "          7.2434e-02, -2.0031e-02, -1.2369e-02,  9.1696e-02,  2.6549e-03,\n",
            "          5.4434e-02,  2.8954e-02, -5.7393e-02, -1.5268e-02, -6.2657e-03,\n",
            "          4.9166e-02,  3.0592e-02,  3.2405e-02,  9.1469e-02,  6.1314e-02,\n",
            "          3.0075e-02, -1.2736e-01,  8.9420e-02,  1.3806e-02, -4.6619e-02,\n",
            "         -4.6750e-02, -3.8591e-02, -6.8695e-02,  8.4186e-02, -1.1099e-03,\n",
            "         -3.8574e-02,  7.5281e-02, -3.1682e-02,  1.4861e-02,  2.4245e-02,\n",
            "         -2.1347e-02,  1.8223e-02, -4.0250e-02, -1.2762e-02, -1.7452e-02,\n",
            "         -5.4369e-02, -6.7864e-02, -2.5120e-02, -6.3244e-02, -2.7883e-02,\n",
            "         -5.3275e-02, -1.8438e-02,  1.6121e-02, -1.5587e-02,  9.0892e-03,\n",
            "         -5.7568e-04, -4.0977e-02, -2.3433e-02,  4.1306e-02, -5.1756e-03,\n",
            "          8.0066e-03,  9.3750e-03, -8.6685e-02,  3.6014e-02,  7.9268e-02,\n",
            "         -3.7328e-02, -7.7657e-02, -3.4246e-02, -5.4513e-02, -6.2774e-02,\n",
            "         -1.5481e-02, -1.5251e-02, -1.7457e-02, -4.4864e-02,  1.0867e-02,\n",
            "          1.2195e-02,  3.3903e-02,  3.0053e-02, -5.1822e-02,  3.9075e-03,\n",
            "          2.7624e-02,  3.0875e-02,  2.9138e-02, -2.5210e-02, -2.1150e-04,\n",
            "          5.4412e-03, -3.5169e-02,  1.9736e-02,  6.0561e-02,  4.6977e-02,\n",
            "         -2.3551e-02, -7.6335e-02,  9.1215e-02, -5.1959e-02, -1.3736e-02,\n",
            "         -1.0375e-02,  3.0652e-02, -2.8456e-02,  1.2225e-02,  2.0137e-03,\n",
            "         -2.9101e-03,  4.9243e-02,  2.9281e-02,  2.6908e-03, -2.8342e-02,\n",
            "          3.5188e-02,  1.8084e-02, -3.6153e-02,  7.7102e-02, -7.0814e-02,\n",
            "         -4.8912e-02, -1.9003e-02, -1.8844e-02, -6.8237e-02, -3.4363e-02,\n",
            "          3.7950e-02, -7.1095e-02, -1.0941e-02, -5.6795e-02, -1.0954e-02,\n",
            "          2.6377e-02, -3.6507e-02, -1.7172e-02, -2.7975e-02, -1.0867e-02,\n",
            "         -1.1024e-02,  9.9816e-03,  7.0375e-02,  2.1491e-02, -1.5471e-02,\n",
            "         -4.3545e-02, -1.9097e-02, -1.0703e-02, -5.8151e-02, -1.0208e-02,\n",
            "         -6.7039e-03, -6.2725e-03,  1.9937e-02, -3.6868e-02,  3.3676e-02,\n",
            "         -1.0451e-02, -2.2363e-02,  6.6751e-02, -1.0592e-01,  4.7453e-03,\n",
            "          1.0952e-02,  7.5536e-02, -1.7521e-02,  6.7356e-03, -4.1120e-02,\n",
            "          4.7100e-02, -2.1421e-02, -3.5124e-02,  2.7731e-02, -9.2340e-02,\n",
            "         -4.0460e-02, -5.9787e-03, -5.3227e-02, -1.0776e-02, -4.1680e-02,\n",
            "         -6.6201e-02,  6.0488e-02, -2.2413e-02, -6.1868e-02, -8.6470e-02,\n",
            "          3.1080e-02,  1.1223e-02,  3.8764e-02,  3.4475e-02, -8.5087e-04,\n",
            "          2.9632e-02, -3.0020e-03,  1.2885e-02,  8.1995e-02, -2.8190e-02,\n",
            "          3.7884e-02,  3.4160e-02, -2.0463e-06,  4.9431e-02, -3.1795e-03,\n",
            "         -2.2198e-02, -4.2723e-03,  7.8363e-02, -1.5464e-05,  6.2423e-02,\n",
            "         -8.0233e-02,  2.6242e-02, -2.2488e-02, -1.9582e-02,  4.9367e-02,\n",
            "         -8.0917e-02, -3.2118e-02,  5.9052e-02, -3.0573e-03, -1.2769e-02,\n",
            "         -2.1772e-02, -2.3977e-02,  3.5260e-02, -2.6958e-03, -1.1091e-02,\n",
            "         -1.8440e-02,  5.3775e-02, -1.4634e-02, -4.9903e-02, -1.6962e-02,\n",
            "          2.1438e-02],\n",
            "        [-1.2325e-02,  5.1821e-02, -4.0180e-02, -1.4763e-03, -4.0293e-03,\n",
            "         -5.0732e-02, -2.5837e-02, -4.0099e-02,  2.5980e-04,  4.5424e-02,\n",
            "         -4.8384e-03, -4.7434e-04,  1.0951e-02, -3.8376e-02, -2.2015e-02,\n",
            "         -1.6359e-02, -1.7528e-02,  1.1510e-02, -1.9936e-02, -2.1240e-02,\n",
            "          2.6158e-04, -6.5333e-03, -3.1672e-03, -3.1805e-02, -4.1099e-02,\n",
            "          5.2450e-02,  1.6579e-02, -5.2055e-02, -5.5452e-02, -1.0270e-03,\n",
            "         -6.5191e-02,  4.2919e-02, -1.8754e-02, -2.0023e-02,  2.8377e-04,\n",
            "         -3.6574e-02, -5.3842e-03,  5.4964e-02,  1.7719e-02, -4.6750e-02,\n",
            "         -6.6030e-02, -1.9237e-02, -4.4243e-02,  3.2954e-02, -2.3730e-02,\n",
            "         -4.8250e-02, -2.8149e-02,  9.1907e-03,  4.5872e-02, -2.5749e-02,\n",
            "         -1.1715e-02, -3.6310e-02, -4.1570e-02,  5.4239e-02,  9.3282e-03,\n",
            "          2.3495e-02, -5.4031e-02,  6.7171e-02,  2.2858e-02,  2.1112e-02,\n",
            "          1.9395e-02,  2.0539e-02, -2.2133e-02, -3.6865e-02, -1.2652e-02,\n",
            "          3.3802e-02,  1.0501e-02,  5.2806e-03, -2.1949e-03,  6.5138e-02,\n",
            "          1.1902e-02,  8.3535e-03,  9.2084e-03, -1.8547e-02, -5.9586e-02,\n",
            "         -5.8394e-02, -1.2759e-03,  1.9058e-02, -7.4197e-03,  1.9724e-02,\n",
            "         -5.2081e-02,  2.5106e-02,  2.4605e-03, -1.0609e-02,  6.4824e-03,\n",
            "         -9.6079e-03,  3.4650e-02,  1.8518e-02,  2.3490e-02, -3.9151e-03,\n",
            "         -8.9441e-03, -2.8766e-02,  2.1994e-02, -4.4773e-02, -5.6262e-02,\n",
            "          3.8732e-02,  2.7737e-02, -6.6328e-03, -1.3095e-02,  1.7044e-02,\n",
            "         -4.6414e-02, -4.2139e-02, -1.7288e-02, -4.7732e-02, -1.2534e-02,\n",
            "         -3.3038e-02,  2.2926e-02, -3.9636e-02,  1.5779e-03, -1.9083e-03,\n",
            "         -1.7765e-02,  2.6704e-02, -5.9712e-03, -1.7747e-03, -1.8329e-03,\n",
            "         -3.8440e-02,  1.9955e-02,  9.8136e-02, -2.3223e-03, -3.5289e-02,\n",
            "          1.8299e-02,  1.7136e-02,  7.2787e-02,  1.1423e-02, -3.0099e-02,\n",
            "         -2.3217e-02,  1.1904e-02,  4.5405e-02,  3.0007e-03, -1.1171e-02,\n",
            "         -7.3597e-02, -1.2515e-03,  3.7771e-03, -3.9757e-02, -2.0057e-03,\n",
            "          2.4550e-02,  1.9584e-02,  2.0718e-02,  3.1725e-02,  2.7863e-02,\n",
            "          2.9620e-02, -2.6516e-02,  1.5873e-02,  2.1363e-02,  1.8503e-02,\n",
            "         -7.6354e-03,  3.2443e-02,  4.7766e-02, -2.7391e-02,  1.7419e-02,\n",
            "         -6.3691e-03, -2.1356e-02,  5.5831e-02,  2.6426e-02,  2.6454e-02,\n",
            "          9.8319e-03, -2.3614e-03, -3.2609e-02,  5.3174e-03, -8.4884e-02,\n",
            "          4.8847e-02,  1.2064e-02,  2.2744e-02, -4.6689e-02, -5.6244e-03,\n",
            "          5.2979e-02, -4.4189e-02, -2.2077e-02, -6.4338e-02, -3.7542e-02,\n",
            "          3.0451e-02, -2.4179e-02,  3.2386e-02, -2.5214e-03,  3.2465e-02,\n",
            "         -3.6342e-02,  1.0455e-01,  3.7058e-02,  3.3187e-02, -8.3102e-02,\n",
            "         -2.2072e-02,  7.4952e-03, -4.2945e-02,  2.7077e-02,  2.7201e-02,\n",
            "         -1.9531e-02, -5.6180e-03,  1.2591e-02,  6.6503e-02,  1.3209e-02,\n",
            "          5.2616e-03, -3.7692e-02,  3.0304e-02, -2.0958e-02,  1.2548e-02,\n",
            "         -4.9056e-02,  4.4332e-02,  2.6289e-03,  7.8355e-02,  9.7310e-02,\n",
            "         -6.0682e-02, -2.1028e-02, -5.6674e-02, -6.0792e-02, -6.6989e-02,\n",
            "         -1.8556e-02,  4.1078e-02, -2.5069e-02,  4.3169e-02,  3.0620e-03,\n",
            "          2.4369e-02,  4.0690e-03,  6.5480e-02,  1.0609e-02, -2.9637e-02,\n",
            "         -3.1802e-02, -1.6605e-02,  3.4981e-02,  6.4953e-02,  2.5543e-02,\n",
            "          3.5458e-02,  2.4116e-02,  4.1193e-02,  2.9078e-02, -6.3123e-03,\n",
            "         -5.3462e-02, -3.6021e-02, -2.2951e-02, -3.7235e-02,  3.4002e-02,\n",
            "          2.8984e-02,  1.5505e-02,  3.1392e-02,  7.1011e-03,  5.4433e-03,\n",
            "         -4.4142e-02, -1.6929e-02,  2.5230e-02, -3.1229e-02,  2.7277e-02,\n",
            "          5.8183e-02, -4.4230e-03,  5.0034e-02, -1.0886e-02, -4.8355e-03,\n",
            "         -1.4783e-02, -4.7764e-02,  8.7813e-02,  7.2654e-02, -8.7851e-03,\n",
            "         -2.0188e-02,  2.5344e-02,  6.0701e-03,  1.4115e-02,  9.0870e-03,\n",
            "          1.1267e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.3857,  1.6501,  5.2417, -5.0968, -3.9549], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.5231,  1.6009,  4.6687, -5.6806, -3.9721], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256, 12534,  1757,  ...,   511,  3176,  3463],\n",
            "        [50256,  3208,    13,  ...,    12,    32,   290],\n",
            "        [50256,   198,   198,  ...,   345,  3730,   892],\n",
            "        ...,\n",
            "        [50256,  1073,   459,  ..., 39534,    13,   628],\n",
            "        [50256,  4268,   286,  ...,   635,  4999,   416],\n",
            "        [50256,  5421,    12,  ...,   220,   220,   220]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0120,  0.0707, -0.0223,  ..., -0.0054,  0.0334,  0.0361],\n",
            "        [-0.0193,  0.0182, -0.0158,  ..., -0.0498, -0.0172,  0.0217],\n",
            "        [-0.0123,  0.0522, -0.0406,  ...,  0.0139,  0.0088,  0.1131],\n",
            "        [-0.0125,  0.0616, -0.0617,  ..., -0.0171, -0.0326,  0.0290],\n",
            "        [ 0.0585, -0.0241, -0.0307,  ..., -0.0021,  0.0231,  0.0366]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.2041e-02,  7.0690e-02, -2.2295e-02, -3.1046e-02, -6.4468e-03,\n",
            "         -5.3487e-03, -1.4837e-02,  3.2345e-03, -7.3584e-02, -1.3414e-02,\n",
            "          4.9382e-03,  1.3894e-02, -3.3815e-02, -2.7772e-02, -4.3486e-02,\n",
            "         -3.0281e-02, -3.8471e-03,  2.2862e-02,  1.7166e-02,  2.3261e-02,\n",
            "         -3.1081e-02,  4.3181e-02, -1.2289e-02, -3.0174e-02, -1.3652e-02,\n",
            "          2.4950e-02, -3.0336e-02,  3.7479e-02,  8.8135e-03, -1.1404e-02,\n",
            "         -7.3325e-02,  2.4281e-02,  1.0155e-02,  2.2084e-02, -3.5781e-02,\n",
            "         -5.7194e-03,  6.4255e-02,  3.3347e-02,  3.3140e-02,  4.9928e-03,\n",
            "         -3.7267e-02, -4.3770e-03,  3.1215e-02, -1.3844e-02,  1.1127e-02,\n",
            "          8.0306e-03,  2.6668e-02, -7.6883e-02, -4.2667e-02, -7.0670e-02,\n",
            "         -2.2002e-02,  1.3191e-02, -2.2818e-02,  4.6277e-02,  2.9941e-02,\n",
            "          2.4149e-03, -2.3474e-02, -1.5460e-02,  4.7075e-02,  2.0746e-02,\n",
            "         -5.8375e-03,  1.7132e-03, -6.1772e-04, -1.5055e-02, -3.5072e-02,\n",
            "          2.1165e-02,  5.2230e-02, -1.9181e-02,  2.5710e-02,  4.5754e-02,\n",
            "          2.3845e-02, -1.4788e-02, -1.3810e-02,  2.8842e-02, -4.4523e-02,\n",
            "         -3.6096e-02, -1.8095e-02,  7.2364e-02,  6.7293e-02,  2.6411e-02,\n",
            "         -1.5140e-02, -2.7069e-02,  3.1965e-02, -3.2415e-02, -1.1844e-02,\n",
            "          9.8533e-02,  4.8124e-03,  3.2583e-02, -1.5418e-02,  4.5599e-03,\n",
            "          2.0805e-03, -5.4842e-02,  2.4406e-02,  9.4351e-06, -2.3245e-02,\n",
            "         -2.0187e-02, -7.2961e-02, -4.6880e-02,  1.5423e-02, -1.4222e-02,\n",
            "         -4.9750e-02,  2.5540e-03,  9.5446e-03,  1.6843e-02, -1.1208e-02,\n",
            "         -8.0899e-03,  5.0986e-02, -2.3764e-02, -5.1263e-02,  4.3683e-02,\n",
            "         -3.4967e-02, -1.7959e-02, -1.0723e-02, -6.4783e-02, -9.8945e-03,\n",
            "         -7.0373e-03,  7.3167e-04,  3.8394e-02,  4.0763e-02,  6.7359e-02,\n",
            "          1.9448e-02, -5.1909e-03, -3.3238e-02,  1.1065e-01,  4.1649e-02,\n",
            "         -9.3416e-03, -3.1369e-02, -8.6919e-03,  2.5802e-02, -6.9757e-03,\n",
            "          6.4151e-02,  1.0048e-02, -5.6252e-03,  2.6792e-02, -4.1456e-02,\n",
            "          9.7522e-04, -5.1072e-02,  4.2048e-02, -8.3614e-03,  1.2681e-02,\n",
            "         -2.0010e-02, -1.3776e-02,  3.5215e-02, -4.2584e-02, -2.8824e-02,\n",
            "          3.8426e-02, -2.9613e-02, -3.3250e-02,  4.0544e-02, -1.3063e-04,\n",
            "         -1.5778e-02,  5.7177e-02,  7.0742e-03,  4.8608e-03,  8.3042e-03,\n",
            "         -1.8824e-02,  1.5810e-02, -9.5246e-03, -4.5316e-03,  2.4573e-02,\n",
            "         -7.7086e-03, -3.0115e-02, -2.8121e-02, -5.7918e-02, -9.9869e-03,\n",
            "          6.5561e-02, -6.6169e-03, -5.3687e-02, -2.1458e-02, -3.7561e-02,\n",
            "         -3.3085e-03, -1.1948e-02, -5.0903e-03, -2.8733e-02, -3.8806e-02,\n",
            "         -2.2361e-02,  4.0764e-03, -1.5734e-02, -1.6309e-02, -8.6808e-02,\n",
            "          3.2048e-02,  4.3906e-02, -1.7269e-02, -7.6221e-03, -2.2699e-02,\n",
            "         -4.8770e-04,  1.1505e-02,  3.8971e-02,  4.9014e-02,  4.0272e-02,\n",
            "          6.6301e-03, -4.1226e-02, -4.7045e-03,  2.9277e-02, -4.2021e-03,\n",
            "         -2.1780e-02, -1.5414e-01, -2.6337e-02, -4.9850e-02, -3.5853e-02,\n",
            "          8.1421e-03, -1.5030e-02, -2.3230e-02,  4.8771e-02,  4.6963e-02,\n",
            "          3.5413e-02,  1.1176e-02,  2.4086e-02,  2.5287e-02, -2.8043e-02,\n",
            "         -1.6850e-02, -3.5378e-02, -1.5373e-02,  8.6181e-03, -3.9464e-02,\n",
            "         -4.3019e-03, -1.5945e-02,  2.0934e-02,  2.2492e-02,  3.5911e-02,\n",
            "          1.1870e-01,  4.2231e-02,  3.5545e-02,  8.2248e-03,  3.1921e-02,\n",
            "         -9.4979e-02,  1.0494e-02, -6.5398e-03, -3.4548e-03, -8.1673e-03,\n",
            "          1.3083e-02, -4.1512e-03, -7.8005e-03, -4.0770e-02,  4.6218e-02,\n",
            "          2.8201e-02, -1.9964e-02,  1.8303e-02,  1.9473e-02, -2.6387e-02,\n",
            "          1.8961e-02,  1.0657e-02,  1.0181e-03,  7.0768e-04,  7.5621e-03,\n",
            "         -1.2524e-01,  6.3850e-03,  7.4174e-03,  4.8204e-02, -2.6023e-02,\n",
            "          1.3224e-02, -6.2913e-03,  1.6920e-02, -5.3647e-03,  3.3358e-02,\n",
            "          3.6105e-02],\n",
            "        [-1.9313e-02,  1.8200e-02, -1.5765e-02,  4.0121e-02,  1.1686e-01,\n",
            "         -4.2360e-02, -2.5793e-02,  4.2649e-02,  1.9032e-02, -1.0708e-02,\n",
            "          3.2357e-03, -5.1716e-03, -1.3008e-02,  2.1194e-02,  1.0710e-01,\n",
            "          4.7045e-02,  1.1966e-03, -1.7220e-02, -1.1880e-02, -3.9531e-02,\n",
            "          9.9115e-02,  1.2640e-02, -4.7968e-03, -8.8173e-02, -4.2272e-02,\n",
            "         -4.5292e-02,  8.2680e-04, -6.0648e-02,  1.9147e-02, -1.1591e-02,\n",
            "          8.3284e-02, -2.6403e-02, -1.6698e-02,  7.5221e-03,  3.2800e-02,\n",
            "         -5.6953e-02, -4.6819e-04, -5.5272e-02, -5.0303e-03,  2.0128e-02,\n",
            "          2.4994e-02, -1.3689e-02, -2.7420e-02,  2.4651e-03, -2.9948e-02,\n",
            "         -1.8485e-02,  7.0962e-04,  8.2972e-02,  1.8883e-02,  4.8689e-04,\n",
            "          8.1475e-02, -8.8416e-03, -4.6088e-02, -4.3872e-02,  8.1329e-02,\n",
            "          9.1940e-02, -1.9698e-03, -9.6227e-02, -7.2410e-03,  3.4047e-02,\n",
            "          7.2191e-02, -1.9969e-02, -1.2303e-02,  9.1683e-02,  2.8895e-03,\n",
            "          5.4402e-02,  2.9123e-02, -5.7442e-02, -1.4917e-02, -5.8712e-03,\n",
            "          4.9184e-02,  3.0460e-02,  3.2254e-02,  9.1728e-02,  6.1285e-02,\n",
            "          3.0054e-02, -1.2760e-01,  8.9299e-02,  1.3776e-02, -4.6552e-02,\n",
            "         -4.6707e-02, -3.8787e-02, -6.8681e-02,  8.4315e-02, -1.4934e-03,\n",
            "         -3.8700e-02,  7.5263e-02, -3.1649e-02,  1.5246e-02,  2.4170e-02,\n",
            "         -2.1037e-02,  1.8415e-02, -4.0123e-02, -1.2554e-02, -1.7472e-02,\n",
            "         -5.4536e-02, -6.8068e-02, -2.5274e-02, -6.3309e-02, -2.7664e-02,\n",
            "         -5.3280e-02, -1.8519e-02,  1.5720e-02, -1.5668e-02,  9.1812e-03,\n",
            "         -5.2972e-04, -4.1229e-02, -2.3658e-02,  4.1303e-02, -5.4247e-03,\n",
            "          7.9684e-03,  9.3030e-03, -8.6902e-02,  3.5872e-02,  7.9361e-02,\n",
            "         -3.7013e-02, -7.7385e-02, -3.4262e-02, -5.4589e-02, -6.2922e-02,\n",
            "         -1.5253e-02, -1.5309e-02, -1.7154e-02, -4.4992e-02,  1.1085e-02,\n",
            "          1.2107e-02,  3.4002e-02,  2.9993e-02, -5.2055e-02,  3.8045e-03,\n",
            "          2.7675e-02,  3.0937e-02,  2.9326e-02, -2.5141e-02, -1.8150e-04,\n",
            "          5.5765e-03, -3.5393e-02,  1.9639e-02,  6.0692e-02,  4.7161e-02,\n",
            "         -2.3534e-02, -7.6357e-02,  9.1136e-02, -5.2068e-02, -1.3980e-02,\n",
            "         -1.0650e-02,  3.0464e-02, -2.8542e-02,  1.1915e-02,  1.7969e-03,\n",
            "         -3.1251e-03,  4.8926e-02,  2.9650e-02,  2.5965e-03, -2.8140e-02,\n",
            "          3.5097e-02,  1.8538e-02, -3.5952e-02,  7.7006e-02, -7.0900e-02,\n",
            "         -4.9000e-02, -1.8944e-02, -1.8947e-02, -6.8077e-02, -3.3926e-02,\n",
            "          3.8008e-02, -7.1065e-02, -1.1067e-02, -5.6690e-02, -1.0934e-02,\n",
            "          2.6235e-02, -3.6461e-02, -1.6982e-02, -2.8283e-02, -1.0805e-02,\n",
            "         -1.0895e-02,  1.0205e-02,  7.0516e-02,  2.1522e-02, -1.5370e-02,\n",
            "         -4.3604e-02, -1.8950e-02, -1.0871e-02, -5.8279e-02, -1.0171e-02,\n",
            "         -6.5675e-03, -6.2941e-03,  2.0118e-02, -3.6889e-02,  3.3805e-02,\n",
            "         -1.0660e-02, -2.2308e-02,  6.6832e-02, -1.0591e-01,  4.6095e-03,\n",
            "          1.1168e-02,  7.5689e-02, -1.7817e-02,  6.7886e-03, -4.1321e-02,\n",
            "          4.7006e-02, -2.1554e-02, -3.5004e-02,  2.7983e-02, -9.2237e-02,\n",
            "         -4.0507e-02, -5.9028e-03, -5.3102e-02, -1.0901e-02, -4.1512e-02,\n",
            "         -6.6154e-02,  6.0467e-02, -2.2492e-02, -6.1791e-02, -8.6154e-02,\n",
            "          3.1122e-02,  1.1574e-02,  3.9114e-02,  3.4652e-02, -8.8326e-04,\n",
            "          2.9520e-02, -3.1672e-03,  1.3070e-02,  8.1721e-02, -2.8024e-02,\n",
            "          3.8061e-02,  3.4190e-02,  3.1898e-05,  4.9702e-02, -3.3981e-03,\n",
            "         -2.2328e-02, -4.3774e-03,  7.8565e-02, -1.6090e-04,  6.2381e-02,\n",
            "         -8.0127e-02,  2.6333e-02, -2.2589e-02, -1.9017e-02,  4.9320e-02,\n",
            "         -8.0540e-02, -3.2572e-02,  5.9127e-02, -3.0988e-03, -1.2891e-02,\n",
            "         -2.1943e-02, -2.3877e-02,  3.5488e-02, -2.7208e-03, -1.1107e-02,\n",
            "         -1.8651e-02,  5.3336e-02, -1.4837e-02, -4.9768e-02, -1.7195e-02,\n",
            "          2.1717e-02],\n",
            "        [-1.2334e-02,  5.2175e-02, -4.0630e-02, -1.4436e-03, -4.1741e-03,\n",
            "         -5.0182e-02, -2.6081e-02, -3.9904e-02,  4.9575e-04,  4.5517e-02,\n",
            "         -4.6277e-03, -4.8482e-04,  1.0333e-02, -3.8642e-02, -2.2058e-02,\n",
            "         -1.6181e-02, -1.7721e-02,  1.1333e-02, -1.9810e-02, -2.1419e-02,\n",
            "          1.8475e-04, -6.7602e-03, -2.7959e-03, -3.1705e-02, -4.1149e-02,\n",
            "          5.2331e-02,  1.6280e-02, -5.1968e-02, -5.5898e-02, -9.6630e-04,\n",
            "         -6.4986e-02,  4.3684e-02, -1.8717e-02, -2.0341e-02,  2.6365e-04,\n",
            "         -3.6217e-02, -5.7779e-03,  5.5076e-02,  1.7741e-02, -4.6710e-02,\n",
            "         -6.5936e-02, -1.9492e-02, -4.4354e-02,  3.2856e-02, -2.3723e-02,\n",
            "         -4.8725e-02, -2.7802e-02,  8.8901e-03,  4.5614e-02, -2.5819e-02,\n",
            "         -1.1786e-02, -3.6019e-02, -4.1863e-02,  5.4020e-02,  9.3967e-03,\n",
            "          2.3325e-02, -5.3842e-02,  6.6678e-02,  2.2965e-02,  2.1579e-02,\n",
            "          1.8828e-02,  2.0620e-02, -2.2207e-02, -3.7175e-02, -1.2652e-02,\n",
            "          3.3790e-02,  1.0646e-02,  5.8169e-03, -1.9234e-03,  6.5545e-02,\n",
            "          1.1723e-02,  8.2947e-03,  8.7733e-03, -1.8588e-02, -5.9801e-02,\n",
            "         -5.8346e-02, -1.0111e-03,  1.9109e-02, -7.6431e-03,  2.0312e-02,\n",
            "         -5.1915e-02,  2.4917e-02,  2.7133e-03, -1.0584e-02,  6.1038e-03,\n",
            "         -9.3747e-03,  3.4819e-02,  1.8679e-02,  2.3686e-02, -3.9863e-03,\n",
            "         -8.6512e-03, -2.8917e-02,  2.2140e-02, -4.4994e-02, -5.6388e-02,\n",
            "          3.8960e-02,  2.7329e-02, -6.3157e-03, -1.3089e-02,  1.6825e-02,\n",
            "         -4.6531e-02, -4.1896e-02, -1.7743e-02, -4.7706e-02, -1.2282e-02,\n",
            "         -3.2631e-02,  2.2733e-02, -3.9727e-02,  1.7466e-03, -2.1436e-03,\n",
            "         -1.7677e-02,  2.6858e-02, -5.9279e-03, -2.2423e-03, -1.8622e-03,\n",
            "         -3.8464e-02,  2.0146e-02,  9.8076e-02, -2.3546e-03, -3.5233e-02,\n",
            "          1.8099e-02,  1.7217e-02,  7.2976e-02,  1.1258e-02, -2.9879e-02,\n",
            "         -2.3335e-02,  1.2229e-02,  4.5267e-02,  2.9690e-03, -1.1698e-02,\n",
            "         -7.3457e-02, -1.2617e-03,  3.4785e-03, -3.9651e-02, -1.7929e-03,\n",
            "          2.4780e-02,  1.9308e-02,  2.0408e-02,  3.1804e-02,  2.7960e-02,\n",
            "          2.9927e-02, -2.6060e-02,  1.5946e-02,  2.1072e-02,  1.8277e-02,\n",
            "         -7.8603e-03,  3.2326e-02,  4.8016e-02, -2.7261e-02,  1.7375e-02,\n",
            "         -6.7935e-03, -2.1638e-02,  5.6330e-02,  2.6475e-02,  2.6515e-02,\n",
            "          9.4719e-03, -2.1469e-03, -3.2335e-02,  5.2076e-03, -8.5167e-02,\n",
            "          4.8929e-02,  1.2272e-02,  2.3155e-02, -4.6334e-02, -5.1353e-03,\n",
            "          5.2964e-02, -4.4081e-02, -2.2339e-02, -6.4530e-02, -3.7553e-02,\n",
            "          3.0275e-02, -2.3999e-02,  3.2358e-02, -2.5921e-03,  3.2701e-02,\n",
            "         -3.5907e-02,  1.0482e-01,  3.7475e-02,  3.3498e-02, -8.3200e-02,\n",
            "         -2.1881e-02,  7.4973e-03, -4.2880e-02,  2.6954e-02,  2.7161e-02,\n",
            "         -1.9267e-02, -5.2897e-03,  1.2952e-02,  6.6636e-02,  1.3378e-02,\n",
            "          4.8831e-03, -3.7742e-02,  3.0438e-02, -2.0658e-02,  1.2688e-02,\n",
            "         -4.9032e-02,  4.4347e-02,  2.5801e-03,  7.8415e-02,  9.7201e-02,\n",
            "         -6.0712e-02, -2.1216e-02, -5.6548e-02, -6.0595e-02, -6.6840e-02,\n",
            "         -1.8755e-02,  4.1198e-02, -2.4903e-02,  4.3061e-02,  2.8071e-03,\n",
            "          2.4584e-02,  3.6344e-03,  6.5400e-02,  1.0380e-02, -2.9664e-02,\n",
            "         -3.2088e-02, -1.6350e-02,  3.4915e-02,  6.5020e-02,  2.5838e-02,\n",
            "          3.5385e-02,  2.4236e-02,  4.1185e-02,  2.8953e-02, -5.9699e-03,\n",
            "         -5.3562e-02, -3.6263e-02, -2.2866e-02, -3.7391e-02,  3.4120e-02,\n",
            "          2.9281e-02,  1.5197e-02,  3.1519e-02,  6.8002e-03,  5.1909e-03,\n",
            "         -4.4270e-02, -1.6771e-02,  2.5123e-02, -3.1200e-02,  2.6939e-02,\n",
            "          5.8721e-02, -4.3969e-03,  4.9839e-02, -1.1208e-02, -5.1516e-03,\n",
            "         -1.4868e-02, -4.7813e-02,  8.8190e-02,  7.2415e-02, -8.9591e-03,\n",
            "         -2.0030e-02,  2.4634e-02,  5.8820e-03,  1.3903e-02,  8.7748e-03,\n",
            "          1.1312e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.3375,  1.7815,  5.3284, -5.1723, -4.1394], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.5566,  1.7114,  4.7175, -5.7916, -4.2059], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,  1176,  7560,  ...,  1080,   318,  7531],\n",
            "        [50256,   262, 15075,  ...,   685,    31,    65],\n",
            "        [50256,    13,  2816,  ...,   555, 26662,   276],\n",
            "        ...,\n",
            "        [50256,   611,   428,  ...,     7, 50139,    25],\n",
            "        [50256,   220,   220,  ...,   220,   220,   220],\n",
            "        [50256,  5057,   669,  ...,    13,   198,  8021]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0117,  0.0705, -0.0223,  ..., -0.0051,  0.0332,  0.0359],\n",
            "        [-0.0194,  0.0181, -0.0161,  ..., -0.0495, -0.0174,  0.0218],\n",
            "        [-0.0125,  0.0524, -0.0411,  ...,  0.0138,  0.0087,  0.1135],\n",
            "        [-0.0127,  0.0619, -0.0620,  ..., -0.0168, -0.0324,  0.0290],\n",
            "        [ 0.0583, -0.0241, -0.0309,  ..., -0.0020,  0.0231,  0.0368]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1693e-02,  7.0522e-02, -2.2333e-02, -3.0849e-02, -6.2684e-03,\n",
            "         -5.4950e-03, -1.4897e-02,  3.1214e-03, -7.3887e-02, -1.3557e-02,\n",
            "          4.8003e-03,  1.3812e-02, -3.3807e-02, -2.7839e-02, -4.3753e-02,\n",
            "         -3.0324e-02, -4.0317e-03,  2.2731e-02,  1.7471e-02,  2.3403e-02,\n",
            "         -3.1263e-02,  4.3204e-02, -1.2512e-02, -3.0042e-02, -1.3459e-02,\n",
            "          2.5093e-02, -3.0222e-02,  3.7799e-02,  8.8663e-03, -1.1315e-02,\n",
            "         -7.3237e-02,  2.4057e-02,  1.0194e-02,  2.2309e-02, -3.6232e-02,\n",
            "         -5.6309e-03,  6.4597e-02,  3.3534e-02,  3.3137e-02,  5.0667e-03,\n",
            "         -3.6898e-02, -4.4869e-03,  3.1340e-02, -1.3902e-02,  1.1334e-02,\n",
            "          7.9725e-03,  2.6791e-02, -7.6945e-02, -4.2862e-02, -7.0553e-02,\n",
            "         -2.2163e-02,  1.3171e-02, -2.2806e-02,  4.6223e-02,  3.0000e-02,\n",
            "          2.4856e-03, -2.3406e-02, -1.5122e-02,  4.7453e-02,  2.0648e-02,\n",
            "         -5.7762e-03,  1.3630e-03, -3.1946e-04, -1.4910e-02, -3.4745e-02,\n",
            "          2.0976e-02,  5.2170e-02, -1.8962e-02,  2.5821e-02,  4.5854e-02,\n",
            "          2.3673e-02, -1.4706e-02, -1.3713e-02,  2.8862e-02, -4.4531e-02,\n",
            "         -3.6232e-02, -1.7877e-02,  7.2526e-02,  6.7475e-02,  2.6564e-02,\n",
            "         -1.5264e-02, -2.7044e-02,  3.1875e-02, -3.2901e-02, -1.1684e-02,\n",
            "          9.8474e-02,  4.6721e-03,  3.2602e-02, -1.5451e-02,  4.7465e-03,\n",
            "          1.7711e-03, -5.4698e-02,  2.4595e-02,  2.4659e-05, -2.3428e-02,\n",
            "         -2.0495e-02, -7.2848e-02, -4.6703e-02,  1.5555e-02, -1.4199e-02,\n",
            "         -4.9871e-02,  2.5327e-03,  9.3687e-03,  1.6873e-02, -1.1039e-02,\n",
            "         -8.1550e-03,  5.0961e-02, -2.3565e-02, -5.1299e-02,  4.3566e-02,\n",
            "         -3.5077e-02, -1.8100e-02, -1.0790e-02, -6.4806e-02, -9.9439e-03,\n",
            "         -6.9678e-03,  8.7531e-04,  3.8529e-02,  4.0689e-02,  6.7118e-02,\n",
            "          1.9494e-02, -5.3019e-03, -3.3044e-02,  1.1061e-01,  4.1777e-02,\n",
            "         -9.4018e-03, -3.1528e-02, -8.6862e-03,  2.5985e-02, -6.7098e-03,\n",
            "          6.4320e-02,  1.0050e-02, -5.8617e-03,  2.7076e-02, -4.1672e-02,\n",
            "          1.1747e-03, -5.1211e-02,  4.2304e-02, -8.2832e-03,  1.2726e-02,\n",
            "         -2.0311e-02, -1.3843e-02,  3.5136e-02, -4.2333e-02, -2.8832e-02,\n",
            "          3.8666e-02, -2.9931e-02, -3.3530e-02,  4.0667e-02,  7.4158e-05,\n",
            "         -1.5909e-02,  5.7215e-02,  7.0802e-03,  4.7187e-03,  8.2107e-03,\n",
            "         -1.8725e-02,  1.6146e-02, -9.4971e-03, -4.4276e-03,  2.4635e-02,\n",
            "         -7.5133e-03, -3.0051e-02, -2.8138e-02, -5.7664e-02, -9.9404e-03,\n",
            "          6.5732e-02, -6.3021e-03, -5.3661e-02, -2.1251e-02, -3.7819e-02,\n",
            "         -3.3673e-03, -1.2405e-02, -4.8302e-03, -2.8864e-02, -3.9161e-02,\n",
            "         -2.2292e-02,  3.7657e-03, -1.6162e-02, -1.6790e-02, -8.6803e-02,\n",
            "          3.1982e-02,  4.3902e-02, -1.7259e-02, -7.8308e-03, -2.2940e-02,\n",
            "         -5.4517e-04,  1.1618e-02,  3.8911e-02,  4.8942e-02,  4.0251e-02,\n",
            "          6.7451e-03, -4.1019e-02, -4.8002e-03,  2.9426e-02, -4.4947e-03,\n",
            "         -2.1744e-02, -1.5434e-01, -2.6660e-02, -4.9782e-02, -3.5686e-02,\n",
            "          8.4310e-03, -1.5168e-02, -2.2942e-02,  4.8916e-02,  4.6968e-02,\n",
            "          3.5393e-02,  1.1193e-02,  2.4191e-02,  2.5313e-02, -2.7604e-02,\n",
            "         -1.7202e-02, -3.5522e-02, -1.5737e-02,  8.7074e-03, -3.9344e-02,\n",
            "         -4.3597e-03, -1.5991e-02,  2.1030e-02,  2.2101e-02,  3.5859e-02,\n",
            "          1.1844e-01,  4.2076e-02,  3.5876e-02,  8.0476e-03,  3.1898e-02,\n",
            "         -9.4821e-02,  1.0549e-02, -6.7957e-03, -3.3480e-03, -8.4360e-03,\n",
            "          1.3394e-02, -4.1011e-03, -7.6693e-03, -4.0607e-02,  4.6156e-02,\n",
            "          2.8535e-02, -1.9969e-02,  1.8353e-02,  1.9514e-02, -2.6040e-02,\n",
            "          1.8845e-02,  1.0502e-02,  9.0823e-04,  9.9643e-04,  7.2778e-03,\n",
            "         -1.2505e-01,  6.6635e-03,  7.2495e-03,  4.8250e-02, -2.6104e-02,\n",
            "          1.3117e-02, -6.3817e-03,  1.6763e-02, -5.0759e-03,  3.3236e-02,\n",
            "          3.5934e-02],\n",
            "        [-1.9432e-02,  1.8121e-02, -1.6142e-02,  4.0446e-02,  1.1678e-01,\n",
            "         -4.2379e-02, -2.5918e-02,  4.2759e-02,  1.9039e-02, -1.0723e-02,\n",
            "          2.8752e-03, -5.1675e-03, -1.2970e-02,  2.0774e-02,  1.0700e-01,\n",
            "          4.7032e-02,  1.2445e-03, -1.7195e-02, -1.1606e-02, -3.9813e-02,\n",
            "          9.9121e-02,  1.2403e-02, -4.8237e-03, -8.8195e-02, -4.2292e-02,\n",
            "         -4.5434e-02,  7.9777e-04, -6.0375e-02,  1.9245e-02, -1.1723e-02,\n",
            "          8.3264e-02, -2.6383e-02, -1.6679e-02,  7.4581e-03,  3.2949e-02,\n",
            "         -5.6835e-02, -5.1702e-04, -5.5045e-02, -4.9187e-03,  2.0307e-02,\n",
            "          2.5013e-02, -1.3960e-02, -2.7373e-02,  2.3942e-03, -2.9866e-02,\n",
            "         -1.8712e-02,  7.6160e-04,  8.2993e-02,  1.8650e-02,  3.9124e-04,\n",
            "          8.1589e-02, -8.8722e-03, -4.6251e-02, -4.3956e-02,  8.1255e-02,\n",
            "          9.2053e-02, -1.8908e-03, -9.6283e-02, -7.1155e-03,  3.4318e-02,\n",
            "          7.2119e-02, -1.9962e-02, -1.2147e-02,  9.1734e-02,  3.0996e-03,\n",
            "          5.4290e-02,  2.9230e-02, -5.7541e-02, -1.4595e-02, -5.5672e-03,\n",
            "          4.9165e-02,  3.0422e-02,  3.2221e-02,  9.1986e-02,  6.1226e-02,\n",
            "          2.9947e-02, -1.2783e-01,  8.9142e-02,  1.3713e-02, -4.6693e-02,\n",
            "         -4.6758e-02, -3.8938e-02, -6.8714e-02,  8.4336e-02, -1.7944e-03,\n",
            "         -3.8886e-02,  7.5132e-02, -3.1589e-02,  1.5626e-02,  2.4196e-02,\n",
            "         -2.0796e-02,  1.8581e-02, -4.0106e-02, -1.2273e-02, -1.7558e-02,\n",
            "         -5.4662e-02, -6.8224e-02, -2.5289e-02, -6.3355e-02, -2.7530e-02,\n",
            "         -5.3284e-02, -1.8606e-02,  1.5367e-02, -1.5776e-02,  9.2779e-03,\n",
            "         -4.6381e-04, -4.1472e-02, -2.3878e-02,  4.1275e-02, -5.7167e-03,\n",
            "          7.9451e-03,  9.1997e-03, -8.7038e-02,  3.5756e-02,  7.9374e-02,\n",
            "         -3.6827e-02, -7.7192e-02, -3.4230e-02, -5.4695e-02, -6.3061e-02,\n",
            "         -1.5021e-02, -1.5496e-02, -1.6853e-02, -4.5138e-02,  1.1203e-02,\n",
            "          1.2060e-02,  3.4116e-02,  2.9858e-02, -5.2265e-02,  3.8572e-03,\n",
            "          2.7657e-02,  3.1059e-02,  2.9533e-02, -2.5097e-02, -1.9440e-04,\n",
            "          5.6447e-03, -3.5450e-02,  1.9594e-02,  6.0614e-02,  4.7313e-02,\n",
            "         -2.3492e-02, -7.6437e-02,  9.1088e-02, -5.2117e-02, -1.4089e-02,\n",
            "         -1.0804e-02,  3.0335e-02, -2.8811e-02,  1.1635e-02,  1.7133e-03,\n",
            "         -3.2405e-03,  4.8772e-02,  2.9847e-02,  2.5388e-03, -2.8018e-02,\n",
            "          3.5027e-02,  1.8927e-02, -3.5790e-02,  7.6900e-02, -7.0848e-02,\n",
            "         -4.9075e-02, -1.8908e-02, -1.9023e-02, -6.7885e-02, -3.3610e-02,\n",
            "          3.7982e-02, -7.1055e-02, -1.1098e-02, -5.6605e-02, -1.0854e-02,\n",
            "          2.6095e-02, -3.6435e-02, -1.6724e-02, -2.8521e-02, -1.0739e-02,\n",
            "         -1.0902e-02,  1.0330e-02,  7.0563e-02,  2.1456e-02, -1.5222e-02,\n",
            "         -4.3668e-02, -1.8865e-02, -1.0991e-02, -5.8403e-02, -1.0164e-02,\n",
            "         -6.4829e-03, -6.2955e-03,  2.0220e-02, -3.6932e-02,  3.3875e-02,\n",
            "         -1.0830e-02, -2.2147e-02,  6.6866e-02, -1.0588e-01,  4.5128e-03,\n",
            "          1.1344e-02,  7.5888e-02, -1.8154e-02,  6.8365e-03, -4.1511e-02,\n",
            "          4.6873e-02, -2.1633e-02, -3.4838e-02,  2.8187e-02, -9.2191e-02,\n",
            "         -4.0643e-02, -5.7057e-03, -5.3102e-02, -1.1037e-02, -4.1382e-02,\n",
            "         -6.6094e-02,  6.0510e-02, -2.2545e-02, -6.1695e-02, -8.5816e-02,\n",
            "          3.1187e-02,  1.1829e-02,  3.9487e-02,  3.4750e-02, -8.6580e-04,\n",
            "          2.9331e-02, -3.3366e-03,  1.3210e-02,  8.1501e-02, -2.7908e-02,\n",
            "          3.8233e-02,  3.4198e-02,  1.5543e-04,  4.9886e-02, -3.5897e-03,\n",
            "         -2.2415e-02, -4.4527e-03,  7.8751e-02, -3.0059e-04,  6.2418e-02,\n",
            "         -7.9848e-02,  2.6394e-02, -2.2677e-02, -1.8529e-02,  4.9358e-02,\n",
            "         -8.0363e-02, -3.2957e-02,  5.9225e-02, -3.1364e-03, -1.2966e-02,\n",
            "         -2.2064e-02, -2.3705e-02,  3.5580e-02, -2.7004e-03, -1.1104e-02,\n",
            "         -1.8872e-02,  5.3097e-02, -1.4960e-02, -4.9521e-02, -1.7368e-02,\n",
            "          2.1819e-02],\n",
            "        [-1.2510e-02,  5.2440e-02, -4.1066e-02, -1.5368e-03, -4.2123e-03,\n",
            "         -4.9784e-02, -2.6242e-02, -3.9766e-02,  6.3609e-04,  4.5623e-02,\n",
            "         -4.6497e-03, -4.2256e-04,  9.7201e-03, -3.8809e-02, -2.1988e-02,\n",
            "         -1.6126e-02, -1.7954e-02,  1.1076e-02, -1.9597e-02, -2.1730e-02,\n",
            "         -3.3919e-05, -6.8300e-03, -2.6032e-03, -3.1643e-02, -4.1286e-02,\n",
            "          5.2135e-02,  1.6172e-02, -5.1835e-02, -5.6278e-02, -8.4500e-04,\n",
            "         -6.4858e-02,  4.4170e-02, -1.8794e-02, -2.0541e-02,  1.7095e-04,\n",
            "         -3.5910e-02, -6.0282e-03,  5.5299e-02,  1.7792e-02, -4.6798e-02,\n",
            "         -6.5801e-02, -1.9718e-02, -4.4476e-02,  3.2775e-02, -2.3577e-02,\n",
            "         -4.9132e-02, -2.7584e-02,  8.6439e-03,  4.5428e-02, -2.5980e-02,\n",
            "         -1.1833e-02, -3.5731e-02, -4.2121e-02,  5.3834e-02,  9.4921e-03,\n",
            "          2.3275e-02, -5.3607e-02,  6.6326e-02,  2.3090e-02,  2.2013e-02,\n",
            "          1.8497e-02,  2.0698e-02, -2.2308e-02, -3.7353e-02, -1.2546e-02,\n",
            "          3.3713e-02,  1.0801e-02,  6.3175e-03, -1.7255e-03,  6.5947e-02,\n",
            "          1.1528e-02,  8.2884e-03,  8.5294e-03, -1.8613e-02, -5.9907e-02,\n",
            "         -5.8345e-02, -7.6861e-04,  1.9267e-02, -7.7326e-03,  2.0875e-02,\n",
            "         -5.1976e-02,  2.4749e-02,  2.8705e-03, -1.0578e-02,  5.7258e-03,\n",
            "         -9.1598e-03,  3.5019e-02,  1.8741e-02,  2.3794e-02, -4.0486e-03,\n",
            "         -8.3664e-03, -2.8996e-02,  2.2164e-02, -4.5184e-02, -5.6574e-02,\n",
            "          3.9108e-02,  2.6944e-02, -6.0252e-03, -1.3255e-02,  1.6673e-02,\n",
            "         -4.6588e-02, -4.1688e-02, -1.8256e-02, -4.7650e-02, -1.2026e-02,\n",
            "         -3.2210e-02,  2.2636e-02, -3.9728e-02,  1.8639e-03, -2.2668e-03,\n",
            "         -1.7649e-02,  2.6895e-02, -5.8715e-03, -2.5993e-03, -1.9411e-03,\n",
            "         -3.8509e-02,  2.0242e-02,  9.8085e-02, -2.4526e-03, -3.5296e-02,\n",
            "          1.8024e-02,  1.7256e-02,  7.3160e-02,  1.1149e-02, -2.9708e-02,\n",
            "         -2.3371e-02,  1.2436e-02,  4.5013e-02,  2.9831e-03, -1.1997e-02,\n",
            "         -7.3370e-02, -1.2342e-03,  3.2109e-03, -3.9465e-02, -1.7092e-03,\n",
            "          2.4892e-02,  1.9057e-02,  2.0242e-02,  3.1908e-02,  2.8031e-02,\n",
            "          3.0176e-02, -2.5681e-02,  1.6047e-02,  2.0864e-02,  1.8125e-02,\n",
            "         -8.0251e-03,  3.2230e-02,  4.8201e-02, -2.7150e-02,  1.7356e-02,\n",
            "         -7.1980e-03, -2.1825e-02,  5.6631e-02,  2.6538e-02,  2.6410e-02,\n",
            "          9.2904e-03, -1.9436e-03, -3.2186e-02,  5.2251e-03, -8.5341e-02,\n",
            "          4.9018e-02,  1.2486e-02,  2.3443e-02, -4.5993e-02, -4.6146e-03,\n",
            "          5.3012e-02, -4.3854e-02, -2.2637e-02, -6.4667e-02, -3.7545e-02,\n",
            "          3.0175e-02, -2.3944e-02,  3.2445e-02, -2.6832e-03,  3.2938e-02,\n",
            "         -3.5631e-02,  1.0491e-01,  3.7749e-02,  3.3720e-02, -8.3287e-02,\n",
            "         -2.1725e-02,  7.5929e-03, -4.2814e-02,  2.6752e-02,  2.7054e-02,\n",
            "         -1.9112e-02, -5.0104e-03,  1.3232e-02,  6.6658e-02,  1.3461e-02,\n",
            "          4.6342e-03, -3.7913e-02,  3.0564e-02, -2.0534e-02,  1.2679e-02,\n",
            "         -4.8997e-02,  4.4386e-02,  2.4668e-03,  7.8462e-02,  9.7098e-02,\n",
            "         -6.0588e-02, -2.1391e-02, -5.6414e-02, -6.0474e-02, -6.6721e-02,\n",
            "         -1.8935e-02,  4.1369e-02, -2.4713e-02,  4.3046e-02,  2.7276e-03,\n",
            "          2.4730e-02,  3.2890e-03,  6.5228e-02,  1.0241e-02, -2.9794e-02,\n",
            "         -3.2191e-02, -1.6229e-02,  3.4883e-02,  6.5067e-02,  2.6108e-02,\n",
            "          3.5375e-02,  2.4215e-02,  4.1295e-02,  2.8859e-02, -5.6845e-03,\n",
            "         -5.3625e-02, -3.6324e-02, -2.2775e-02, -3.7684e-02,  3.4159e-02,\n",
            "          2.9560e-02,  1.4966e-02,  3.1619e-02,  6.4633e-03,  5.0123e-03,\n",
            "         -4.4376e-02, -1.6737e-02,  2.4926e-02, -3.1232e-02,  2.6800e-02,\n",
            "          5.9174e-02, -4.3987e-03,  4.9776e-02, -1.1387e-02, -5.4918e-03,\n",
            "         -1.4904e-02, -4.7607e-02,  8.8433e-02,  7.2341e-02, -9.0893e-03,\n",
            "         -2.0039e-02,  2.4074e-02,  5.8051e-03,  1.3801e-02,  8.6953e-03,\n",
            "          1.1355e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.2427,  1.7883,  5.2985, -5.2212, -4.3220], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.5272,  1.7057,  4.6315, -5.8502, -4.4615], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,    13,    15,  ...,   220,   220,   220],\n",
            "        [50256,     8,   393,  ...,  3826, 25458,   828],\n",
            "        [50256,  1314,    11,  ..., 45739,   110, 28618],\n",
            "        ...,\n",
            "        [50256,   256, 40593,  ...,   262, 30773,   373],\n",
            "        [50256,    12,    18,  ..., 11283,    11,  5433],\n",
            "        [50256,   317,  4825,  ...,    51, 33994,    66]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0111,  0.0705, -0.0223,  ..., -0.0046,  0.0332,  0.0357],\n",
            "        [-0.0196,  0.0181, -0.0165,  ..., -0.0491, -0.0175,  0.0219],\n",
            "        [-0.0127,  0.0526, -0.0413,  ...,  0.0140,  0.0087,  0.1138],\n",
            "        [-0.0128,  0.0623, -0.0623,  ..., -0.0165, -0.0322,  0.0290],\n",
            "        [ 0.0582, -0.0241, -0.0312,  ..., -0.0018,  0.0231,  0.0372]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.1147e-02,  7.0474e-02, -2.2257e-02, -3.0957e-02, -6.0752e-03,\n",
            "         -5.6959e-03, -1.4751e-02,  2.9917e-03, -7.4172e-02, -1.3559e-02,\n",
            "          4.5406e-03,  1.3561e-02, -3.3761e-02, -2.8026e-02, -4.3820e-02,\n",
            "         -3.0518e-02, -4.3227e-03,  2.2434e-02,  1.7650e-02,  2.3416e-02,\n",
            "         -3.1445e-02,  4.3164e-02, -1.2764e-02, -2.9929e-02, -1.3471e-02,\n",
            "          2.5237e-02, -2.9949e-02,  3.8052e-02,  8.9551e-03, -1.1215e-02,\n",
            "         -7.3213e-02,  2.3719e-02,  1.0461e-02,  2.2512e-02, -3.6567e-02,\n",
            "         -5.5973e-03,  6.5070e-02,  3.3723e-02,  3.3006e-02,  5.0907e-03,\n",
            "         -3.6570e-02, -4.6050e-03,  3.1372e-02, -1.4043e-02,  1.1741e-02,\n",
            "          7.8203e-03,  2.6902e-02, -7.6882e-02, -4.2909e-02, -7.0506e-02,\n",
            "         -2.2356e-02,  1.3134e-02, -2.2795e-02,  4.6279e-02,  3.0030e-02,\n",
            "          2.6514e-03, -2.3238e-02, -1.4738e-02,  4.7802e-02,  2.0720e-02,\n",
            "         -5.5470e-03,  9.6932e-04, -1.7312e-04, -1.4646e-02, -3.4217e-02,\n",
            "          2.0721e-02,  5.2009e-02, -1.8613e-02,  2.5806e-02,  4.5923e-02,\n",
            "          2.3402e-02, -1.4747e-02, -1.3663e-02,  2.8837e-02, -4.4442e-02,\n",
            "         -3.6553e-02, -1.7662e-02,  7.2714e-02,  6.7626e-02,  2.6713e-02,\n",
            "         -1.5461e-02, -2.6867e-02,  3.1742e-02, -3.3399e-02, -1.1675e-02,\n",
            "          9.8576e-02,  4.4750e-03,  3.2568e-02, -1.5328e-02,  4.9835e-03,\n",
            "          1.5337e-03, -5.4490e-02,  2.4552e-02,  1.5731e-04, -2.3502e-02,\n",
            "         -2.0572e-02, -7.2938e-02, -4.6217e-02,  1.5716e-02, -1.4287e-02,\n",
            "         -4.9863e-02,  2.5205e-03,  8.9944e-03,  1.6836e-02, -1.0891e-02,\n",
            "         -8.0734e-03,  5.0991e-02, -2.3409e-02, -5.1198e-02,  4.3454e-02,\n",
            "         -3.5168e-02, -1.8343e-02, -1.0596e-02, -6.4815e-02, -1.0192e-02,\n",
            "         -6.8865e-03,  7.5182e-04,  3.8658e-02,  4.0510e-02,  6.6863e-02,\n",
            "          1.9569e-02, -5.5338e-03, -3.2688e-02,  1.1058e-01,  4.1824e-02,\n",
            "         -9.3858e-03, -3.1704e-02, -8.8630e-03,  2.6041e-02, -6.3797e-03,\n",
            "          6.4481e-02,  1.0071e-02, -6.0624e-03,  2.7366e-02, -4.1868e-02,\n",
            "          1.2046e-03, -5.1127e-02,  4.2600e-02, -8.2887e-03,  1.2700e-02,\n",
            "         -2.0664e-02, -1.4006e-02,  3.5155e-02, -4.1999e-02, -2.8564e-02,\n",
            "          3.8853e-02, -2.9949e-02, -3.3871e-02,  4.0757e-02,  3.9360e-04,\n",
            "         -1.6025e-02,  5.7389e-02,  6.9337e-03,  4.6789e-03,  8.0715e-03,\n",
            "         -1.8584e-02,  1.6516e-02, -9.6010e-03, -4.3527e-03,  2.4791e-02,\n",
            "         -7.3418e-03, -3.0042e-02, -2.8149e-02, -5.7377e-02, -9.8251e-03,\n",
            "          6.5909e-02, -6.0074e-03, -5.3515e-02, -2.1172e-02, -3.7903e-02,\n",
            "         -3.4732e-03, -1.2889e-02, -4.4283e-03, -2.8914e-02, -3.9385e-02,\n",
            "         -2.2249e-02,  3.3989e-03, -1.6636e-02, -1.7285e-02, -8.6930e-02,\n",
            "          3.1786e-02,  4.3922e-02, -1.6917e-02, -8.2385e-03, -2.3099e-02,\n",
            "         -6.6835e-04,  1.1760e-02,  3.8738e-02,  4.8744e-02,  4.0163e-02,\n",
            "          6.9925e-03, -4.0771e-02, -4.9004e-03,  2.9645e-02, -4.7212e-03,\n",
            "         -2.1964e-02, -1.5462e-01, -2.7021e-02, -4.9796e-02, -3.5561e-02,\n",
            "          8.7847e-03, -1.5195e-02, -2.2662e-02,  4.9065e-02,  4.7145e-02,\n",
            "          3.5264e-02,  1.1222e-02,  2.4296e-02,  2.5510e-02, -2.7137e-02,\n",
            "         -1.7608e-02, -3.5704e-02, -1.5967e-02,  8.7211e-03, -3.9302e-02,\n",
            "         -4.4009e-03, -1.6051e-02,  2.1073e-02,  2.1580e-02,  3.5751e-02,\n",
            "          1.1835e-01,  4.1919e-02,  3.6222e-02,  8.0640e-03,  3.1928e-02,\n",
            "         -9.4731e-02,  1.0520e-02, -6.7236e-03, -3.6590e-03, -8.7160e-03,\n",
            "          1.3809e-02, -3.9913e-03, -7.5629e-03, -4.0494e-02,  4.6295e-02,\n",
            "          2.8811e-02, -1.9993e-02,  1.8186e-02,  1.9416e-02, -2.5615e-02,\n",
            "          1.8590e-02,  1.0493e-02,  7.5068e-04,  1.2059e-03,  6.8575e-03,\n",
            "         -1.2490e-01,  7.0901e-03,  7.0705e-03,  4.8312e-02, -2.6259e-02,\n",
            "          1.2950e-02, -6.4926e-03,  1.6814e-02, -4.5553e-03,  3.3198e-02,\n",
            "          3.5700e-02],\n",
            "        [-1.9572e-02,  1.8107e-02, -1.6506e-02,  4.0653e-02,  1.1665e-01,\n",
            "         -4.2406e-02, -2.6028e-02,  4.2924e-02,  1.9108e-02, -1.0574e-02,\n",
            "          2.4427e-03, -5.1320e-03, -1.3079e-02,  2.0285e-02,  1.0695e-01,\n",
            "          4.6915e-02,  1.2907e-03, -1.7199e-02, -1.1391e-02, -4.0005e-02,\n",
            "          9.9000e-02,  1.2199e-02, -4.8041e-03, -8.8219e-02, -4.2452e-02,\n",
            "         -4.5574e-02,  8.3265e-04, -5.9965e-02,  1.9356e-02, -1.1811e-02,\n",
            "          8.3336e-02, -2.6662e-02, -1.6709e-02,  7.3549e-03,  3.2920e-02,\n",
            "         -5.6757e-02, -5.1210e-04, -5.4759e-02, -4.6960e-03,  2.0526e-02,\n",
            "          2.5292e-02, -1.4246e-02, -2.7386e-02,  2.1494e-03, -2.9709e-02,\n",
            "         -1.9060e-02,  8.6432e-04,  8.3037e-02,  1.8410e-02,  4.5355e-04,\n",
            "          8.1687e-02, -8.8304e-03, -4.6396e-02, -4.3885e-02,  8.1356e-02,\n",
            "          9.2126e-02, -1.6780e-03, -9.6366e-02, -6.8121e-03,  3.4613e-02,\n",
            "          7.2076e-02, -2.0132e-02, -1.1905e-02,  9.1895e-02,  3.2769e-03,\n",
            "          5.3961e-02,  2.9323e-02, -5.7436e-02, -1.4193e-02, -5.1579e-03,\n",
            "          4.9074e-02,  3.0441e-02,  3.2204e-02,  9.2285e-02,  6.1171e-02,\n",
            "          2.9747e-02, -1.2806e-01,  8.8979e-02,  1.3614e-02, -4.6831e-02,\n",
            "         -4.6887e-02, -3.9010e-02, -6.8762e-02,  8.4130e-02, -1.9910e-03,\n",
            "         -3.8985e-02,  7.4955e-02, -3.1398e-02,  1.6046e-02,  2.4303e-02,\n",
            "         -2.0623e-02,  1.8670e-02, -4.0060e-02, -1.1951e-02, -1.7828e-02,\n",
            "         -5.4741e-02, -6.8440e-02, -2.5153e-02, -6.3310e-02, -2.7467e-02,\n",
            "         -5.3236e-02, -1.8627e-02,  1.4887e-02, -1.6037e-02,  9.4659e-03,\n",
            "         -2.6005e-04, -4.1697e-02, -2.4068e-02,  4.1189e-02, -6.0066e-03,\n",
            "          8.0569e-03,  9.1150e-03, -8.7259e-02,  3.5532e-02,  7.9276e-02,\n",
            "         -3.6752e-02, -7.7132e-02, -3.4089e-02, -5.4847e-02, -6.3151e-02,\n",
            "         -1.4726e-02, -1.5904e-02, -1.6426e-02, -4.5356e-02,  1.1246e-02,\n",
            "          1.2175e-02,  3.4296e-02,  2.9590e-02, -5.2334e-02,  4.0325e-03,\n",
            "          2.7582e-02,  3.1063e-02,  2.9528e-02, -2.4952e-02, -2.0269e-04,\n",
            "          5.6312e-03, -3.5430e-02,  1.9568e-02,  6.0234e-02,  4.7522e-02,\n",
            "         -2.3417e-02, -7.6504e-02,  9.1125e-02, -5.1998e-02, -1.4081e-02,\n",
            "         -1.0871e-02,  3.0188e-02, -2.9305e-02,  1.1538e-02,  1.8003e-03,\n",
            "         -3.3968e-03,  4.8806e-02,  2.9903e-02,  2.5531e-03, -2.8029e-02,\n",
            "          3.4897e-02,  1.9445e-02, -3.5533e-02,  7.6707e-02, -7.0626e-02,\n",
            "         -4.9195e-02, -1.8853e-02, -1.8934e-02, -6.7490e-02, -3.3278e-02,\n",
            "          3.7978e-02, -7.1045e-02, -1.1102e-02, -5.6537e-02, -1.0856e-02,\n",
            "          2.5836e-02, -3.6382e-02, -1.6348e-02, -2.8684e-02, -1.0720e-02,\n",
            "         -1.0912e-02,  1.0388e-02,  7.0505e-02,  2.1236e-02, -1.5038e-02,\n",
            "         -4.3730e-02, -1.8910e-02, -1.0913e-02, -5.8695e-02, -1.0227e-02,\n",
            "         -6.3533e-03, -6.1678e-03,  2.0217e-02, -3.7031e-02,  3.3974e-02,\n",
            "         -1.0883e-02, -2.1893e-02,  6.6752e-02, -1.0577e-01,  4.4338e-03,\n",
            "          1.1502e-02,  7.6029e-02, -1.8674e-02,  6.9523e-03, -4.1649e-02,\n",
            "          4.6793e-02, -2.1759e-02, -3.4510e-02,  2.8270e-02, -9.2249e-02,\n",
            "         -4.0884e-02, -5.3184e-03, -5.3135e-02, -1.1308e-02, -4.1194e-02,\n",
            "         -6.6025e-02,  6.0457e-02, -2.2637e-02, -6.1701e-02, -8.5546e-02,\n",
            "          3.1386e-02,  1.2063e-02,  3.9890e-02,  3.4734e-02, -8.1638e-04,\n",
            "          2.8980e-02, -3.4756e-03,  1.3315e-02,  8.1221e-02, -2.7798e-02,\n",
            "          3.8353e-02,  3.4092e-02,  2.5978e-04,  4.9958e-02, -3.8694e-03,\n",
            "         -2.2425e-02, -4.6075e-03,  7.8998e-02, -4.7033e-04,  6.2399e-02,\n",
            "         -7.9359e-02,  2.6541e-02, -2.2788e-02, -1.8222e-02,  4.9472e-02,\n",
            "         -8.0277e-02, -3.3274e-02,  5.9184e-02, -3.2127e-03, -1.3160e-02,\n",
            "         -2.2160e-02, -2.3490e-02,  3.5544e-02, -2.6508e-03, -1.1207e-02,\n",
            "         -1.9039e-02,  5.2928e-02, -1.4990e-02, -4.9117e-02, -1.7455e-02,\n",
            "          2.1884e-02],\n",
            "        [-1.2691e-02,  5.2641e-02, -4.1281e-02, -1.6814e-03, -4.4558e-03,\n",
            "         -4.9370e-02, -2.6279e-02, -3.9552e-02,  8.1562e-04,  4.5798e-02,\n",
            "         -4.7962e-03, -2.4591e-04,  9.2049e-03, -3.9117e-02, -2.1895e-02,\n",
            "         -1.6095e-02, -1.8102e-02,  1.0735e-02, -1.9545e-02, -2.1909e-02,\n",
            "         -2.8945e-04, -6.9173e-03, -2.4604e-03, -3.1568e-02, -4.1474e-02,\n",
            "          5.1958e-02,  1.6085e-02, -5.1536e-02, -5.6565e-02, -6.6275e-04,\n",
            "         -6.4704e-02,  4.4251e-02, -1.8810e-02, -2.0810e-02,  1.0519e-04,\n",
            "         -3.5731e-02, -6.2191e-03,  5.5444e-02,  1.7987e-02, -4.6823e-02,\n",
            "         -6.5606e-02, -1.9835e-02, -4.4630e-02,  3.2601e-02, -2.3345e-02,\n",
            "         -4.9565e-02, -2.7368e-02,  8.5313e-03,  4.5229e-02, -2.5950e-02,\n",
            "         -1.1981e-02, -3.5259e-02, -4.2269e-02,  5.3790e-02,  9.6713e-03,\n",
            "          2.3115e-02, -5.3251e-02,  6.5975e-02,  2.3293e-02,  2.2379e-02,\n",
            "          1.8361e-02,  2.0613e-02, -2.2426e-02, -3.7385e-02, -1.2493e-02,\n",
            "          3.3361e-02,  1.0851e-02,  7.0165e-03, -1.5081e-03,  6.6235e-02,\n",
            "          1.1430e-02,  8.3607e-03,  8.3776e-03, -1.8568e-02, -5.9957e-02,\n",
            "         -5.8509e-02, -6.5226e-04,  1.9308e-02, -7.9407e-03,  2.1291e-02,\n",
            "         -5.1989e-02,  2.4553e-02,  2.9895e-03, -1.0746e-02,  5.5228e-03,\n",
            "         -8.9491e-03,  3.5198e-02,  1.8954e-02,  2.3964e-02, -3.9394e-03,\n",
            "         -8.0553e-03, -2.9039e-02,  2.2165e-02, -4.5194e-02, -5.6859e-02,\n",
            "          3.9375e-02,  2.6647e-02, -5.5890e-03, -1.3281e-02,  1.6517e-02,\n",
            "         -4.6554e-02, -4.1465e-02, -1.8740e-02, -4.7753e-02, -1.1762e-02,\n",
            "         -3.1654e-02,  2.2557e-02, -3.9813e-02,  1.9454e-03, -2.3402e-03,\n",
            "         -1.7549e-02,  2.6874e-02, -5.7767e-03, -2.9871e-03, -2.1550e-03,\n",
            "         -3.8671e-02,  2.0187e-02,  9.7983e-02, -2.5798e-03, -3.5179e-02,\n",
            "          1.7998e-02,  1.7021e-02,  7.3400e-02,  1.0986e-02, -2.9655e-02,\n",
            "         -2.3224e-02,  1.2674e-02,  4.4640e-02,  2.9202e-03, -1.2104e-02,\n",
            "         -7.3344e-02, -1.3313e-03,  2.9005e-03, -3.9339e-02, -1.5862e-03,\n",
            "          2.4845e-02,  1.9033e-02,  2.0098e-02,  3.1606e-02,  2.7909e-02,\n",
            "          3.0503e-02, -2.5321e-02,  1.6133e-02,  2.0707e-02,  1.8207e-02,\n",
            "         -8.1198e-03,  3.2161e-02,  4.8042e-02, -2.6898e-02,  1.7411e-02,\n",
            "         -7.5920e-03, -2.1798e-02,  5.6729e-02,  2.6610e-02,  2.6289e-02,\n",
            "          9.0505e-03, -1.6194e-03, -3.1905e-02,  4.9967e-03, -8.5159e-02,\n",
            "          4.9087e-02,  1.2616e-02,  2.3761e-02, -4.5491e-02, -4.3057e-03,\n",
            "          5.2863e-02, -4.3781e-02, -2.2720e-02, -6.4748e-02, -3.7553e-02,\n",
            "          2.9954e-02, -2.3816e-02,  3.2642e-02, -2.6277e-03,  3.3194e-02,\n",
            "         -3.5457e-02,  1.0492e-01,  3.7967e-02,  3.3788e-02, -8.3297e-02,\n",
            "         -2.1529e-02,  7.5756e-03, -4.2484e-02,  2.6426e-02,  2.7069e-02,\n",
            "         -1.8969e-02, -4.7526e-03,  1.3316e-02,  6.6632e-02,  1.3469e-02,\n",
            "          4.4280e-03, -3.8048e-02,  3.0468e-02, -2.0389e-02,  1.2715e-02,\n",
            "         -4.8990e-02,  4.4389e-02,  2.1792e-03,  7.8425e-02,  9.6996e-02,\n",
            "         -6.0360e-02, -2.1460e-02, -5.6160e-02, -6.0374e-02, -6.6547e-02,\n",
            "         -1.9303e-02,  4.1681e-02, -2.4603e-02,  4.2919e-02,  2.6106e-03,\n",
            "          2.4972e-02,  3.0261e-03,  6.5111e-02,  9.9911e-03, -2.9767e-02,\n",
            "         -3.2115e-02, -1.6116e-02,  3.4887e-02,  6.4940e-02,  2.6377e-02,\n",
            "          3.5191e-02,  2.4160e-02,  4.1365e-02,  2.8823e-02, -5.4537e-03,\n",
            "         -5.3740e-02, -3.6543e-02, -2.2641e-02, -3.8225e-02,  3.4054e-02,\n",
            "          2.9797e-02,  1.4676e-02,  3.1711e-02,  6.1763e-03,  5.0722e-03,\n",
            "         -4.4381e-02, -1.6612e-02,  2.4774e-02, -3.1366e-02,  2.6670e-02,\n",
            "          5.9372e-02, -4.2638e-03,  4.9601e-02, -1.1676e-02, -5.7646e-03,\n",
            "         -1.4859e-02, -4.7289e-02,  8.8449e-02,  7.2274e-02, -9.2137e-03,\n",
            "         -1.9963e-02,  2.3623e-02,  5.7223e-03,  1.3969e-02,  8.7058e-03,\n",
            "          1.1378e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.1975,  1.7356,  5.2719, -5.2683, -4.4621], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4853,  1.6991,  4.5843, -5.9209, -4.6806], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,   400, 21239,  ...,    79,  3713,   550],\n",
            "        [50256,     9,    69,  ...,    64,    13,  1867],\n",
            "        [50256,   843,   345,  ...,   286,   262,  6745],\n",
            "        ...,\n",
            "        [50256,  8889,   286,  ..., 12032,    11,   617],\n",
            "        [50256,   360,    13,  ..., 12949,   306, 12049],\n",
            "        [50256,    13,    50,  ..., 20011,    13, 30581]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0107,  0.0704, -0.0221,  ..., -0.0040,  0.0333,  0.0354],\n",
            "        [-0.0197,  0.0181, -0.0167,  ..., -0.0487, -0.0174,  0.0219],\n",
            "        [-0.0129,  0.0528, -0.0413,  ...,  0.0142,  0.0087,  0.1139],\n",
            "        [-0.0129,  0.0624, -0.0624,  ..., -0.0162, -0.0320,  0.0288],\n",
            "        [ 0.0581, -0.0241, -0.0314,  ..., -0.0016,  0.0233,  0.0374]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.0729e-02,  7.0401e-02, -2.2097e-02, -3.1206e-02, -5.9927e-03,\n",
            "         -5.9191e-03, -1.4504e-02,  2.9532e-03, -7.4284e-02, -1.3440e-02,\n",
            "          4.2429e-03,  1.3320e-02, -3.3693e-02, -2.8277e-02, -4.3751e-02,\n",
            "         -3.0744e-02, -4.4220e-03,  2.2170e-02,  1.7682e-02,  2.3429e-02,\n",
            "         -3.1521e-02,  4.3184e-02, -1.2939e-02, -2.9959e-02, -1.3570e-02,\n",
            "          2.5347e-02, -2.9637e-02,  3.8173e-02,  9.0734e-03, -1.1210e-02,\n",
            "         -7.3191e-02,  2.3148e-02,  1.0794e-02,  2.2650e-02, -3.6770e-02,\n",
            "         -5.6706e-03,  6.5484e-02,  3.3842e-02,  3.2899e-02,  5.0584e-03,\n",
            "         -3.6458e-02, -4.6827e-03,  3.1392e-02, -1.4155e-02,  1.2023e-02,\n",
            "          7.7081e-03,  2.6975e-02, -7.6728e-02, -4.2715e-02, -7.0467e-02,\n",
            "         -2.2513e-02,  1.3165e-02, -2.2756e-02,  4.6377e-02,  3.0065e-02,\n",
            "          2.8464e-03, -2.3062e-02, -1.4410e-02,  4.8045e-02,  2.0812e-02,\n",
            "         -5.2150e-03,  6.3129e-04, -1.6693e-04, -1.4367e-02, -3.3836e-02,\n",
            "          2.0446e-02,  5.1877e-02, -1.8335e-02,  2.5758e-02,  4.5913e-02,\n",
            "          2.3257e-02, -1.4844e-02, -1.3581e-02,  2.8908e-02, -4.4380e-02,\n",
            "         -3.6867e-02, -1.7536e-02,  7.2923e-02,  6.7702e-02,  2.6670e-02,\n",
            "         -1.5577e-02, -2.6655e-02,  3.1696e-02, -3.3956e-02, -1.1594e-02,\n",
            "          9.8730e-02,  4.2756e-03,  3.2578e-02, -1.5136e-02,  5.2113e-03,\n",
            "          1.3508e-03, -5.4406e-02,  2.4388e-02,  2.8551e-04, -2.3577e-02,\n",
            "         -2.0484e-02, -7.3124e-02, -4.5746e-02,  1.5892e-02, -1.4351e-02,\n",
            "         -4.9866e-02,  2.6623e-03,  8.6614e-03,  1.6767e-02, -1.0865e-02,\n",
            "         -7.9783e-03,  5.1079e-02, -2.3311e-02, -5.1089e-02,  4.3436e-02,\n",
            "         -3.5209e-02, -1.8459e-02, -1.0319e-02, -6.4807e-02, -1.0502e-02,\n",
            "         -6.8526e-03,  4.8695e-04,  3.8721e-02,  4.0316e-02,  6.6768e-02,\n",
            "          1.9609e-02, -5.8975e-03, -3.2403e-02,  1.1063e-01,  4.1679e-02,\n",
            "         -9.2287e-03, -3.1868e-02, -9.0056e-03,  2.6056e-02, -6.0205e-03,\n",
            "          6.4668e-02,  1.0101e-02, -6.1785e-03,  2.7466e-02, -4.1978e-02,\n",
            "          1.2098e-03, -5.0805e-02,  4.2810e-02, -8.3968e-03,  1.2610e-02,\n",
            "         -2.0874e-02, -1.4194e-02,  3.5250e-02, -4.1743e-02, -2.8166e-02,\n",
            "          3.9025e-02, -2.9745e-02, -3.4182e-02,  4.0847e-02,  6.5916e-04,\n",
            "         -1.6052e-02,  5.7791e-02,  6.5441e-03,  4.7687e-03,  7.9906e-03,\n",
            "         -1.8446e-02,  1.6652e-02, -9.6686e-03, -4.3413e-03,  2.5049e-02,\n",
            "         -7.2336e-03, -3.0137e-02, -2.8231e-02, -5.7114e-02, -9.8018e-03,\n",
            "          6.5931e-02, -5.8421e-03, -5.3329e-02, -2.1193e-02, -3.7806e-02,\n",
            "         -3.6062e-03, -1.3218e-02, -4.0182e-03, -2.8714e-02, -3.9459e-02,\n",
            "         -2.2223e-02,  3.0952e-03, -1.6968e-02, -1.7649e-02, -8.7043e-02,\n",
            "          3.1483e-02,  4.3852e-02, -1.6585e-02, -8.5930e-03, -2.3263e-02,\n",
            "         -8.7430e-04,  1.1758e-02,  3.8539e-02,  4.8587e-02,  4.0105e-02,\n",
            "          7.2869e-03, -4.0520e-02, -4.9547e-03,  2.9863e-02, -4.8466e-03,\n",
            "         -2.2264e-02, -1.5492e-01, -2.7343e-02, -4.9822e-02, -3.5345e-02,\n",
            "          8.9830e-03, -1.5062e-02, -2.2522e-02,  4.9040e-02,  4.7257e-02,\n",
            "          3.5027e-02,  1.1331e-02,  2.4212e-02,  2.5655e-02, -2.6835e-02,\n",
            "         -1.7863e-02, -3.5765e-02, -1.6043e-02,  8.6313e-03, -3.9307e-02,\n",
            "         -4.3380e-03, -1.6071e-02,  2.1091e-02,  2.1197e-02,  3.5640e-02,\n",
            "          1.1837e-01,  4.1786e-02,  3.6406e-02,  8.1967e-03,  3.1930e-02,\n",
            "         -9.4825e-02,  1.0445e-02, -6.4753e-03, -4.1027e-03, -8.9862e-03,\n",
            "          1.4016e-02, -3.9089e-03, -7.4806e-03, -4.0389e-02,  4.6512e-02,\n",
            "          2.9088e-02, -1.9969e-02,  1.8011e-02,  1.9202e-02, -2.5283e-02,\n",
            "          1.8204e-02,  1.0645e-02,  5.2642e-04,  1.2636e-03,  6.5308e-03,\n",
            "         -1.2488e-01,  7.4660e-03,  6.8173e-03,  4.8470e-02, -2.6441e-02,\n",
            "          1.2812e-02, -6.3448e-03,  1.6947e-02, -3.9668e-03,  3.3313e-02,\n",
            "          3.5401e-02],\n",
            "        [-1.9744e-02,  1.8108e-02, -1.6703e-02,  4.0760e-02,  1.1662e-01,\n",
            "         -4.2513e-02, -2.6030e-02,  4.3016e-02,  1.9149e-02, -1.0372e-02,\n",
            "          1.9855e-03, -5.1620e-03, -1.3041e-02,  1.9864e-02,  1.0688e-01,\n",
            "          4.6744e-02,  1.3560e-03, -1.7262e-02, -1.1102e-02, -4.0106e-02,\n",
            "          9.8897e-02,  1.2108e-02, -4.8837e-03, -8.8240e-02, -4.2691e-02,\n",
            "         -4.5742e-02,  8.7121e-04, -5.9550e-02,  1.9567e-02, -1.1848e-02,\n",
            "          8.3311e-02, -2.7193e-02, -1.6836e-02,  7.2864e-03,  3.2865e-02,\n",
            "         -5.6805e-02, -4.2595e-04, -5.4535e-02, -4.5145e-03,  2.0784e-02,\n",
            "          2.5545e-02, -1.4450e-02, -2.7481e-02,  2.0273e-03, -2.9562e-02,\n",
            "         -1.9309e-02,  9.6118e-04,  8.3154e-02,  1.8214e-02,  5.1526e-04,\n",
            "          8.1799e-02, -8.8759e-03, -4.6483e-02, -4.3741e-02,  8.1463e-02,\n",
            "          9.2142e-02, -1.4622e-03, -9.6395e-02, -6.4568e-03,  3.4787e-02,\n",
            "          7.2200e-02, -2.0356e-02, -1.1719e-02,  9.2224e-02,  3.4377e-03,\n",
            "          5.3595e-02,  2.9331e-02, -5.7323e-02, -1.3934e-02, -4.9056e-03,\n",
            "          4.9037e-02,  3.0443e-02,  3.2326e-02,  9.2538e-02,  6.1148e-02,\n",
            "          2.9351e-02, -1.2835e-01,  8.8839e-02,  1.3577e-02, -4.6978e-02,\n",
            "         -4.7070e-02, -3.9034e-02, -6.8843e-02,  8.3875e-02, -2.1507e-03,\n",
            "         -3.8963e-02,  7.4871e-02, -3.1273e-02,  1.6405e-02,  2.4399e-02,\n",
            "         -2.0445e-02,  1.8786e-02, -4.0054e-02, -1.1583e-02, -1.8026e-02,\n",
            "         -5.4864e-02, -6.8670e-02, -2.5009e-02, -6.3225e-02, -2.7297e-02,\n",
            "         -5.3157e-02, -1.8664e-02,  1.4431e-02, -1.6339e-02,  9.5505e-03,\n",
            "         -1.2220e-04, -4.1811e-02, -2.4275e-02,  4.1106e-02, -6.1906e-03,\n",
            "          8.1149e-03,  9.0165e-03, -8.7478e-02,  3.5380e-02,  7.9120e-02,\n",
            "         -3.6606e-02, -7.7219e-02, -3.3878e-02, -5.5053e-02, -6.3208e-02,\n",
            "         -1.4409e-02, -1.6340e-02, -1.5986e-02, -4.5413e-02,  1.1264e-02,\n",
            "          1.2368e-02,  3.4345e-02,  2.9195e-02, -5.2452e-02,  4.2605e-03,\n",
            "          2.7528e-02,  3.0943e-02,  2.9459e-02, -2.4785e-02, -1.2089e-04,\n",
            "          5.5112e-03, -3.5405e-02,  1.9648e-02,  5.9746e-02,  4.7709e-02,\n",
            "         -2.3413e-02, -7.6639e-02,  9.1205e-02, -5.1765e-02, -1.4024e-02,\n",
            "         -1.0909e-02,  3.0028e-02, -2.9869e-02,  1.1455e-02,  1.9434e-03,\n",
            "         -3.5197e-03,  4.8980e-02,  2.9842e-02,  2.6071e-03, -2.8123e-02,\n",
            "          3.4826e-02,  1.9984e-02, -3.5367e-02,  7.6561e-02, -7.0381e-02,\n",
            "         -4.9380e-02, -1.8842e-02, -1.8814e-02, -6.7104e-02, -3.3095e-02,\n",
            "          3.8123e-02, -7.1083e-02, -1.0943e-02, -5.6532e-02, -1.0947e-02,\n",
            "          2.5630e-02, -3.6358e-02, -1.5997e-02, -2.8856e-02, -1.0747e-02,\n",
            "         -1.1007e-02,  1.0391e-02,  7.0325e-02,  2.0898e-02, -1.4965e-02,\n",
            "         -4.3837e-02, -1.8911e-02, -1.0714e-02, -5.9034e-02, -1.0221e-02,\n",
            "         -6.2985e-03, -6.1164e-03,  2.0193e-02, -3.7200e-02,  3.4045e-02,\n",
            "         -1.0759e-02, -2.1710e-02,  6.6525e-02, -1.0575e-01,  4.3913e-03,\n",
            "          1.1542e-02,  7.6110e-02, -1.9228e-02,  6.9899e-03, -4.1784e-02,\n",
            "          4.6821e-02, -2.1750e-02, -3.4230e-02,  2.8368e-02, -9.2291e-02,\n",
            "         -4.1086e-02, -4.9606e-03, -5.3079e-02, -1.1590e-02, -4.0893e-02,\n",
            "         -6.5969e-02,  6.0406e-02, -2.2684e-02, -6.1731e-02, -8.5301e-02,\n",
            "          3.1636e-02,  1.2215e-02,  4.0229e-02,  3.4676e-02, -8.8594e-04,\n",
            "          2.8622e-02, -3.5504e-03,  1.3450e-02,  8.1060e-02, -2.7759e-02,\n",
            "          3.8459e-02,  3.4117e-02,  3.4622e-04,  4.9945e-02, -4.1762e-03,\n",
            "         -2.2490e-02, -4.7239e-03,  7.9192e-02, -5.1278e-04,  6.2634e-02,\n",
            "         -7.8842e-02,  2.6675e-02, -2.2986e-02, -1.8010e-02,  4.9571e-02,\n",
            "         -8.0291e-02, -3.3500e-02,  5.9216e-02, -3.2613e-03, -1.3209e-02,\n",
            "         -2.2232e-02, -2.3240e-02,  3.5456e-02, -2.5474e-03, -1.1276e-02,\n",
            "         -1.9141e-02,  5.2955e-02, -1.4960e-02, -4.8653e-02, -1.7405e-02,\n",
            "          2.1871e-02],\n",
            "        [-1.2883e-02,  5.2818e-02, -4.1308e-02, -1.7283e-03, -4.6269e-03,\n",
            "         -4.9155e-02, -2.6134e-02, -3.9512e-02,  9.2778e-04,  4.5914e-02,\n",
            "         -4.9986e-03, -2.1962e-04,  8.8915e-03, -3.9388e-02, -2.1805e-02,\n",
            "         -1.6089e-02, -1.8295e-02,  1.0423e-02, -1.9605e-02, -2.2163e-02,\n",
            "         -5.8740e-04, -6.9270e-03, -2.4474e-03, -3.1505e-02, -4.1650e-02,\n",
            "          5.1980e-02,  1.6091e-02, -5.1351e-02, -5.6802e-02, -6.6267e-04,\n",
            "         -6.4710e-02,  4.4179e-02, -1.8744e-02, -2.1025e-02,  2.4520e-04,\n",
            "         -3.5601e-02, -6.2691e-03,  5.5534e-02,  1.7876e-02, -4.6812e-02,\n",
            "         -6.5496e-02, -1.9861e-02, -4.4789e-02,  3.2484e-02, -2.3170e-02,\n",
            "         -4.9881e-02, -2.7296e-02,  8.5678e-03,  4.5199e-02, -2.6083e-02,\n",
            "         -1.1958e-02, -3.4983e-02, -4.2306e-02,  5.3737e-02,  9.6910e-03,\n",
            "          2.3127e-02, -5.3097e-02,  6.5844e-02,  2.3324e-02,  2.2529e-02,\n",
            "          1.8541e-02,  2.0498e-02, -2.2461e-02, -3.7325e-02, -1.2376e-02,\n",
            "          3.3139e-02,  1.0735e-02,  7.5595e-03, -1.3962e-03,  6.6343e-02,\n",
            "          1.1290e-02,  8.4446e-03,  8.4039e-03, -1.8607e-02, -6.0070e-02,\n",
            "         -5.8785e-02, -6.3527e-04,  1.9343e-02, -8.0755e-03,  2.1545e-02,\n",
            "         -5.2119e-02,  2.4471e-02,  2.9560e-03, -1.0797e-02,  5.3934e-03,\n",
            "         -8.8225e-03,  3.5349e-02,  1.8982e-02,  2.4109e-02, -3.7901e-03,\n",
            "         -7.8696e-03, -2.8969e-02,  2.1995e-02, -4.5112e-02, -5.7028e-02,\n",
            "          3.9623e-02,  2.6377e-02, -5.0008e-03, -1.3403e-02,  1.6417e-02,\n",
            "         -4.6686e-02, -4.1380e-02, -1.9159e-02, -4.7775e-02, -1.1583e-02,\n",
            "         -3.1306e-02,  2.2476e-02, -3.9994e-02,  2.2961e-03, -2.4336e-03,\n",
            "         -1.7613e-02,  2.6896e-02, -5.5019e-03, -3.1615e-03, -2.3548e-03,\n",
            "         -3.8800e-02,  2.0032e-02,  9.7939e-02, -2.8529e-03, -3.5238e-02,\n",
            "          1.7951e-02,  1.6836e-02,  7.3711e-02,  1.0887e-02, -2.9651e-02,\n",
            "         -2.3190e-02,  1.2820e-02,  4.4262e-02,  2.8062e-03, -1.2061e-02,\n",
            "         -7.3441e-02, -1.3620e-03,  2.6001e-03, -3.9181e-02, -1.5205e-03,\n",
            "          2.4760e-02,  1.9051e-02,  2.0123e-02,  3.1327e-02,  2.7744e-02,\n",
            "          3.0670e-02, -2.5154e-02,  1.6272e-02,  2.0666e-02,  1.8327e-02,\n",
            "         -8.1030e-03,  3.2188e-02,  4.7738e-02, -2.6878e-02,  1.7634e-02,\n",
            "         -7.8704e-03, -2.1754e-02,  5.6815e-02,  2.6584e-02,  2.6192e-02,\n",
            "          9.0000e-03, -1.3861e-03, -3.1912e-02,  4.9412e-03, -8.5135e-02,\n",
            "          4.9118e-02,  1.2580e-02,  2.3937e-02, -4.5148e-02, -4.1636e-03,\n",
            "          5.2756e-02, -4.3914e-02, -2.2614e-02, -6.4854e-02, -3.7479e-02,\n",
            "          2.9866e-02, -2.3756e-02,  3.2955e-02, -2.7714e-03,  3.3454e-02,\n",
            "         -3.5331e-02,  1.0475e-01,  3.8043e-02,  3.3735e-02, -8.3300e-02,\n",
            "         -2.1459e-02,  7.7100e-03, -4.2148e-02,  2.6212e-02,  2.7158e-02,\n",
            "         -1.8988e-02, -4.5564e-03,  1.3308e-02,  6.6655e-02,  1.3389e-02,\n",
            "          4.3282e-03, -3.7984e-02,  3.0355e-02, -2.0234e-02,  1.2739e-02,\n",
            "         -4.9081e-02,  4.4462e-02,  1.9663e-03,  7.8294e-02,  9.6920e-02,\n",
            "         -6.0144e-02, -2.1479e-02, -5.6024e-02, -6.0250e-02, -6.6379e-02,\n",
            "         -1.9759e-02,  4.1895e-02, -2.4495e-02,  4.2937e-02,  2.6351e-03,\n",
            "          2.5094e-02,  2.9209e-03,  6.5075e-02,  9.8364e-03, -2.9801e-02,\n",
            "         -3.2224e-02, -1.6182e-02,  3.4797e-02,  6.4680e-02,  2.6495e-02,\n",
            "          3.5084e-02,  2.4129e-02,  4.1565e-02,  2.8966e-02, -5.3160e-03,\n",
            "         -5.3758e-02, -3.6664e-02, -2.2441e-02, -3.8735e-02,  3.4078e-02,\n",
            "          3.0123e-02,  1.4646e-02,  3.1815e-02,  6.0667e-03,  5.2699e-03,\n",
            "         -4.4338e-02, -1.6611e-02,  2.4584e-02, -3.1522e-02,  2.6689e-02,\n",
            "          5.9466e-02, -4.1490e-03,  4.9582e-02, -1.1906e-02, -5.8228e-03,\n",
            "         -1.4713e-02, -4.6876e-02,  8.8512e-02,  7.2270e-02, -9.1344e-03,\n",
            "         -1.9998e-02,  2.3346e-02,  5.7412e-03,  1.4167e-02,  8.7189e-03,\n",
            "          1.1390e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.2355,  1.6580,  5.2137, -5.3034, -4.5663], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.4921,  1.6765,  4.5311, -6.0011, -4.8619], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n",
            "Step: 1000, Loss: 6.1036\n",
            "[DEBUG - DemoTransformer] tokens.shape: torch.Size([8, 256])\n",
            "[DEBUG - DemoTransformer] embed.shape: torch.Size([8, 256, 256])\n",
            "tokens shape: torch.Size([8, 256])\n",
            "tokens: tensor([[50256,  3519,   326,  ...,   392,   259,    11],\n",
            "        [50256,  2476,   286,  ...,  4899,   284,   670],\n",
            "        [50256,  1581,    37,  ..., 15227,  3883,   357],\n",
            "        ...,\n",
            "        [50256,  3850,   317,  ...,   351, 17526,  5470],\n",
            "        [50256,   220,   220,  ...,  5769,  1776,   198],\n",
            "        [50256,  4613, 29244,  ...,   684,   290, 12800]], device='cuda:0')\n",
            "positions shape: torch.Size([256])\n",
            "positions: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255], device='cuda:0')\n",
            "W_pos shape: torch.Size([256, 256])\n",
            "W_pos (first few rows): tensor([[ 0.0102,  0.0703, -0.0216,  ..., -0.0032,  0.0336,  0.0349],\n",
            "        [-0.0198,  0.0180, -0.0166,  ..., -0.0482, -0.0173,  0.0217],\n",
            "        [-0.0131,  0.0532, -0.0413,  ...,  0.0143,  0.0088,  0.1141],\n",
            "        [-0.0128,  0.0625, -0.0624,  ..., -0.0160, -0.0317,  0.0286],\n",
            "        [ 0.0579, -0.0240, -0.0314,  ..., -0.0014,  0.0235,  0.0375]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (before broadcast): torch.Size([256, 256])\n",
            "pos_embeddings (first few positions): tensor([[ 1.0199e-02,  7.0284e-02, -2.1621e-02, -3.1584e-02, -5.8434e-03,\n",
            "         -6.2866e-03, -1.4028e-02,  2.7317e-03, -7.4345e-02, -1.3377e-02,\n",
            "          3.8958e-03,  1.3047e-02, -3.3425e-02, -2.8501e-02, -4.3650e-02,\n",
            "         -3.1119e-02, -4.6405e-03,  2.1611e-02,  1.7689e-02,  2.3437e-02,\n",
            "         -3.1697e-02,  4.3378e-02, -1.3316e-02, -2.9945e-02, -1.3690e-02,\n",
            "          2.5657e-02, -2.9175e-02,  3.8324e-02,  9.2824e-03, -1.1139e-02,\n",
            "         -7.3358e-02,  2.2371e-02,  1.1176e-02,  2.2889e-02, -3.6845e-02,\n",
            "         -5.9655e-03,  6.6098e-02,  3.3773e-02,  3.2684e-02,  4.9701e-03,\n",
            "         -3.6435e-02, -4.6384e-03,  3.1299e-02, -1.4125e-02,  1.2307e-02,\n",
            "          7.8033e-03,  2.7029e-02, -7.6353e-02, -4.2332e-02, -7.0395e-02,\n",
            "         -2.2700e-02,  1.3118e-02, -2.2563e-02,  4.6719e-02,  2.9975e-02,\n",
            "          3.0994e-03, -2.2831e-02, -1.3816e-02,  4.8277e-02,  2.0655e-02,\n",
            "         -4.5773e-03,  1.9614e-04, -2.6826e-04, -1.4020e-02, -3.3377e-02,\n",
            "          2.0107e-02,  5.1569e-02, -1.7991e-02,  2.5511e-02,  4.5634e-02,\n",
            "          2.2998e-02, -1.5010e-02, -1.3300e-02,  2.8854e-02, -4.4292e-02,\n",
            "         -3.7413e-02, -1.7391e-02,  7.3148e-02,  6.7794e-02,  2.6621e-02,\n",
            "         -1.5661e-02, -2.6406e-02,  3.1480e-02, -3.4638e-02, -1.1415e-02,\n",
            "          9.8862e-02,  4.0929e-03,  3.2443e-02, -1.4977e-02,  5.3745e-03,\n",
            "          1.1564e-03, -5.4183e-02,  2.3970e-02,  5.6784e-04, -2.3487e-02,\n",
            "         -2.0270e-02, -7.3192e-02, -4.5061e-02,  1.6049e-02, -1.4399e-02,\n",
            "         -4.9670e-02,  2.6798e-03,  8.3692e-03,  1.6759e-02, -1.1049e-02,\n",
            "         -7.9863e-03,  5.1352e-02, -2.3421e-02, -5.0839e-02,  4.3584e-02,\n",
            "         -3.5456e-02, -1.8797e-02, -9.7957e-03, -6.4593e-02, -1.0991e-02,\n",
            "         -6.8689e-03, -1.0772e-04,  3.8787e-02,  4.0034e-02,  6.6611e-02,\n",
            "          1.9664e-02, -6.2900e-03, -3.2093e-02,  1.1090e-01,  4.1330e-02,\n",
            "         -9.0060e-03, -3.2250e-02, -9.3079e-03,  2.5980e-02, -5.5427e-03,\n",
            "          6.4684e-02,  1.0091e-02, -6.3746e-03,  2.7583e-02, -4.2090e-02,\n",
            "          1.1286e-03, -5.0347e-02,  4.3285e-02, -8.5840e-03,  1.2323e-02,\n",
            "         -2.1204e-02, -1.4514e-02,  3.5441e-02, -4.1374e-02, -2.7520e-02,\n",
            "          3.9183e-02, -2.9376e-02, -3.4723e-02,  4.0864e-02,  1.1978e-03,\n",
            "         -1.5803e-02,  5.8472e-02,  5.9791e-03,  4.9318e-03,  7.7628e-03,\n",
            "         -1.8113e-02,  1.6713e-02, -9.9245e-03, -4.2613e-03,  2.5418e-02,\n",
            "         -7.1600e-03, -3.0495e-02, -2.8488e-02, -5.6919e-02, -9.9839e-03,\n",
            "          6.5979e-02, -5.8787e-03, -5.2840e-02, -2.1272e-02, -3.7828e-02,\n",
            "         -3.5809e-03, -1.3729e-02, -3.4461e-03, -2.8492e-02, -3.9501e-02,\n",
            "         -2.2391e-02,  2.6140e-03, -1.7518e-02, -1.8219e-02, -8.7192e-02,\n",
            "          3.1157e-02,  4.3920e-02, -1.5890e-02, -8.9689e-03, -2.3170e-02,\n",
            "         -1.2627e-03,  1.1653e-02,  3.8031e-02,  4.8408e-02,  3.9838e-02,\n",
            "          7.7752e-03, -4.0255e-02, -5.1264e-03,  3.0020e-02, -4.9858e-03,\n",
            "         -2.2854e-02, -1.5524e-01, -2.7788e-02, -5.0075e-02, -3.5021e-02,\n",
            "          9.3550e-03, -1.4754e-02, -2.2536e-02,  4.9096e-02,  4.7560e-02,\n",
            "          3.4715e-02,  1.1323e-02,  2.4213e-02,  2.6010e-02, -2.6372e-02,\n",
            "         -1.8237e-02, -3.5732e-02, -1.5931e-02,  8.5225e-03, -3.9354e-02,\n",
            "         -4.3646e-03, -1.6277e-02,  2.0883e-02,  2.0573e-02,  3.5395e-02,\n",
            "          1.1851e-01,  4.1706e-02,  3.6650e-02,  8.6557e-03,  3.1790e-02,\n",
            "         -9.4887e-02,  1.0467e-02, -6.0605e-03, -4.9021e-03, -9.2431e-03,\n",
            "          1.4275e-02, -3.6303e-03, -7.4733e-03, -4.0082e-02,  4.7001e-02,\n",
            "          2.9352e-02, -2.0104e-02,  1.7617e-02,  1.8671e-02, -2.4716e-02,\n",
            "          1.7559e-02,  1.1097e-02,  4.3668e-04,  1.4744e-03,  6.3017e-03,\n",
            "         -1.2477e-01,  8.0886e-03,  6.4680e-03,  4.8721e-02, -2.6475e-02,\n",
            "          1.2644e-02, -6.0485e-03,  1.7336e-02, -3.2098e-03,  3.3622e-02,\n",
            "          3.4892e-02],\n",
            "        [-1.9838e-02,  1.8011e-02, -1.6611e-02,  4.0802e-02,  1.1642e-01,\n",
            "         -4.2690e-02, -2.5896e-02,  4.3115e-02,  1.9216e-02, -1.0148e-02,\n",
            "          1.6533e-03, -5.2576e-03, -1.3038e-02,  1.9560e-02,  1.0698e-01,\n",
            "          4.6504e-02,  1.3762e-03, -1.7299e-02, -1.1101e-02, -4.0119e-02,\n",
            "          9.8798e-02,  1.2183e-02, -4.9053e-03, -8.8167e-02, -4.2855e-02,\n",
            "         -4.5695e-02,  1.0012e-03, -5.9273e-02,  1.9812e-02, -1.1826e-02,\n",
            "          8.3429e-02, -2.7895e-02, -1.7028e-02,  7.1092e-03,  3.2797e-02,\n",
            "         -5.6889e-02, -3.6457e-04, -5.4476e-02, -4.3895e-03,  2.1010e-02,\n",
            "          2.5900e-02, -1.4447e-02, -2.7647e-02,  1.7667e-03, -2.9451e-02,\n",
            "         -1.9471e-02,  1.0158e-03,  8.3463e-02,  1.8057e-02,  6.8668e-04,\n",
            "          8.1904e-02, -8.8584e-03, -4.6351e-02, -4.3482e-02,  8.1604e-02,\n",
            "          9.2154e-02, -1.2836e-03, -9.6393e-02, -6.2075e-03,  3.4882e-02,\n",
            "          7.2404e-02, -2.0653e-02, -1.1500e-02,  9.2538e-02,  3.5138e-03,\n",
            "          5.3269e-02,  2.9215e-02, -5.7141e-02, -1.3742e-02, -4.7996e-03,\n",
            "          4.9008e-02,  3.0561e-02,  3.2475e-02,  9.2668e-02,  6.1146e-02,\n",
            "          2.9090e-02, -1.2858e-01,  8.8684e-02,  1.3436e-02, -4.7162e-02,\n",
            "         -4.7293e-02, -3.8945e-02, -6.8886e-02,  8.3521e-02, -2.1600e-03,\n",
            "         -3.9053e-02,  7.4758e-02, -3.1110e-02,  1.6672e-02,  2.4571e-02,\n",
            "         -2.0331e-02,  1.8786e-02, -4.0102e-02, -1.1288e-02, -1.8277e-02,\n",
            "         -5.4812e-02, -6.8794e-02, -2.4752e-02, -6.3130e-02, -2.7284e-02,\n",
            "         -5.3071e-02, -1.8614e-02,  1.4140e-02, -1.6767e-02,  9.6382e-03,\n",
            "          8.5546e-05, -4.1913e-02, -2.4434e-02,  4.1041e-02, -6.4511e-03,\n",
            "          8.2485e-03,  8.9935e-03, -8.7586e-02,  3.5278e-02,  7.8843e-02,\n",
            "         -3.6697e-02, -7.7434e-02, -3.3706e-02, -5.5228e-02, -6.3231e-02,\n",
            "         -1.4258e-02, -1.6770e-02, -1.5707e-02, -4.5585e-02,  1.1197e-02,\n",
            "          1.2548e-02,  3.4470e-02,  2.8889e-02, -5.2348e-02,  4.6048e-03,\n",
            "          2.7421e-02,  3.0808e-02,  2.9353e-02, -2.4742e-02, -1.9503e-05,\n",
            "          5.2818e-03, -3.5217e-02,  1.9640e-02,  5.9193e-02,  4.7826e-02,\n",
            "         -2.3346e-02, -7.6810e-02,  9.1296e-02, -5.1461e-02, -1.3868e-02,\n",
            "         -1.0799e-02,  2.9973e-02, -3.0500e-02,  1.1458e-02,  2.2320e-03,\n",
            "         -3.4912e-03,  4.9216e-02,  2.9641e-02,  2.6282e-03, -2.8272e-02,\n",
            "          3.4727e-02,  2.0414e-02, -3.5237e-02,  7.6374e-02, -7.0009e-02,\n",
            "         -4.9523e-02, -1.8838e-02, -1.8536e-02, -6.6664e-02, -3.3154e-02,\n",
            "          3.8007e-02, -7.1151e-02, -1.0712e-02, -5.6548e-02, -1.1023e-02,\n",
            "          2.5446e-02, -3.6267e-02, -1.5709e-02, -2.8888e-02, -1.0788e-02,\n",
            "         -1.1151e-02,  1.0356e-02,  7.0204e-02,  2.0607e-02, -1.4746e-02,\n",
            "         -4.3922e-02, -1.9050e-02, -1.0431e-02, -5.9314e-02, -1.0282e-02,\n",
            "         -6.2258e-03, -6.0397e-03,  2.0049e-02, -3.7370e-02,  3.4007e-02,\n",
            "         -1.0546e-02, -2.1468e-02,  6.6296e-02, -1.0576e-01,  4.5200e-03,\n",
            "          1.1597e-02,  7.6299e-02, -1.9618e-02,  7.0583e-03, -4.1911e-02,\n",
            "          4.6805e-02, -2.1720e-02, -3.4003e-02,  2.8353e-02, -9.2510e-02,\n",
            "         -4.1426e-02, -4.6170e-03, -5.3183e-02, -1.1946e-02, -4.0814e-02,\n",
            "         -6.5852e-02,  6.0481e-02, -2.2697e-02, -6.1852e-02, -8.5055e-02,\n",
            "          3.1914e-02,  1.2202e-02,  4.0395e-02,  3.4493e-02, -9.2267e-04,\n",
            "          2.8173e-02, -3.6093e-03,  1.3428e-02,  8.0925e-02, -2.7785e-02,\n",
            "          3.8538e-02,  3.4028e-02,  5.2765e-04,  4.9885e-02, -4.4237e-03,\n",
            "         -2.2358e-02, -4.7675e-03,  7.9327e-02, -4.9327e-04,  6.2625e-02,\n",
            "         -7.8376e-02,  2.6816e-02, -2.3112e-02, -1.7998e-02,  4.9704e-02,\n",
            "         -8.0569e-02, -3.3586e-02,  5.9120e-02, -3.3612e-03, -1.3273e-02,\n",
            "         -2.2136e-02, -2.3061e-02,  3.5208e-02, -2.4713e-03, -1.1426e-02,\n",
            "         -1.9090e-02,  5.3231e-02, -1.4813e-02, -4.8172e-02, -1.7265e-02,\n",
            "          2.1688e-02],\n",
            "        [-1.3113e-02,  5.3160e-02, -4.1314e-02, -1.9307e-03, -4.6834e-03,\n",
            "         -4.8957e-02, -2.5919e-02, -3.9537e-02,  9.5459e-04,  4.6107e-02,\n",
            "         -5.1012e-03, -1.4802e-04,  8.6499e-03, -3.9547e-02, -2.1701e-02,\n",
            "         -1.6048e-02, -1.8538e-02,  1.0065e-02, -1.9624e-02, -2.2372e-02,\n",
            "         -8.3393e-04, -6.8671e-03, -2.4624e-03, -3.1473e-02, -4.1955e-02,\n",
            "          5.1967e-02,  1.6070e-02, -5.1308e-02, -5.7030e-02, -6.3121e-04,\n",
            "         -6.4812e-02,  4.4071e-02, -1.8710e-02, -2.1255e-02,  2.9975e-04,\n",
            "         -3.5511e-02, -6.3052e-03,  5.5633e-02,  1.7680e-02, -4.6863e-02,\n",
            "         -6.5518e-02, -1.9864e-02, -4.4980e-02,  3.2406e-02, -2.3030e-02,\n",
            "         -5.0176e-02, -2.7215e-02,  8.6199e-03,  4.5209e-02, -2.6309e-02,\n",
            "         -1.1973e-02, -3.4824e-02, -4.2404e-02,  5.3665e-02,  9.7111e-03,\n",
            "          2.3105e-02, -5.3039e-02,  6.5798e-02,  2.3414e-02,  2.2634e-02,\n",
            "          1.8816e-02,  2.0331e-02, -2.2552e-02, -3.7150e-02, -1.2274e-02,\n",
            "          3.3000e-02,  1.0566e-02,  8.1011e-03, -1.3976e-03,  6.6416e-02,\n",
            "          1.1204e-02,  8.4850e-03,  8.5022e-03, -1.8633e-02, -6.0175e-02,\n",
            "         -5.9110e-02, -6.4393e-04,  1.9489e-02, -8.1333e-03,  2.1800e-02,\n",
            "         -5.2277e-02,  2.4554e-02,  2.9271e-03, -1.0808e-02,  5.3247e-03,\n",
            "         -8.5217e-03,  3.5564e-02,  1.8950e-02,  2.4200e-02, -3.5740e-03,\n",
            "         -7.7474e-03, -2.8932e-02,  2.1768e-02, -4.5093e-02, -5.7170e-02,\n",
            "          3.9812e-02,  2.5954e-02, -4.4625e-03, -1.3495e-02,  1.6459e-02,\n",
            "         -4.6839e-02, -4.1303e-02, -1.9547e-02, -4.7788e-02, -1.1424e-02,\n",
            "         -3.0955e-02,  2.2517e-02, -4.0092e-02,  2.6462e-03, -2.3493e-03,\n",
            "         -1.7692e-02,  2.6935e-02, -5.2580e-03, -3.3168e-03, -2.5263e-03,\n",
            "         -3.8845e-02,  1.9815e-02,  9.8004e-02, -3.1035e-03, -3.5281e-02,\n",
            "          1.7891e-02,  1.6693e-02,  7.4028e-02,  1.0915e-02, -2.9607e-02,\n",
            "         -2.3179e-02,  1.2817e-02,  4.3871e-02,  2.7010e-03, -1.2034e-02,\n",
            "         -7.3488e-02, -1.4488e-03,  2.2454e-03, -3.8983e-02, -1.3741e-03,\n",
            "          2.4659e-02,  1.8952e-02,  2.0193e-02,  3.1104e-02,  2.7581e-02,\n",
            "          3.0710e-02, -2.5046e-02,  1.6410e-02,  2.0732e-02,  1.8456e-02,\n",
            "         -8.0382e-03,  3.2278e-02,  4.7642e-02, -2.6852e-02,  1.7871e-02,\n",
            "         -8.2074e-03, -2.1719e-02,  5.6833e-02,  2.6643e-02,  2.6051e-02,\n",
            "          9.0039e-03, -1.1682e-03, -3.2106e-02,  5.0079e-03, -8.5249e-02,\n",
            "          4.9148e-02,  1.2523e-02,  2.4081e-02, -4.4900e-02, -4.0331e-03,\n",
            "          5.2829e-02, -4.4146e-02, -2.2446e-02, -6.5020e-02, -3.7477e-02,\n",
            "          2.9829e-02, -2.3698e-02,  3.3247e-02, -2.9223e-03,  3.3695e-02,\n",
            "         -3.5097e-02,  1.0459e-01,  3.8079e-02,  3.3646e-02, -8.3495e-02,\n",
            "         -2.1463e-02,  7.8622e-03, -4.1723e-02,  2.5925e-02,  2.7247e-02,\n",
            "         -1.9110e-02, -4.4021e-03,  1.3326e-02,  6.6695e-02,  1.3493e-02,\n",
            "          4.3286e-03, -3.7959e-02,  3.0165e-02, -2.0120e-02,  1.2816e-02,\n",
            "         -4.9244e-02,  4.4415e-02,  1.8364e-03,  7.8165e-02,  9.6831e-02,\n",
            "         -5.9949e-02, -2.1475e-02, -5.5965e-02, -6.0106e-02, -6.6255e-02,\n",
            "         -2.0130e-02,  4.2019e-02, -2.4329e-02,  4.2965e-02,  2.7110e-03,\n",
            "          2.5176e-02,  2.8048e-03,  6.5064e-02,  9.6150e-03, -2.9950e-02,\n",
            "         -3.2338e-02, -1.6287e-02,  3.4734e-02,  6.4373e-02,  2.6571e-02,\n",
            "          3.5080e-02,  2.4256e-02,  4.1903e-02,  2.9247e-02, -5.0808e-03,\n",
            "         -5.3812e-02, -3.6768e-02, -2.2219e-02, -3.9304e-02,  3.4131e-02,\n",
            "          3.0442e-02,  1.4616e-02,  3.1911e-02,  6.0634e-03,  5.6468e-03,\n",
            "         -4.4307e-02, -1.6640e-02,  2.4327e-02, -3.1687e-02,  2.6629e-02,\n",
            "          5.9469e-02, -4.0471e-03,  4.9606e-02, -1.2127e-02, -5.8686e-03,\n",
            "         -1.4586e-02, -4.6557e-02,  8.8603e-02,  7.2231e-02, -9.0943e-03,\n",
            "         -1.9988e-02,  2.3153e-02,  5.8596e-03,  1.4323e-02,  8.7799e-03,\n",
            "          1.1410e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "pos_embeddings shape (after broadcast): torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] pos_embed.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] x.shape after embedding: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.1654,  1.4404,  5.0716, -5.2777, -4.6308], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_pre.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] attn_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_mid.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] normalized_resid_mid.shape: torch.Size([8, 256, 256])\n",
            "cfg.d_model 768\n",
            "cfg.d_mlp 3072\n",
            "normalized_resid_mid.shape torch.Size([8, 256, 256])\n",
            "self.W_in.shape torch.Size([256, 1024])\n",
            "self.b_in.shape torch.Size([1024])\n",
            "self.W_out.shape torch.Size([1024, 256])\n",
            "self.b_out.shape torch.Size([256])\n",
            "output.shape torch.Size([8, 256, 1024])\n",
            "hidden_activtaion.shape torch.Size([8, 256, 1024])\n",
            "output_out.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] mlp_out.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post.shape: torch.Size([8, 256, 256])\n",
            "[DEBUG - TransformerBlock] resid_post (first 5 elements): tensor([ 0.3631,  1.4918,  4.4014, -5.9825, -4.9934], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "residual.shape torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] normalized_residual_final: torch.Size([8, 256, 256])\n",
            "[DEBUG - DemoTransformer] logits.shape: torch.Size([8, 256, 50257])\n"
          ]
        }
      ],
      "source": [
        "losses = []\n",
        "print(\"Number of batches:\", len(data_loader))\n",
        "for epoch in range(num_epochs):\n",
        "    for c, batch in tqdm.tqdm(enumerate(data_loader)):\n",
        "        tokens = batch['tokens'].cuda()\n",
        "        logits = model(tokens)\n",
        "        loss = lm_cross_entropy_loss(logits, tokens)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        losses.append(loss.item())\n",
        "        if c % log_every == 0:\n",
        "            print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
        "        if c > max_steps:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUTqerZ-x625"
      },
      "source": [
        "We can now plot a loss curve!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGaZLaQCx625",
        "outputId": "62397086-f0db-4d3a-dac6-a5f93b846f71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"3e38a786-3a90-4200-920c-59a1d61ba127\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3e38a786-3a90-4200-920c-59a1d61ba127\")) {                    Plotly.newPlot(                        \"3e38a786-3a90-4200-920c-59a1d61ba127\",                        [{\"hovertemplate\":\"Tokens=%{x}\\u003cbr\\u003eLoss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"showlegend\":false,\"x\":[0,2048,4096,6144,8192,10240,12288,14336,16384,18432,20480,22528,24576,26624,28672,30720,32768,34816,36864,38912,40960,43008,45056,47104,49152,51200,53248,55296,57344,59392,61440,63488,65536,67584,69632,71680,73728,75776,77824,79872,81920,83968,86016,88064,90112,92160,94208,96256,98304,100352,102400,104448,106496,108544,110592,112640,114688,116736,118784,120832,122880,124928,126976,129024,131072,133120,135168,137216,139264,141312,143360,145408,147456,149504,151552,153600,155648,157696,159744,161792,163840,165888,167936,169984,172032,174080,176128,178176,180224,182272,184320,186368,188416,190464,192512,194560,196608,198656,200704,202752,204800,206848,208896,210944,212992,215040,217088,219136,221184,223232,225280,227328,229376,231424,233472,235520,237568,239616,241664,243712,245760,247808,249856,251904,253952,256000,258048,260096,262144,264192,266240,268288,270336,272384,274432,276480,278528,280576,282624,284672,286720,288768,290816,292864,294912,296960,299008,301056,303104,305152,307200,309248,311296,313344,315392,317440,319488,321536,323584,325632,327680,329728,331776,333824,335872,337920,339968,342016,344064,346112,348160,350208,352256,354304,356352,358400,360448,362496,364544,366592,368640,370688,372736,374784,376832,378880,380928,382976,385024,387072,389120,391168,393216,395264,397312,399360,401408,403456,405504,407552,409600,411648,413696,415744,417792,419840,421888,423936,425984,428032,430080,432128,434176,436224,438272,440320,442368,444416,446464,448512,450560,452608,454656,456704,458752,460800,462848,464896,466944,468992,471040,473088,475136,477184,479232,481280,483328,485376,487424,489472,491520,493568,495616,497664,499712,501760,503808,505856,507904,509952,512000,514048,516096,518144,520192,522240,524288,526336,528384,530432,532480,534528,536576,538624,540672,542720,544768,546816,548864,550912,552960,555008,557056,559104,561152,563200,565248,567296,569344,571392,573440,575488,577536,579584,581632,583680,585728,587776,589824,591872,593920,595968,598016,600064,602112,604160,606208,608256,610304,612352,614400,616448,618496,620544,622592,624640,626688,628736,630784,632832,634880,636928,638976,641024,643072,645120,647168,649216,651264,653312,655360,657408,659456,661504,663552,665600,667648,669696,671744,673792,675840,677888,679936,681984,684032,686080,688128,690176,692224,694272,696320,698368,700416,702464,704512,706560,708608,710656,712704,714752,716800,718848,720896,722944,724992,727040,729088,731136,733184,735232,737280,739328,741376,743424,745472,747520,749568,751616,753664,755712,757760,759808,761856,763904,765952,768000,770048,772096,774144,776192,778240,780288,782336,784384,786432,788480,790528,792576,794624,796672,798720,800768,802816,804864,806912,808960,811008,813056,815104,817152,819200,821248,823296,825344,827392,829440,831488,833536,835584,837632,839680,841728,843776,845824,847872,849920,851968,854016,856064,858112,860160,862208,864256,866304,868352,870400,872448,874496,876544,878592,880640,882688,884736,886784,888832,890880,892928,894976,897024,899072,901120,903168,905216,907264,909312,911360,913408,915456,917504,919552,921600,923648,925696,927744,929792,931840,933888,935936,937984,940032,942080,944128,946176,948224,950272,952320,954368,956416,958464,960512,962560,964608,966656,968704,970752,972800,974848,976896,978944,980992,983040,985088,987136,989184,991232,993280,995328,997376,999424,1001472,1003520,1005568,1007616,1009664,1011712,1013760,1015808,1017856,1019904,1021952,1024000,1026048,1028096,1030144,1032192,1034240,1036288,1038336,1040384,1042432,1044480,1046528,1048576,1050624,1052672,1054720,1056768,1058816,1060864,1062912,1064960,1067008,1069056,1071104,1073152,1075200,1077248,1079296,1081344,1083392,1085440,1087488,1089536,1091584,1093632,1095680,1097728,1099776,1101824,1103872,1105920,1107968,1110016,1112064,1114112,1116160,1118208,1120256,1122304,1124352,1126400,1128448,1130496,1132544,1134592,1136640,1138688,1140736,1142784,1144832,1146880,1148928,1150976,1153024,1155072,1157120,1159168,1161216,1163264,1165312,1167360,1169408,1171456,1173504,1175552,1177600,1179648,1181696,1183744,1185792,1187840,1189888,1191936,1193984,1196032,1198080,1200128,1202176,1204224,1206272,1208320,1210368,1212416,1214464,1216512,1218560,1220608,1222656,1224704,1226752,1228800,1230848,1232896,1234944,1236992,1239040,1241088,1243136,1245184,1247232,1249280,1251328,1253376,1255424,1257472,1259520,1261568,1263616,1265664,1267712,1269760,1271808,1273856,1275904,1277952,1280000,1282048,1284096,1286144,1288192,1290240,1292288,1294336,1296384,1298432,1300480,1302528,1304576,1306624,1308672,1310720,1312768,1314816,1316864,1318912,1320960,1323008,1325056,1327104,1329152,1331200,1333248,1335296,1337344,1339392,1341440,1343488,1345536,1347584,1349632,1351680,1353728,1355776,1357824,1359872,1361920,1363968,1366016,1368064,1370112,1372160,1374208,1376256,1378304,1380352,1382400,1384448,1386496,1388544,1390592,1392640,1394688,1396736,1398784,1400832,1402880,1404928,1406976,1409024,1411072,1413120,1415168,1417216,1419264,1421312,1423360,1425408,1427456,1429504,1431552,1433600,1435648,1437696,1439744,1441792,1443840,1445888,1447936,1449984,1452032,1454080,1456128,1458176,1460224,1462272,1464320,1466368,1468416,1470464,1472512,1474560,1476608,1478656,1480704,1482752,1484800,1486848,1488896,1490944,1492992,1495040,1497088,1499136,1501184,1503232,1505280,1507328,1509376,1511424,1513472,1515520,1517568,1519616,1521664,1523712,1525760,1527808,1529856,1531904,1533952,1536000,1538048,1540096,1542144,1544192,1546240,1548288,1550336,1552384,1554432,1556480,1558528,1560576,1562624,1564672,1566720,1568768,1570816,1572864,1574912,1576960,1579008,1581056,1583104,1585152,1587200,1589248,1591296,1593344,1595392,1597440,1599488,1601536,1603584,1605632,1607680,1609728,1611776,1613824,1615872,1617920,1619968,1622016,1624064,1626112,1628160,1630208,1632256,1634304,1636352,1638400,1640448,1642496,1644544,1646592,1648640,1650688,1652736,1654784,1656832,1658880,1660928,1662976,1665024,1667072,1669120,1671168,1673216,1675264,1677312,1679360,1681408,1683456,1685504,1687552,1689600,1691648,1693696,1695744,1697792,1699840,1701888,1703936,1705984,1708032,1710080,1712128,1714176,1716224,1718272,1720320,1722368,1724416,1726464,1728512,1730560,1732608,1734656,1736704,1738752,1740800,1742848,1744896,1746944,1748992,1751040,1753088,1755136,1757184,1759232,1761280,1763328,1765376,1767424,1769472,1771520,1773568,1775616,1777664,1779712,1781760,1783808,1785856,1787904,1789952,1792000,1794048,1796096,1798144,1800192,1802240,1804288,1806336,1808384,1810432,1812480,1814528,1816576,1818624,1820672,1822720,1824768,1826816,1828864,1830912,1832960,1835008,1837056,1839104,1841152,1843200,1845248,1847296,1849344,1851392,1853440,1855488,1857536,1859584,1861632,1863680,1865728,1867776,1869824,1871872,1873920,1875968,1878016,1880064,1882112,1884160,1886208,1888256,1890304,1892352,1894400,1896448,1898496,1900544,1902592,1904640,1906688,1908736,1910784,1912832,1914880,1916928,1918976,1921024,1923072,1925120,1927168,1929216,1931264,1933312,1935360,1937408,1939456,1941504,1943552,1945600,1947648,1949696,1951744,1953792,1955840,1957888,1959936,1961984,1964032,1966080,1968128,1970176,1972224,1974272,1976320,1978368,1980416,1982464,1984512,1986560,1988608,1990656,1992704,1994752,1996800,1998848,2000896,2002944,2004992,2007040,2009088,2011136,2013184,2015232,2017280,2019328,2021376,2023424,2025472,2027520,2029568,2031616,2033664,2035712,2037760,2039808,2041856,2043904,2045952,2048000,2050048],\"xaxis\":\"x\",\"y\":[10.827051162719727,10.4627046585083,9.820755004882812,9.74729061126709,9.743914604187012,8.977696418762207,9.386046409606934,9.207179069519043,9.041712760925293,7.47626256942749,8.731893539428711,8.317161560058594,7.163965702056885,8.305222511291504,6.943396091461182,7.406515121459961,6.761206150054932,8.126065254211426,7.237849712371826,7.999102592468262,7.840875625610352,7.203570365905762,7.845748424530029,6.8430609703063965,8.032286643981934,8.10824203491211,8.369283676147461,7.320500373840332,7.682297706604004,8.865550994873047,8.341444969177246,7.665449142456055,6.638803482055664,8.061437606811523,7.611839771270752,8.174323081970215,6.950134754180908,7.961367130279541,7.946389198303223,7.784919261932373,7.439785003662109,7.948718070983887,7.573058605194092,8.006638526916504,7.9963765144348145,7.333552837371826,7.93156099319458,7.839768409729004,7.7215399742126465,7.6539411544799805,7.4050726890563965,6.556657314300537,7.721806526184082,8.11965274810791,7.909263610839844,7.19211483001709,7.406332969665527,8.387669563293457,6.0327043533325195,7.491594314575195,7.5996246337890625,6.5676045417785645,6.743006706237793,7.799614429473877,6.944352149963379,6.70115852355957,7.4473466873168945,7.82598352432251,7.735339641571045,6.686671733856201,7.4529900550842285,7.433802127838135,7.199458122253418,7.709170341491699,7.985673904418945,7.89496374130249,6.289065361022949,6.410251140594482,8.553037643432617,7.420799255371094,7.259633541107178,7.601092338562012,7.730001449584961,5.996098041534424,7.079218864440918,7.279481887817383,7.186569690704346,7.827770233154297,6.324709415435791,7.310756683349609,6.449371814727783,7.273812294006348,7.1418280601501465,6.5871124267578125,7.598065376281738,6.499610900878906,7.224673748016357,4.594654083251953,7.098460674285889,7.116329193115234,5.861419200897217,7.509352684020996,6.619752883911133,6.879225730895996,7.671104907989502,6.397485256195068,6.752346992492676,7.8555731773376465,7.336592674255371,6.818466663360596,6.709506034851074,6.7073445320129395,7.402984619140625,6.897595405578613,5.3361639976501465,6.439934730529785,7.107890605926514,6.890290260314941,7.368287086486816,7.056219577789307,6.681662559509277,6.671926975250244,7.428158760070801,6.960343360900879,5.586147308349609,7.1059346199035645,5.468528747558594,5.14462423324585,6.353768348693848,7.274134159088135,6.805314064025879,7.630080223083496,6.474194049835205,6.1741790771484375,7.139220714569092,6.769538402557373,7.3777174949646,7.558695316314697,6.431338787078857,6.8554205894470215,7.448310375213623,6.716111660003662,5.999391555786133,7.314694881439209,6.928828716278076,7.445350646972656,7.334232330322266,7.24879789352417,7.860372066497803,6.480332851409912,6.692857265472412,7.453462600708008,4.30596399307251,6.777794361114502,6.980434417724609,7.1849870681762695,7.338450908660889,7.641209602355957,7.024200916290283,7.226143836975098,7.028778076171875,6.815379619598389,6.3468523025512695,7.103885173797607,7.068551063537598,7.543460845947266,6.4565253257751465,6.978308200836182,7.022786617279053,6.459380149841309,7.223010540008545,7.4179840087890625,6.380610942840576,7.135262966156006,6.529976844787598,6.592623233795166,6.483500003814697,6.421502113342285,5.956380367279053,6.955187797546387,6.979493618011475,7.1254377365112305,6.088230609893799,7.34998893737793,6.409397602081299,7.017890453338623,6.0220746994018555,7.252763748168945,6.710555076599121,6.348274230957031,7.243241786956787,6.142012596130371,5.587371826171875,6.622201442718506,7.293536186218262,7.185337066650391,5.739060878753662,5.638742923736572,7.462558746337891,6.2707014083862305,6.781866073608398,7.373496055603027,7.158176422119141,4.982669353485107,6.577141284942627,6.235270023345947,6.892230033874512,7.301455020904541,5.94510555267334,5.902015686035156,7.060814380645752,6.069406986236572,6.693307876586914,6.391199588775635,7.486800193786621,6.0419511795043945,6.123093605041504,6.499837398529053,6.873190879821777,7.069601535797119,5.526218891143799,7.070226669311523,6.845363616943359,6.397796630859375,7.2165656089782715,6.36257791519165,5.045802116394043,6.192082405090332,7.16011381149292,6.943662166595459,6.509929656982422,6.9823150634765625,5.776032447814941,7.096192836761475,7.036093235015869,6.128973007202148,7.1025495529174805,6.9102253913879395,6.046412944793701,7.052309513092041,6.705694198608398,6.2245283126831055,6.468494892120361,6.366489887237549,6.922119617462158,6.553726673126221,7.2925801277160645,5.600844860076904,6.861651420593262,7.043081283569336,6.778890609741211,6.128705978393555,6.315864562988281,6.829256057739258,4.77468729019165,6.674757480621338,7.209478378295898,6.698125839233398,6.391638278961182,7.510626792907715,7.7647600173950195,6.778867244720459,6.648822784423828,7.200028419494629,4.555611610412598,6.2504496574401855,7.300556659698486,6.5209832191467285,7.328976631164551,6.992447376251221,5.965704441070557,5.253148555755615,6.821783542633057,6.468041896820068,6.846325397491455,6.826323509216309,6.318423748016357,7.017374038696289,5.4375762939453125,7.1534881591796875,5.492923259735107,6.503918170928955,6.098947525024414,5.96976900100708,6.721305847167969,6.038077354431152,6.7965087890625,6.239636421203613,6.409868240356445,6.935886383056641,5.6894450187683105,6.427369594573975,6.547307968139648,6.6295623779296875,5.620596408843994,5.99500036239624,6.8640828132629395,7.080390930175781,5.989953517913818,5.534154891967773,6.733346462249756,6.299536228179932,7.101995468139648,7.249181270599365,6.503488063812256,7.015136241912842,5.588470935821533,5.074263095855713,6.7155632972717285,6.534903049468994,7.148256778717041,5.470428943634033,6.440725803375244,7.306710720062256,6.45457124710083,6.349857807159424,6.861856937408447,6.077304840087891,7.558014869689941,6.693042755126953,5.214907169342041,6.355043888092041,6.405433654785156,6.7018280029296875,6.272732257843018,6.335953235626221,7.0692548751831055,6.038268089294434,6.80523157119751,6.203802108764648,5.359567642211914,6.287761688232422,7.479562759399414,6.954071521759033,6.505764961242676,6.2163567543029785,6.667895317077637,6.804490566253662,6.005517959594727,6.07693338394165,5.465457916259766,6.766369342803955,5.073411464691162,6.8671698570251465,5.19005012512207,5.815090179443359,5.677014350891113,6.161300182342529,5.654486179351807,6.327545166015625,6.6310858726501465,6.1621527671813965,7.263007164001465,6.703655242919922,6.760745525360107,6.2921462059021,6.675349712371826,6.4173994064331055,6.720005035400391,6.826094627380371,5.669196128845215,7.077097415924072,6.731217861175537,6.823268413543701,6.416181564331055,7.04111385345459,6.266920566558838,7.11743688583374,4.867831707000732,6.71816873550415,6.650371074676514,5.9922075271606445,6.850611209869385,6.298409461975098,5.399263858795166,5.819205284118652,6.611786365509033,6.024127006530762,6.5029144287109375,7.198276996612549,6.044139862060547,6.628497123718262,6.151702880859375,5.807314872741699,7.026803970336914,6.32744836807251,6.214137554168701,6.331515789031982,5.493635654449463,6.831811428070068,6.097796440124512,6.178968906402588,5.313510417938232,6.061843395233154,6.470879077911377,6.18026876449585,5.6940507888793945,6.510148048400879,6.049078941345215,5.6844964027404785,6.0936713218688965,6.209113121032715,6.0778093338012695,7.141905784606934,5.871643543243408,6.1356401443481445,6.774848937988281,6.692483901977539,6.088156223297119,6.518019199371338,6.94255256652832,6.893274307250977,6.313372611999512,6.444960594177246,6.976223945617676,5.773815155029297,7.205204010009766,6.541782855987549,6.67402458190918,6.718822002410889,6.654636383056641,6.401493549346924,5.757053375244141,6.463776111602783,5.969468116760254,6.748151779174805,5.857611656188965,6.766296863555908,6.6469950675964355,5.877615928649902,5.873284816741943,5.999920845031738,7.0658674240112305,5.392565727233887,6.493720531463623,6.872372150421143,7.081112384796143,6.084096908569336,6.0812811851501465,6.9534382820129395,7.026670932769775,6.734379768371582,5.818863391876221,6.716304779052734,6.976685523986816,6.963589191436768,6.643831729888916,6.679239749908447,5.839061737060547,6.0900774002075195,6.610424041748047,6.370828151702881,5.297563552856445,5.22922420501709,6.909409999847412,5.289857387542725,5.561377048492432,6.65851354598999,5.444947719573975,5.400805473327637,6.009324550628662,5.904417037963867,5.374161720275879,6.047407150268555,5.832546710968018,6.480838298797607,6.218052387237549,6.1247878074646,5.70815896987915,6.040245056152344,5.3712992668151855,6.0535736083984375,6.090433120727539,7.227097034454346,6.643465042114258,6.867699146270752,6.7841691970825195,6.704765319824219,6.013260841369629,5.862777233123779,6.5667853355407715,6.371889114379883,6.153464317321777,6.021871089935303,6.201747417449951,6.482736110687256,5.7141499519348145,6.4424519538879395,6.303360939025879,6.160935401916504,6.445476055145264,6.352118968963623,6.101235866546631,5.829823970794678,7.061123371124268,6.522218704223633,6.336264133453369,6.430289268493652,4.850401401519775,5.80350923538208,5.0323100090026855,6.680882930755615,6.0668511390686035,5.351384162902832,6.855130195617676,6.699138641357422,6.700838088989258,6.226161479949951,3.88188099861145,6.651205539703369,6.494217872619629,6.3102827072143555,6.195902347564697,7.059447288513184,4.3632121086120605,6.366669654846191,6.270768642425537,6.164252281188965,6.866450786590576,4.951345443725586,5.795162677764893,5.976885795593262,6.36589241027832,5.12570333480835,5.759657859802246,4.741433143615723,6.4088850021362305,4.481536388397217,6.323906898498535,6.032675743103027,6.165857791900635,6.074370384216309,6.144968032836914,6.780843734741211,6.310458183288574,5.642491340637207,6.045779228210449,6.595731258392334,5.335379123687744,5.627954006195068,6.86686897277832,6.775088310241699,5.897010803222656,6.9140400886535645,4.463573455810547,5.009815692901611,6.263533115386963,5.662099361419678,6.7790398597717285,5.986814022064209,6.490821838378906,5.612914085388184,6.277186870574951,6.408072471618652,6.186450958251953,6.565116882324219,5.4187912940979,5.078037261962891,6.054017066955566,6.21346378326416,5.794094562530518,5.251338005065918,6.326147556304932,6.76627254486084,6.475273132324219,6.962440013885498,6.4234232902526855,6.504761695861816,6.583684921264648,6.515296936035156,5.986121654510498,6.136442184448242,7.003728866577148,6.128963470458984,5.7898430824279785,5.796302318572998,4.852760314941406,6.032080173492432,6.659598350524902,6.889903545379639,6.416598796844482,6.135126113891602,6.340809345245361,5.834559917449951,6.05198335647583,6.464055061340332,5.931929111480713,6.579193592071533,5.146371841430664,5.735968589782715,6.800075054168701,4.112600803375244,6.326262474060059,5.212991237640381,6.007424354553223,5.906754970550537,6.686208724975586,6.490384578704834,6.560880661010742,5.280706405639648,6.552947998046875,6.445375919342041,5.2690019607543945,7.184919834136963,6.580160617828369,4.718746662139893,5.380893230438232,6.845898628234863,5.826272010803223,6.787037372589111,5.6183247566223145,6.822730541229248,6.525441646575928,5.311966896057129,6.015798091888428,6.762580394744873,6.567931652069092,6.571068286895752,6.428995609283447,5.807554721832275,5.944369792938232,5.418254375457764,5.748723983764648,5.610611915588379,5.893240451812744,5.90458869934082,6.435509204864502,4.454386234283447,6.110105037689209,5.695845603942871,5.721312522888184,6.009896278381348,5.438628196716309,6.578113079071045,5.927470684051514,5.71920919418335,3.832369089126587,5.680246353149414,5.885800838470459,4.992056369781494,4.742458343505859,6.817885875701904,6.74020528793335,6.447032451629639,5.998063087463379,6.929246425628662,5.517033576965332,6.580036640167236,5.267410755157471,5.9321370124816895,5.14203405380249,6.490994453430176,5.9935302734375,5.56538724899292,6.666811943054199,5.261733531951904,5.704411029815674,5.687189102172852,6.601239204406738,6.383541584014893,6.107330799102783,5.272181510925293,6.041346073150635,6.444886207580566,6.416411399841309,6.616173267364502,6.276137828826904,6.170986652374268,6.118661403656006,5.649109840393066,6.747905731201172,5.7187652587890625,6.277218818664551,6.331748962402344,6.443161487579346,5.994563102722168,5.8447442054748535,5.85881233215332,6.766468048095703,6.794854164123535,6.222548484802246,6.4729743003845215,6.208377838134766,5.827929496765137,5.639521598815918,5.144979953765869,5.180989742279053,4.778034210205078,5.432344913482666,5.35684061050415,6.615839958190918,5.987027168273926,4.7733378410339355,6.4955525398254395,5.893819332122803,6.503514289855957,6.029123306274414,5.5772809982299805,6.500555992126465,5.816181182861328,5.546903133392334,6.665261268615723,5.6862993240356445,6.211038589477539,6.544479846954346,5.692309379577637,6.009903430938721,6.874267101287842,6.465384006500244,6.489918231964111,6.181740760803223,6.694500923156738,5.462414264678955,6.673835754394531,6.876892566680908,6.141688346862793,6.611340045928955,5.826788902282715,6.274994850158691,5.768147945404053,4.405518054962158,6.51972770690918,6.555079936981201,5.879599094390869,6.674765586853027,6.597903728485107,6.763993740081787,5.805090427398682,5.047091960906982,6.32344913482666,6.391402721405029,6.122027397155762,6.197591781616211,5.683310031890869,5.85168981552124,6.39132833480835,5.74348783493042,5.3926615715026855,5.427706718444824,5.680588722229004,6.00292444229126,6.361031532287598,6.132514953613281,5.710747718811035,6.160494327545166,5.997671604156494,5.8130669593811035,5.488643646240234,5.209956645965576,6.656659126281738,4.875015735626221,5.685473918914795,6.527324199676514,5.937211990356445,5.6042866706848145,6.252457141876221,6.039977073669434,5.287991523742676,6.145386219024658,6.4978461265563965,6.055325508117676,5.9073591232299805,6.024145603179932,6.025413990020752,6.173679351806641,5.2349700927734375,4.944239616394043,6.169106483459473,4.589328765869141,6.376733779907227,6.182062149047852,6.288661479949951,5.891627788543701,6.471980094909668,5.463170051574707,5.447773456573486,6.661435604095459,5.978115558624268,6.009971618652344,5.752616882324219,4.879009246826172,6.238775253295898,6.238050937652588,6.3938469886779785,6.653380870819092,6.620893955230713,4.181859016418457,6.135290145874023,6.243005275726318,6.667332649230957,4.284509181976318,6.993732452392578,5.919339179992676,5.864583492279053,6.777019500732422,6.539087295532227,5.5227131843566895,6.090308666229248,6.5584492683410645,5.604185104370117,6.22675895690918,5.746176242828369,5.8660759925842285,5.214390277862549,6.297029495239258,6.406487941741943,6.232845306396484,5.953465461730957,5.75141716003418,5.923267841339111,5.3285064697265625,5.146028518676758,4.62490177154541,6.1731791496276855,5.357665061950684,4.825372219085693,5.685497760772705,4.979937553405762,5.816591262817383,6.400013446807861,5.817028999328613,5.729108810424805,6.249640941619873,6.507883548736572,5.57105016708374,6.566609859466553,6.234363079071045,6.090243339538574,6.57110071182251,6.061446189880371,6.141663074493408,6.321877479553223,5.488357067108154,6.469091892242432,6.283434867858887,5.5223846435546875,4.992962837219238,5.989412307739258,5.661622047424316,5.977930068969727,5.266729354858398,5.804474353790283,6.024596214294434,5.523943901062012,6.427420139312744,6.467281818389893,6.5245866775512695,4.861266136169434,6.315650463104248,5.979443550109863,6.500410556793213,5.706437110900879,5.131185054779053,5.464945316314697,6.486242294311523,5.0105204582214355,5.719631195068359,6.2946062088012695,6.172357559204102,6.436199188232422,6.631049633026123,6.289969444274902,5.594885349273682,6.036766052246094,6.095154762268066,6.353626251220703,6.387752056121826,5.8172688484191895,6.3371405601501465,6.4029083251953125,5.623970031738281,5.629125595092773,5.756141185760498,5.715243339538574,4.598836898803711,6.310717582702637,5.582869529724121,5.70070219039917,5.844297885894775,6.2345871925354,5.871020317077637,5.448488235473633,5.642242431640625,5.071062088012695,6.025089740753174,6.251041889190674,5.774267196655273,4.8683671951293945,6.092489242553711,5.694293975830078,6.11142635345459,6.073615550994873,5.209556579589844,5.02053165435791,6.301302909851074,6.108128070831299,6.048326015472412,5.528035640716553,6.4205827713012695,5.581361293792725,5.743842124938965,6.483750343322754,4.70750093460083,6.501239776611328,5.9864501953125,5.3554487228393555,6.282090663909912,5.395895481109619,5.3871750831604,6.181637287139893,6.1811747550964355,5.123889923095703,5.553387641906738,5.094335556030273,5.7574143409729,6.348339080810547,5.7957563400268555,5.001338481903076,5.912802696228027,5.644686698913574,5.051860809326172,4.269863605499268,6.595860004425049,5.777382850646973,6.041992664337158,5.833200454711914,5.974177360534668,6.3520097732543945,6.306802749633789,6.2519121170043945,6.10103178024292,6.409969806671143,6.396157264709473,5.489413738250732,5.496773719787598,6.045324802398682,6.040307521820068,5.794432163238525,4.942668437957764,6.495800971984863,6.543684482574463,5.830605983734131,4.937530040740967,5.543832778930664,5.450538158416748,5.218689918518066,5.461742401123047,6.076324939727783,6.116353511810303,6.225987434387207,6.484716415405273,5.680018901824951,6.362748146057129,6.348659515380859,5.558005332946777,5.341170787811279,5.753523349761963,5.718108177185059,6.809092044830322,4.799673557281494,5.566118240356445,6.463679313659668,5.248294830322266,5.159145832061768,6.111258029937744,6.228891372680664,5.691249370574951,6.186933517456055,5.541065216064453,6.280270099639893,6.484227180480957,5.469377517700195,6.179590702056885,4.026223182678223,5.346877574920654,6.605380058288574,6.306597709655762,6.537539005279541,5.574575424194336,6.05136775970459,5.903984069824219,5.725018501281738,5.3578410148620605,5.446066379547119,5.878396511077881,5.449873447418213,6.101894855499268,5.384463310241699,5.764683246612549,5.812304973602295,5.502336025238037,6.2639265060424805,6.53233003616333,5.57494592666626,6.033020496368408,6.066638469696045,5.1207122802734375,5.5332136154174805,5.2634968757629395,5.9426703453063965,6.103572845458984,6.2052717208862305],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Tokens\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Training curve for my tiny demo model!\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3e38a786-3a90-4200-920c-59a1d61ba127');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "px.line(y=losses, x=np.arange(len(losses))*(model_cfg.n_ctx * batch_size), labels={\"y\":\"Loss\", \"x\":\"Tokens\"}, title=\"Training curve for my tiny demo model!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PuhF2xb4DuWg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7fcb9775cdfb48a180c69449280f60a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3987fc0774141fc87b3802ea88f91eb",
              "IPY_MODEL_84dce524a4aa4c4883d1929a998046ef",
              "IPY_MODEL_1b0f5d13471748b38254708274f47220"
            ],
            "layout": "IPY_MODEL_04b8730d9e5d404a90eb874ab2e95f16"
          }
        },
        "a3987fc0774141fc87b3802ea88f91eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30873368ed8a4152acd9bd4cebaaf7ef",
            "placeholder": "​",
            "style": "IPY_MODEL_8a2373c0ee4943cbae94acdd0acc14a8",
            "value": "100%"
          }
        },
        "84dce524a4aa4c4883d1929a998046ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9aa0783ece664599902d0cbe685f85bf",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e4b48e528e04de499d360d2b56f6a32",
            "value": 100
          }
        },
        "1b0f5d13471748b38254708274f47220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1164538d2def4a389778bfd68785af79",
            "placeholder": "​",
            "style": "IPY_MODEL_6f81b426dca34afdb15127a0c14527d9",
            "value": " 100/100 [00:13&lt;00:00,  7.26it/s]"
          }
        },
        "04b8730d9e5d404a90eb874ab2e95f16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30873368ed8a4152acd9bd4cebaaf7ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a2373c0ee4943cbae94acdd0acc14a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9aa0783ece664599902d0cbe685f85bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e4b48e528e04de499d360d2b56f6a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1164538d2def4a389778bfd68785af79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f81b426dca34afdb15127a0c14527d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57ce05c380aa4fc586299e839032533c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb9356d803314a418dafc742a7df01dc",
              "IPY_MODEL_3b5c7a80c9844431ad9b729e07856403",
              "IPY_MODEL_9488f664974b4f93b5df25c53d3e14ff"
            ],
            "layout": "IPY_MODEL_bfcd9f574d4e43c0afd20f945decedc1"
          }
        },
        "bb9356d803314a418dafc742a7df01dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11cf68a2605d44ceba0d9a2751545497",
            "placeholder": "​",
            "style": "IPY_MODEL_dc6ff916e7a740b68adbc04cc4631b43",
            "value": "README.md: 100%"
          }
        },
        "3b5c7a80c9844431ad9b729e07856403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_690a692fe7f6402c91a92de51ebfe963",
            "max": 373,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cb087f2d82a431d807922c8556ed736",
            "value": 373
          }
        },
        "9488f664974b4f93b5df25c53d3e14ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4f059fd0c894935ab3a2bc3f4b7f4dd",
            "placeholder": "​",
            "style": "IPY_MODEL_8e26ed2b56174cb88aff2acbed31f169",
            "value": " 373/373 [00:00&lt;00:00, 28.9kB/s]"
          }
        },
        "bfcd9f574d4e43c0afd20f945decedc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11cf68a2605d44ceba0d9a2751545497": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc6ff916e7a740b68adbc04cc4631b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "690a692fe7f6402c91a92de51ebfe963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cb087f2d82a431d807922c8556ed736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4f059fd0c894935ab3a2bc3f4b7f4dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e26ed2b56174cb88aff2acbed31f169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44d5ca7a36704342b8c2e35c4e5600f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_164ab06aeeaf43cbbf629ff4c56378f7",
              "IPY_MODEL_df7e9ff1e0bb49f889628da9571a2700",
              "IPY_MODEL_297b0ec4b424475a8659ee68855df4d8"
            ],
            "layout": "IPY_MODEL_6847db13a51e449e93ccadbd52700f4c"
          }
        },
        "164ab06aeeaf43cbbf629ff4c56378f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9170a15997a640808c3d4abc129046a4",
            "placeholder": "​",
            "style": "IPY_MODEL_e7de58f368ca4cc896086af800c910c9",
            "value": "dataset_infos.json: 100%"
          }
        },
        "df7e9ff1e0bb49f889628da9571a2700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c7e8d3dc7de475bb6d96013961db218",
            "max": 921,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2d5074eb8074b3db607a32d40226c1a",
            "value": 921
          }
        },
        "297b0ec4b424475a8659ee68855df4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8689cb223b014289a59147617c9be304",
            "placeholder": "​",
            "style": "IPY_MODEL_a0291a8674d1413e85a092e1035eeb73",
            "value": " 921/921 [00:00&lt;00:00, 79.2kB/s]"
          }
        },
        "6847db13a51e449e93ccadbd52700f4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9170a15997a640808c3d4abc129046a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7de58f368ca4cc896086af800c910c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c7e8d3dc7de475bb6d96013961db218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2d5074eb8074b3db607a32d40226c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8689cb223b014289a59147617c9be304": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0291a8674d1413e85a092e1035eeb73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "002d84fa3d7a41fc9d75da0b091c2cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b42ea6d105ce4ae485c493b482da9eb4",
              "IPY_MODEL_90750f51430a4def9bf55bbe3e708736",
              "IPY_MODEL_c00846c5d527401795a798ebbfe861d2"
            ],
            "layout": "IPY_MODEL_e496233771874cf1a3f569c2989730cb"
          }
        },
        "b42ea6d105ce4ae485c493b482da9eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbcff679ce4347369eb39dab18311fff",
            "placeholder": "​",
            "style": "IPY_MODEL_85e5b876d3a04be3af67a4917d38b15f",
            "value": "(…)-00000-of-00001-4746b8785c874cc7.parquet: 100%"
          }
        },
        "90750f51430a4def9bf55bbe3e708736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e581698676d4a588e968c7f1afaac48",
            "max": 33262901,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcc28140bcbb4527a29a9946ecb34cdb",
            "value": 33262901
          }
        },
        "c00846c5d527401795a798ebbfe861d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc30edbb40af404cb8a833286269547c",
            "placeholder": "​",
            "style": "IPY_MODEL_89c13c5167b947c5b7566cf719f1d70e",
            "value": " 33.3M/33.3M [00:01&lt;00:00, 23.0MB/s]"
          }
        },
        "e496233771874cf1a3f569c2989730cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbcff679ce4347369eb39dab18311fff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85e5b876d3a04be3af67a4917d38b15f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e581698676d4a588e968c7f1afaac48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc28140bcbb4527a29a9946ecb34cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc30edbb40af404cb8a833286269547c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89c13c5167b947c5b7566cf719f1d70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af6b054347414a528773bbadb9121c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64057d667aba4ea989052bc4f4b43370",
              "IPY_MODEL_305cfdbb940f4009a3a6a20923337ff4",
              "IPY_MODEL_7c45a65c99b946a0b91bc49c656645b3"
            ],
            "layout": "IPY_MODEL_a7257c08058b4600982cccfc28767eab"
          }
        },
        "64057d667aba4ea989052bc4f4b43370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40af14c86324459790df9b98ab9068b9",
            "placeholder": "​",
            "style": "IPY_MODEL_0ba725b7ccfc470c824597474e3df80d",
            "value": "Generating train split: 100%"
          }
        },
        "305cfdbb940f4009a3a6a20923337ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bab323dea9764d39b83c754bde294dff",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1f8863445f94f2eaf4306a1c82b7a7f",
            "value": 10000
          }
        },
        "7c45a65c99b946a0b91bc49c656645b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70d351a824e04c588b5e421ba74961ae",
            "placeholder": "​",
            "style": "IPY_MODEL_f971ff26807e417c9bde60367cec1d94",
            "value": " 10000/10000 [00:00&lt;00:00, 32014.58 examples/s]"
          }
        },
        "a7257c08058b4600982cccfc28767eab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40af14c86324459790df9b98ab9068b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ba725b7ccfc470c824597474e3df80d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bab323dea9764d39b83c754bde294dff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f8863445f94f2eaf4306a1c82b7a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70d351a824e04c588b5e421ba74961ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f971ff26807e417c9bde60367cec1d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b3c103cb75d4d61b948ad4f963793b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36c5279d4aec4b11bb96913d085b67fe",
              "IPY_MODEL_da82ef11204e46ddbf8e7d7e19fc2305",
              "IPY_MODEL_aec0200fb9d04db69efb954ad463fa60"
            ],
            "layout": "IPY_MODEL_5187df2b344542f1a7e6b2b0a45abdf8"
          }
        },
        "36c5279d4aec4b11bb96913d085b67fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a06a6852e663415e9600365545b099b3",
            "placeholder": "​",
            "style": "IPY_MODEL_fbf61fc29e394071a751219162f38eae",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "da82ef11204e46ddbf8e7d7e19fc2305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3abe1d3e67a4a1ba96e2a8f9386fdca",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b30f4f7b311349e993c28a32f2fb5415",
            "value": 10000
          }
        },
        "aec0200fb9d04db69efb954ad463fa60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_150df001338044d3bb115e556ba563eb",
            "placeholder": "​",
            "style": "IPY_MODEL_2a82e85d106f49239c0724db3218a609",
            "value": " 10000/10000 [00:22&lt;00:00, 368.63 examples/s]"
          }
        },
        "5187df2b344542f1a7e6b2b0a45abdf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a06a6852e663415e9600365545b099b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbf61fc29e394071a751219162f38eae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3abe1d3e67a4a1ba96e2a8f9386fdca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b30f4f7b311349e993c28a32f2fb5415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "150df001338044d3bb115e556ba563eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a82e85d106f49239c0724db3218a609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acb4ceaedd7e40e290f0356894acb36f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24a3f41ee07443ca989433257874c1d3",
              "IPY_MODEL_c6d0c4a8c66c450cb7fa5dad05220e5f",
              "IPY_MODEL_ec721abd2fa8475fa4d1dd5fde477d31"
            ],
            "layout": "IPY_MODEL_9ae9059384d348cfbb79799e7ccb8fcb"
          }
        },
        "24a3f41ee07443ca989433257874c1d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d72388df4a9845cf856b2557e5e9675e",
            "placeholder": "​",
            "style": "IPY_MODEL_61f226e603db462eaad4eecdc81d393b",
            "value": ""
          }
        },
        "c6d0c4a8c66c450cb7fa5dad05220e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4521623748b4a21b43357e9b2884131",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2950972ce54a486baa01dfb2302fa5b3",
            "value": 1
          }
        },
        "ec721abd2fa8475fa4d1dd5fde477d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8452bfb34604c0b8d2c69ac9216c46d",
            "placeholder": "​",
            "style": "IPY_MODEL_b6622497e5c342418d86a8cdbace0f8e",
            "value": " 1001/? [01:14&lt;00:00, 13.42it/s]"
          }
        },
        "9ae9059384d348cfbb79799e7ccb8fcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d72388df4a9845cf856b2557e5e9675e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61f226e603db462eaad4eecdc81d393b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4521623748b4a21b43357e9b2884131": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2950972ce54a486baa01dfb2302fa5b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8452bfb34604c0b8d2c69ac9216c46d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6622497e5c342418d86a8cdbace0f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}